


\documentclass[english,11pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{amsmath}
\usepackage{babel}
\usepackage[symbol]{footmisc}

\usepackage{fullpage}
\usepackage{hyperref,url}
\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{LastRevised=Friday, August 23, 2019 12:15:11}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{Language=American English}


\begin{document}


\begin{center}
\textbf{Economics 600a}

\textbf{Fall 2025}

\textbf{Marek Chadim}\footnote[1]{marek.chadim@yale.edu}

\textbf{Homework Solution}

\end{center}

\bigskip

\section{Overview}

I estimate demand and supply in a stylized model of the market for pay-TV services.
I create my own fake data set
for the industry and do some relatively simple estimation. Then, using the \texttt{pyBLP} package of Conlon and Gortmaker, I estimate the model and perform some merger simulations.

\section{Model}

There are $T$ markets, each with four inside goods $j\in
\{1,2,3,4\}$ and an outside option. Goods 1 and 2 are satellite television
services (e.g., DirecTV and Dish); goods 3 and 4 are wired television
services (e.g., Frontier and Comcast in New Haven).
The conditional indirect utility of consumer $i$ for good $j$ in market $t$
is given by
\begin{align*}
u_{ijt}& =\beta ^{\left( 1\right) }x_{jt}+\beta
_{i}^{(2)}satellite_{jt}+\beta _{i}^{(3)}wired_{jt}+\alpha p_{jt}+\xi
_{jt}+\epsilon _{ijt}\text{ \qquad }j>0 \\
u_{i0t}& =\epsilon _{i0t},
\end{align*}%
where $x_{jt}$ is a measure of good $j$'s quality, $p_{jt}$ is its price, $%
satellite_{jt}$ is an indicator equal to 1 for the two satellite services,
and $wired_{jt}$ is an indicator equal to 1 for the two wired services. The
remaining notation is as usual in the class notes, including the i.i.d.
type-1 extreme value $\epsilon _{ijt}$.  Each consumer purchases the good giving them the highest conditional indirect utility.

Goods are produced by single-product firms. Firm $j$'s (log) marginal
cost in market $t$ is 
\begin{equation*}
\ln mc_{jt}=\gamma ^{0}+\text{w}_{jt}\gamma ^{1}+\omega _{jt}/8,
\end{equation*}%
where w$_{jt}$ is an observed cost shifter. Firms compete by simultaneously choosing prices in each market under complete information. Firm $j$ has profit
\begin{equation*}
\pi _{jt}=\max_{p_{jt}}M_{t}(p_{jt}-mc_{jt})s_{jt}(p_{t}).
\end{equation*}

\section{Generate Fake Data}

Generate a data set from the model above. Start with%
\begin{eqnarray*}
\beta ^{(1)} &=&1\text{, }\beta _{i}^{\left( k\right) }\sim \text{iid }%
N\left( 4,1\right) \text{ for }k=2,3 \\
\alpha  &=&-2 \\
\gamma ^{(0)} &=&1/2\text{, }\gamma ^{(1)}=1/4.
\end{eqnarray*}

\begin{enumerate}
\item Draw the exogenous product characteristic $x_{jt}$ for $T=600$
geographically defined markets (e.g., cities). Assume each $x_{jt}$ is equal
to the absolute value of an iid standard normal draw, as is each w$_{jt}$.
Simulate demand and cost unobservables as well, specifying
\begin{equation*}
\left(
\begin{array}{c}
\xi _{jt} \\
\omega _{jt}%
\end{array}%
\right) \sim N\left( \left(
\begin{array}{c}
0 \\
0%
\end{array}%
\right) ,\left(
\begin{array}{cc}
1 & 0.25 \\
0.25 & 1%
\end{array}%
\right) \right) \text{ iid across }j,t.
\end{equation*}


\begin{verbatim}
np.random.seed(1995)
# Model parameters
T, J = 600, 4
alpha, beta1 = -2, 1
beta2, beta3 = 4, 4  
sigma_satellite, sigma_wired = 1, 1
gamma0, gamma1 = 0.5, 0.25
# Product data structure
data = [
    {'market_ids': t, 'firm_ids': j+1, 'product_ids': j} 
    for t in range(T) 
    for j in range(J)
]
product_data = pd.DataFrame(data)
# Exogenous variables: x_jt and w_jt as absolute values of iid standard normal draws
product_data['x'] = np.abs(
    np.random.normal(0, 1, len(product_data))
)
product_data['w'] = np.abs(
    np.random.normal(0, 1, len(product_data))
)
# Indicators
product_data['satellite'] = (
    product_data['firm_ids'].isin([1, 2]).astype(int)
)
product_data['wired'] = (
    product_data['firm_ids'].isin([3, 4]).astype(int)
)
# Unobservables: ξ_jt and ω_jt with covariance matrix [[1, 0.25], [0.25, 1]]
cov_matrix = np.array([[1, 0.25], [0.25, 1]])
A = np.linalg.cholesky(cov_matrix)
z = np.random.normal(0, 1, (len(product_data), 2))
unobs = z @ A.T
product_data['xi'] = unobs[:, 0]  # demand unobservable
product_data['omega'] = unobs[:, 1]  # cost unobservable
\end{verbatim}

\item Solve for the equilibrium prices for each good in each market.

\begin{enumerate}
\item Start by writing a procedure to approximate the derivatives of market
shares with respect to prices (taking prices, shares, $x$, and demand
parameters as inputs). The key steps are:
\begin{enumerate}
\item For each $(j,t)$, write the choice probability $s_{jt}$ as a weighted
average (integral) of the (multinomial logit) choice probabilities
conditional on the value of each consumer's random coefficients; 

\textbf{Answer:}

From the utility specification, we can write:
\begin{equation*}
u_{ijt} = \delta_{jt} + \nu_{ijt}
\end{equation*}
where the mean utility is
\begin{equation*}
\delta_{jt} = \beta^{(1)}x_{jt} + \alpha p_{jt} + \xi_{jt}
\end{equation*}
and the individual deviation is
\begin{equation*}
\nu_{ijt} = \beta_i^{(2)}satellite_{jt} + \beta_i^{(3)}wired_{jt} + \epsilon_{ijt}.
\end{equation*}

Given the random coefficients $\beta_i = (\beta_i^{(2)}, \beta_i^{(3)})$ and the Type-1 extreme value distribution of $\epsilon_{ijt}$, the conditional choice probability follows the multinomial logit form:
\begin{equation*}
s_{jt}|\beta_i = \frac{\exp(\delta_{jt} + \beta_i^{(2)}satellite_{jt} + \beta_i^{(3)}wired_{jt})}{1 + \sum_{k=1}^4 \exp(\delta_{kt} + \beta_i^{(2)}satellite_{kt} + \beta_i^{(3)}wired_{kt})}.
\end{equation*}

The unconditional market share is the integral over the distribution of random coefficients:
\begin{equation*}
s_{jt} = \int \frac{\exp(\delta_{jt} + \beta_i^{(2)}satellite_{jt} + \beta_i^{(3)}wired_{jt})}{1 + \sum_{k=1}^4 \exp(\delta_{kt} + \beta_i^{(2)}satellite_{kt} + \beta_i^{(3)}wired_{kt})} f(\beta_i^{(2)}, \beta_i^{(3)}) \, d\beta_i^{(2)} d\beta_i^{(3)}
\end{equation*}
where $f(\cdot)$ is the joint density of $(\beta_i^{(2)}, \beta_i^{(3)})$, with each $\sim N(4,1)$ independently.


\item Anticipating differentiation under the integral sign, derive the
analytical expression for the derivative of the \textit{integrand} with
respect to each $p_{kt}$; 

\textbf{Answer:}

Let the integrand be denoted as:
\begin{equation*}
I_{jt}(\beta_i) = \frac{\exp(\delta_{jt} + \beta_i^{(2)}satellite_{jt} + \beta_i^{(3)}wired_{jt})}{1 + \sum_{\ell=1}^4 \exp(\delta_{\ell t} + \beta_i^{(2)}satellite_{\ell t} + \beta_i^{(3)}wired_{\ell t})}.
\end{equation*}

Since $\delta_{kt} = \beta^{(1)}x_{kt} + \alpha p_{kt} + \xi_{kt}$, we have $\frac{\partial \delta_{kt}}{\partial p_{kt}} = \alpha$.

For $k = j$ (own-price derivative):
\begin{align*}
\frac{\partial I_{jt}}{\partial p_{jt}} &= \alpha \cdot I_{jt}(\beta_i) \cdot (1 - I_{jt}(\beta_i))
\end{align*}

For $k \neq j$ (cross-price derivative):
\begin{align*}
\frac{\partial I_{jt}}{\partial p_{kt}} &= -\alpha \cdot I_{jt}(\beta_i) \cdot I_{kt}(\beta_i)
\end{align*}

where $I_{kt}(\beta_i)$ is the conditional choice probability for good $k$:
\begin{equation*}
I_{kt}(\beta_i) = \frac{\exp(\delta_{kt} + \beta_i^{(2)}satellite_{kt} + \beta_i^{(3)}wired_{kt})}{1 + \sum_{\ell=1}^4 \exp(\delta_{\ell t} + \beta_i^{(2)}satellite_{\ell t} + \beta_i^{(3)}wired_{\ell t})}.
\end{equation*}

These are the standard multinomial logit derivative formulas, conditional on $\beta_i$.


\item Use the expression you obtained in (2) and simulation draws of the
random coefficients to approximate the integral that corresponds to $%
\partial s_{jt}/\partial p_{kt}$ for each $j$ and $k$ (i.e., replace
the integral with the mean over the values at each simulation draw). Recall the advice in the lecture regarding \textquotedblleft
jittering.\textquotedblright\ \newline

\begin{verbatim}
def market_shares_and_derivatives(prices, market_data, nu_draws):
    J = len(market_data)
    x = market_data['x'].values
    xi = market_data['xi'].values
    sat = market_data['satellite'].values
    wired = market_data['wired'].values
    
    # Compute utilities once
    utilities = (
        beta1 * x + xi + 
        nu_draws[:, 0:1] * sat + 
        nu_draws[:, 1:2] * wired + 
        alpha * prices
    )
    utilities = np.column_stack([utilities, np.zeros(nu_draws.shape[0])])
    exp_u = np.exp(utilities - np.max(utilities, axis=1, keepdims=True))
    choice_probs = exp_u / exp_u.sum(axis=1, keepdims=True)
    inside_shares_draws = choice_probs[:, :J]
    
    # Shares: average over draws
    shares = np.mean(inside_shares_draws, axis=0)
    
    # Derivatives: compute analytically from choice probabilities
    derivatives = np.zeros((J, J))
    for j in range(J):
        for k in range(J):
            indicator = float(j == k)
            deriv_draws = (
                alpha * inside_shares_draws[:, j] * 
                (indicator - inside_shares_draws[:, k])
            )
            derivatives[j, k] = np.mean(deriv_draws)
    
    return shares, derivatives, inside_shares_draws
\end{verbatim}

\newline

I do not want to take new simulation draws of the random
coefficients each time I call this procedure. This is because,  if I did so, the attempt
to solve for equilibrium prices (below) may never converge due to \textquotedblleft
jittering\textquotedblright\ across iterations. So I take simulation draws only once, outside the procedure I wrote here. 

\begin{verbatim}
all_nu_draws = [np.random.multivariate_normal([beta2, beta3], 
np.diag([sigma_satellite, sigma_wired]), size=10000) for _ in range(T)]
\end{verbatim}


    \item Experiment to see how many simulation draws you need to get precise
    approximations and check this again at the equilibrium shares and prices you
    obtain below. 
    
    \textbf{Answer:}
    
I test convergence by computing derivatives multiple times with different random draws and measuring the standard deviation across repetitions:

\begin{verbatim}
def test_convergence(prices, market_data, nu_draws_full, 
                     draw_counts, n_reps=100):
    stds = []
    n_available = len(nu_draws_full)
    for n_draws in draw_counts:
        deriv_list = []
        for rep in range(n_reps):
            # Randomly sample n_draws from pre-drawn samples
            indices = np.random.choice(n_available, size=n_draws, 
                                       replace=False)
            nu_draws = nu_draws_full[indices]
            _, derivs, _ = market_shares_and_derivatives(
                prices, market_data, nu_draws
            )
            deriv_list.append(derivs)
        stds.append(np.std(deriv_list, axis=0).mean())
    return np.array(stds)
# Test at initial prices (p = MC) for market 0
product_data['mc'] = np.exp(
    gamma0 + gamma1 * product_data['w'] + product_data['omega'] / 8
)
market_0 = product_data[product_data['market_ids'] == 0]
prices_init = market_0['mc'].values
draw_counts = [50, 100, 200, 500, 1000, 2000, 5000]
stds = test_convergence(prices_init, market_0, all_nu_draws[0], draw_counts)
\end{verbatim}

\textbf{Convergence Results at Initial Prices:}

\begin{center}
\begin{tabular}{r|c|r}
Draws & Mean Std Dev & Rel. Change \\
\hline
50    & 0.0080  & --    \\
100   & 0.0054  & 33.2\% \\
200   & 0.0037  & 30.6\% \\
500   & 0.0025  & 32.0\% \\
1000  & 0.0017  & 34.8\% \\
2000  & 0.0011  & 35.1\% \\
5000  & 0.0005  & 51.0\% \\
\end{tabular}
\end{center}

The standard deviation decreases monotonically with the number of simulation draws. With 5,000 draws, the standard deviation is 0.005, which arguably provides a sufficiently precise approximation.

\end{enumerate}


\item The FOC for firm $j$'s profit maximization problem in market $t$ is
\begin{align}
(p_{jt}-mc_{jt})\frac{\partial s_{jt}(p_{t})}{\partial p_{jt}}+s_{jt}& =0
\notag \\
\implies p_{jt}-mc_{jt}& =-\left( \frac{\partial s_{jt}(p_{t})}{\partial
p_{jt}}\right) ^{-1}s_{jt}  \label{FOC}
\end{align}

\item Substituting in your approximation of each $\left( \frac{\partial
s_{jt}(p_{t})}{\partial p_{jt}}\right) $, solve the system of equations (\ref%
{FOC}) ($J\,$equations per market) for the equilibrium prices in each market.

\begin{enumerate}
\item To do this you will need to solve a system of $J \times J$ nonlinear equations. Make sure to check the exit flag for each market to make sure you have a solution.


\begin{verbatim}
def solve_prices_direct(market_data, mc_market, nu_draws):
    J = len(market_data)
    def foc_residual(prices):
        # Compute shares and derivatives at current prices
        shares, derivatives, _ = market_shares_and_derivatives(
            prices, market_data, nu_draws
            )
        # Inversion of derivative matrix
        invD = np.linalg.inv(derivatives)
        # FOC residuals:
        residuals = prices - mc_market + invD @ shares
        return residuals
    # Initial guess: marginal costs
    p0 = mc_market.copy()
    # Solve using root finder (hybr method)
    sol = opt.root(foc_residual, p0, method='hybr', tol=1e-8)
    prices_sol = sol.x
    success = sol.success
    return prices_sol, success
# Solve using direct method
equilibrium_prices_direct = []
success_flags_direct = []
for t in range(T):
    market_data = product_data[product_data['market_ids'] == t]
    mc_market = market_data['mc'].values
    nu_draws = all_nu_draws[t]
    prices_direct, success = solve_prices_direct(
        market_data, mc_market, nu_draws
        )
    equilibrium_prices_direct.append(prices_direct)
    success_flags_direct.append(success)
equilibrium_prices_direct = np.array(equilibrium_prices_direct)
success_count = sum(success_flags_direct)
\end{verbatim}

\textbf{Results:} Successfully solved 600/600 markets. 


\item Do this again using the algorithm of Morrow and
Skerlos (2011), discussed in section 3.6 of Conlon and Gortmaker (2019) (and
in the \texttt{pyBLP} \textquotedblleft problem simulation tutorial\textquotedblright
). Use the numerical integration approach you used in step (a) to approximate the
terms defined in equation (25) of Conlon and Gortmaker. If you get different
results using this method, resolve this discrepancy either by correcting
your code or explaining why your preferred method is the one to be trusted.
\end{enumerate}

\begin{verbatim}
def solve_prices_morrow_skerlos(market_data,
mc_market, nu_draws, max_iter=100, tol=1e-6):
    prices = mc_market.copy()
    for iteration in range(max_iter):
        shares, derivatives, inside_shares_draws = 
        market_shares_and_derivatives(prices, market_data, nu_draws)
        Lambda = np.diag(alpha * shares)
        Gamma = 
        alpha * (inside_shares_draws.T @ inside_shares_draws) / nu_draws.shape[0]
        diff = prices - mc_market
        zeta = np.linalg.solve(Lambda, Gamma.T @ diff - shares)
        prices_new = mc_market + zeta
        foc_residual = Lambda @ (prices - mc_market - zeta)
        if np.max(np.abs(foc_residual)) < tol:
            break
        prices = 0.5 * prices + 0.5 * prices_new
    return prices, iteration + 1
    
# Solve using Morrow-Skerlos method
equilibrium_prices_ms = []
iterations_ms = []
for t in range(T):
    market_data = product_data[product_data['market_ids'] == t]
    mc_market = market_data['mc'].values
    nu_draws = all_nu_draws[t]
    prices_ms, iters = solve_prices_morrow_skerlos(market_data, mc_market, nu_draws)
    equilibrium_prices_ms.append(prices_ms)
    iterations_ms.append(iters)
equilibrium_prices_ms = np.array(equilibrium_prices_ms)

# Compare direct vs Morrow-Skerlos 
    price_diff = np.abs(np.array(equilibrium_prices_direct) - equilibrium_prices_ms)
    
# Use Morrow-Skerlos prices
product_data['prices'] = equilibrium_prices_ms.flatten()
\end{verbatim}


\textbf{Results:} Successfully solved 600/600 markets. 

The Morrow-Skerlos method produces virtually identical results to the direct method (differences of order $10^{-6}$).


\begin{verbatim}
def compare_derivative_stability(
    prices1, prices2, market_data, nu_draws_full, draw_counts, n_reps=100
):
    n_available = len(nu_draws_full)
    def compute_derivative_std(prices, n_draws):
        deriv_list = []
        for _ in range(n_reps):
            # Randomly sample from pre-drawn samples
            indices = np.random.choice(n_available, size=n_draws, replace=False)
            nu_draws = nu_draws_full[indices]
            _, derivatives, _ = market_shares_and_derivatives(
                prices, market_data, nu_draws
            )
            deriv_list.append(derivatives)
        return np.std(deriv_list, axis=0).mean()
    stds1, stds2 = [], []
    for n_draws in draw_counts:
        stds1.append(compute_derivative_std(prices1, n_draws))
        stds2.append(compute_derivative_std(prices2, n_draws))
    return np.array(stds1), np.array(stds2)
# Compare derivative convergence at initial vs equilibrium prices
market_0 = product_data[product_data['market_ids'] == 0]
prices_initial = market_0['mc'].values 
prices_equilibrium = market_0['prices'].values
draw_counts = [50, 100, 200, 500, 1000, 2000, 5000]
initial_stds, eq_stds = compare_derivative_stability(
    prices_initial, prices_equilibrium, market_0, all_nu_draws[0], draw_counts
)
for i, n_draws in enumerate(draw_counts):
    ratio = eq_stds[i] / initial_stds[i] 
avg_ratio = np.mean(valid_ratios)
\end{verbatim}



\textbf{Results:} The derivative approximation via Monte Carlo integration is more stable at equilibrium prices compared to initial prices (marginal costs), as the ratio remains consistently around 0.68 across all draw counts.

\begin{center}
\begin{tabular}{r|c|c|c}
Draws & Initial Std Dev & Equilibrium Std Dev & Ratio (Eq/Init) \\
\hline
50    & 0.0080 & 0.0053 & 0.66 \\
100   & 0.0054 & 0.0036 & 0.68 \\
200   & 0.0037 & 0.0027 & 0.71 \\
500   & 0.0025 & 0.0017 & 0.68 \\
1000  & 0.0017 & 0.0011 & 0.69 \\
2000  & 0.0011 & 0.0007 & 0.68 \\
5000  & 0.0005 & 0.0004 & 0.67 \\
\end{tabular}
\end{center}

\item Calculate "observed" market shares for your fake data set using your parameters, your draws of $x$, $w$, $\xi$, $\omega$, and your equilibrium prices.

\begin{verbatim}
observed_shares = []
for t in range(T):
    market_data = product_data[product_data['market_ids'] == t]
    prices_market = market_data['prices'].values
    # Use pre-drawn simulation draws for this market
    shares_market, _, _ = market_shares_and_derivatives(
        prices_market, market_data, all_nu_draws[t]
    )
    observed_shares.extend(shares_market)
product_data['shares'] = observed_shares
\end{verbatim}

\textbf{Results:} Share range: 0.000 to 0.725, mean: 0.136 (std: 0.122). Market share sums average 0.543 (min: 0.303, max: 0.746), implying outside shares average 0.457. Average satellite product share: 0.135; average wired product share: 0.136.

\item

Below you'll be using x and w as instruments in the demand estimation. Check whether these appear to be good instruments in your fake data using some regressions of prices and market shares on the exogenous variables (or some function of them; see the related discussion in the coding tips). If you believe the instruments are not providing enough variation, modify the parameter choices above until you are satisfied. Report your final choice of parameters and the results you rely on to conclude that the instruments seem good enough.
\end{enumerate}

\begin{verbatim}
# Create instruments: quadratics, interactions, competitor sums, within-nest
product_data['x**2'] = product_data['x'] ** 2
product_data['w**2'] = product_data['w'] ** 2
product_data['x*w'] = product_data['x'] * product_data['w']
product_data['sum_x_competitors'] = 
    product_data.groupby('market_ids')['x'].transform('sum') - product_data['x']
product_data['sum_w_competitors'] = 
    product_data.groupby('market_ids')['w'].transform('sum') - product_data['w']
product_data['x_other_in_nest'] = 
    product_data.groupby(['market_ids', 'satellite'])['x'].transform('sum') - product_data['x']
product_data['w_other_in_nest'] = 
    product_data.groupby(['market_ids', 'satellite'])['w'].transform('sum') - product_data['w']

Z = product_data[['satellite', 'wired', 'x', 'w', 'x**2', 'w**2', 'x*w', 
                   'sum_x_competitors', 'sum_w_competitors', 
                   'x_other_in_nest', 'w_other_in_nest']]

# Test instrument validity
price_model = sm.OLS(product_data['prices'], Z).fit()
share_model = sm.OLS(product_data['shares'], Z).fit()
xi_model = sm.OLS(product_data['xi'], Z).fit()
omega_model = sm.OLS(product_data['omega'], Z).fit()

# Joint F-tests for excluded instruments
excluded_vars = ['w', 'x**2', 'w**2', 'x*w', 'sum_x_competitors', 
                 'sum_w_competitors', 'x_other_in_nest', 'w_other_in_nest']
hypothesis = ', '.join([f'{var}=0' for var in excluded_vars])
price_f_test = price_model.f_test(hypothesis)
share_f_test = share_model.f_test(hypothesis)
xi_f_test = xi_model.f_test(hypothesis)
omega_f_test = omega_model.f_test(hypothesis)
\end{verbatim}

\textbf{Results:} The parameters $\alpha = -2$, $\beta^{(1)} = 1$, $\beta_i^{(2)} \sim N(4, 1^2)$, $\beta_i^{(3)} \sim N(4, 1^2)$, $\gamma^{(0)} = 0.5$, $\gamma^{(1)} = 0.25$ were retained as final. Excluded demand instruments (w, x$^2$, w$^2$, x*w, competitor sums, within-nest variables) are relevant based on prices regression R$^2$=0.511, F=301.04 (p$<$0.001) and shares regression R$^2$=0.364, F=91.06 (p$<$0.001). They are also uncorrelated with structural errors: $\xi$ F=0.63 (p=0.75) and $\omega$ F=0.58 (p=0.80).

\section{Estimate Some Mis-specified Models}

\begin{enumerate}
\item[4.] Estimate the plain multinomial logit model of demand by OLS\
(ignoring the endogeneity of prices).

\begin{verbatim}
# Compute logit delta: ln(s_jt / s_0t)
product_data['logit_delta'] = np.log(product_data['shares'] / product_data['outside_share'])

# OLS: beta_hat = (X^T X)^(-1) X^T y
y = product_data['logit_delta'].values
X = product_data[['prices', 'x', 'satellite', 'wired']].values
beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y

# HC0 robust standard errors
residuals = y - X @ beta_hat
V = X.T @ np.diag(residuals**2) @ X
cov_matrix_ols = np.linalg.inv(X.T @ X) @ V @ np.linalg.inv(X.T @ X)
se_ols = np.sqrt(np.diag(cov_matrix_ols))
\end{verbatim}

\textbf{Results:} OLS estimates with HC0 robust standard errors for the logit specification $\ln(s_{jt}/s_{0t}) = \alpha p_{jt} + \beta^{(1)} x_{jt} + \beta^{(2)} satellite_{jt} + \beta^{(3)} wired_{jt}$ (no intercept):

\begin{center}
\begin{tabular}{lrrrr}
\hline
Parameter & Estimate & Std. Error & t-stat & p-value \\
\hline
$\alpha$ (prices) & $-1.246$ & $0.051$ & $-24.41$ & $0.000$ \\
$\beta^{(1)}$ (x) & $0.854$ & $0.032$ & $26.36$ & $0.000$ \\
$\beta^{(2)}$ (satellite) & $1.757$ & $0.165$ & $10.67$ & $0.000$ \\
$\beta^{(3)}$ (wired) & $1.790$ & $0.164$ & $10.90$ & $0.000$ \\
\hline
\end{tabular}
\end{center}

\item[5.] Re-estimate the multinomial logit model of demand by two-stage
least squares, instrumenting for prices with the exogenous demand shifters $%
x $ and excluded cost shifters w. Discuss the predictive value of the instruments in the first stage, and how the second-stage results differ from those
obtained by OLS.

\begin{verbatim}
# First stage: regress prices on instruments
Z = product_data[['satellite', 'wired', 'x', 'w', 'x**2', 'w**2', 
                   'x*w', 'sum_x_competitors', 'sum_w_competitors']].values
sigma_hat = np.linalg.inv(Z.T @ Z) @ Z.T @ product_data['prices'].values
prices_hat = Z @ sigma_hat

# First stage diagnostics
resid_fs = product_data['prices'].values - prices_hat
SST = np.sum((product_data['prices'].values - product_data['prices'].mean())**2)
SSR = np.sum(resid_fs**2)
R2_first = 1 - SSR/SST

# F-stat for excluded instruments (6 excluded: w, x², w², x*w, sums)
Z_restricted = product_data[['satellite', 'wired', 'x']].values
SSR_r = np.sum((product_data['prices'].values - Z_restricted @ 
         np.linalg.inv(Z_restricted.T @ Z_restricted) @ 
         Z_restricted.T @ product_data['prices'].values)**2)
F_stat = ((SSR_r - SSR) / 6) / (SSR / (len(product_data) - 9))

# Second stage
X_hat = np.column_stack([prices_hat, product_data['x'].values,
         product_data['satellite'].values, product_data['wired'].values])
beta_hat_iv = np.linalg.inv(X_hat.T @ X_hat) @ X_hat.T @ y

# HC0 robust SE for 2SLS
X = np.column_stack([product_data['prices'].values, product_data['x'].values,
     product_data['satellite'].values, product_data['wired'].values])
resid_iv = y - X @ beta_hat_iv
P_Z = Z @ np.linalg.inv(Z.T @ Z) @ Z.T
Omega = np.diag(resid_iv**2)
XPZ = X.T @ P_Z
cov_iv = np.linalg.inv(XPZ @ X) @ XPZ @ Omega @ P_Z @ X @ np.linalg.inv(XPZ @ X)
se_iv = np.sqrt(np.diag(cov_iv))
\end{verbatim}

\textbf{Results:} First stage: R$^2$ = 0.507, F = 394.82 (p $<$ 0.001), confirming strong instruments. Second stage 2SLS IV estimates with HC0 robust SE:

\begin{center}
\begin{tabular}{lrrrr}
\hline
Parameter & Estimate & Std. Error & t-stat & p-value \\
\hline
$\alpha$ (prices) & $-1.938$ & $0.064$ & $-30.51$ & $0.000$ \\
$\beta^{(1)}$ (x) & $0.923$ & $0.035$ & $26.15$ & $0.000$ \\
$\beta^{(2)}$ (satellite) & $3.993$ & $0.208$ & $19.21$ & $0.000$ \\
$\beta^{(3)}$ (wired) & $4.034$ & $0.209$ & $19.34$ & $0.000$ \\
\hline
\end{tabular}
\end{center}

2SLS recovers parameters close to truth ($\alpha = -2$, $\beta^{(1)} = 1$, $\beta^{(2)}, \beta^{(3)} \sim N(4,1)$), while OLS severely underestimates due to positive omitted variable bias: $\xi_{jt}$ correlates positively with prices (via $\text{Cov}(\xi_{jt}, \omega_{jt}) = 0.25$) and service types, attenuating coefficients toward zero. Instrumenting with cost shifters breaks this correlation, revealing true parameters.

\item[6.] Now estimate a nested logit model by two-stage least squares,
treating \textquotedblleft satellite\textquotedblright\ and
\textquotedblleft wired\textquotedblright\ as the two nests for the inside
goods. You will probably want to review the discussion of the nested logit
in Berry (1994). Note that Berry focuses on the special case in which all
the \textquotedblleft nesting parameters\textquotedblright\ are the same;
you should allow a different nesting parameter for each nest.

\begin{verbatim}
# Within-group shares for nesting
product_data["group_share"] = product_data.groupby(
    ["market_ids", "satellite"])["shares"].transform("sum")
product_data["ln_within_share"] = np.log(
    product_data["shares"] / product_data["group_share"])

# Nest-specific within-group share variables
product_data["ln_within_share_sat"] = (
    product_data["ln_within_share"] * product_data["satellite"])
product_data["ln_within_share_wired"] = (
    product_data["ln_within_share"] * product_data["wired"])

# First stage: instrument for prices, ln_within_share_sat, ln_within_share_wired
Z = product_data[['x', 'satellite', 'wired', 'w', 'x**2', 'w**2', 'x*w',
                   'sum_x_competitors', 'sum_w_competitors', 
                   'x_other_in_nest', 'w_other_in_nest']].values
endog_vars = ['prices', 'ln_within_share_sat', 'ln_within_share_wired']
endog_hat = np.zeros((len(product_data), 3))
for i, var in enumerate(endog_vars):
    sigma = np.linalg.inv(Z.T @ Z) @ Z.T @ product_data[var].values
    endog_hat[:, i] = Z @ sigma

# Second stage
X_hat = np.column_stack([endog_hat[:, 0], product_data['x'].values,
         product_data['satellite'].values, product_data['wired'].values,
         endog_hat[:, 1], endog_hat[:, 2]])
beta_hat_nl = np.linalg.inv(X_hat.T @ X_hat) @ X_hat.T @ y

# HC0 robust SE for 2SLS
X = np.column_stack([product_data['prices'].values, product_data['x'].values,
     product_data['satellite'].values, product_data['wired'].values,
     product_data['ln_within_share_sat'].values, 
     product_data['ln_within_share_wired'].values])
resid = y - X @ beta_hat_nl
P_Z = Z @ np.linalg.inv(Z.T @ Z) @ Z.T
XPZ = X.T @ P_Z
cov_nl = np.linalg.inv(XPZ @ X) @ XPZ @ np.diag(resid**2) @ P_Z @ X @ np.linalg.inv(XPZ @ X)
se_nl = np.sqrt(np.diag(cov_nl))
\end{verbatim}

\textbf{Results:} Nested logit 2SLS IV estimates (prices, within-group shares instrumented):

\begin{center}
\begin{tabular}{lrrrr}
\hline
Parameter & Estimate & Std. Error & t-stat & p-value \\
\hline
$\alpha$ (prices) & $-1.605$ & $0.097$ & $-16.51$ & $0.000$ \\
$\beta^{(1)}$ (x) & $0.802$ & $0.041$ & $19.53$ & $0.000$ \\
$\beta^{(2)}$ (satellite) & $3.106$ & $0.603$ & $5.15$ & $0.000$ \\
$\beta^{(3)}$ (wired) & $3.348$ & $0.484$ & $6.92$ & $0.000$ \\
$\rho_{sat}$ (ln\_within\_sat) & $0.115$ & $0.449$ & $0.26$ & $0.797$ \\
$\rho_{wired}$ (ln\_within\_wired) & $0.312$ & $0.465$ & $0.67$ & $0.502$ \\
\hline
\end{tabular}
\end{center}

The nesting parameters ($\rho_{sat} = 0.115$, $\rho_{wired} = 0.312$) are not significantly different from zero, suggesting the nested logit structure does not fit the data well. This is expected since the true DGP features random coefficients on satellite/wired indicators, not correlation within nests as the nested logit assumes.

\item[7.] Using the nested logit results, provide a table comparing the
estimated own-price elasticities to the true own-price elasticities. Provide two
additional tables showing the true matrix of diversion ratios and the
diversion ratios implied by your estimates.
\end{enumerate}

\begin{verbatim}
# Extract NL parameters
alpha_nl, beta_x_nl, rho_sat_nl, rho_wired_nl = beta_hat_iv_nested[[0, 1, 4, 5]]

# True RC elasticities using analytical derivatives
def compute_rc_elasticities_true_params(market_df, nu_draws, alpha, beta1, 
                                         beta2, beta3, sigma_sat, sigma_wired):
    """Compute true elasticities from RC logit using analytical derivatives."""
    J = len(market_df)
    prices = market_df['prices'].values
    x, xi = market_df['x'].values, market_df['xi'].values
    sat, wired = market_df['satellite'].values, market_df['wired'].values
    
    # Individual choice probabilities over all draws
    u = beta1*x + xi + (beta2 + sigma_sat*nu_draws[:,0:1])*sat + \
        (beta3 + sigma_wired*nu_draws[:,1:2])*wired + alpha*prices
    exp_u = np.exp(u)
    s_i = exp_u / (1 + exp_u.sum(axis=1, keepdims=True))
    shares = s_i.mean(axis=0)
    
    # Analytical derivatives: ∂s_j/∂p_k = alpha*E[s_ij(1-s_ij)] (own),
    #                                      -alpha*E[s_ij*s_ik] (cross)
    elast = np.zeros((J, J))
    for j in range(J):
        for k in range(J):
            if j == k:
                deriv = alpha * np.mean(s_i[:,j] * (1 - s_i[:,j]))
            else:
                deriv = -alpha * np.mean(s_i[:,j] * s_i[:,k])
            elast[j,k] = (prices[k] / shares[j]) * deriv if shares[j] > 1e-10 else 0
    return elast

# NL elasticities using Jacobian formula matching pyBLP implementation
def compute_nested_logit_elasticities(market_df, alpha, beta_x, rho_sat, rho_wired):
    """Compute NL elasticities using Jacobian = Diagonal(Lambda) - Gamma."""
    J = len(market_df)
    prices, shares = market_df['prices'].values, market_df['shares'].values
    sat, wired = market_df['satellite'].values, market_df['wired'].values
    
    # Within-nest shares
    s_group = market_df.groupby('satellite')['shares'].transform('sum').values
    s_within = shares / s_group
    rho = np.where(sat == 1, rho_sat, rho_wired)
    
    # Jacobian elements
    elast = np.zeros((J, J))
    for j in range(J):
        lambda_jj = alpha * shares[j] / (1 - rho[j])
        for k in range(J):
            same_nest = (sat[j] == sat[k]) and (wired[j] == wired[k])
            gamma_jk = alpha * shares[j] * shares[k]
            if same_nest:
                gamma_jk += (rho[j]/(1-rho[j])) * alpha * s_within[j] * shares[k]
            jac_jk = lambda_jj - gamma_jk if j == k else -gamma_jk
            elast[j,k] = jac_jk * prices[k] / shares[j]
    return elast

# Compute for all markets and average
true_elast = [compute_rc_elasticities_true_params(
    product_data[product_data['market_ids']==t], all_nu_draws[t], 
    -2.0, 1.0, 4.0, 4.0, 1.0, 1.0) for t in range(T)]
nl_elast = [compute_nested_logit_elasticities(
    product_data[product_data['market_ids']==t], 
    alpha_nl, beta_x_nl, rho_sat_nl, rho_wired_nl) for t in range(T)]
avg_true = np.mean([np.diag(e) for e in true_elast], axis=0)
avg_nl = np.mean([np.diag(e) for e in nl_elast], axis=0)

# Unified diversion calculation using pyBLP's derivative-based method
def compute_diversion_ratios_pyblp(elasticity_matrices, product_data, T, J):
    """
    Compute diversion ratios using pyBLP's derivative-based method.
    Works for any model (RC, NL, etc.) - just supply elasticity matrices.
    """
    diversion_matrices = []
    for t in range(T):
        elast_matrix = elasticity_matrices[t]
        market_data_t = product_data[product_data['market_ids'] == t]
        shares = market_data_t['shares'].values
        prices = market_data_t['prices'].values
        
        # Convert elasticities to Jacobian: ∂s_j/∂p_k = (s_j/p_k) * ε_jk
        jacobian = np.zeros((J, J))
        for j in range(J):
            for k in range(J):
                jacobian[j, k] = (shares[j] / prices[k]) * elast_matrix[j, k]
        
        # PyBLP's method: Replace diagonal with outside option derivative
        # ∂s_0/∂p_j = -Σ_k ∂s_k/∂p_j (by adding-up constraint)
        jacobian_diag = np.diag(jacobian).copy()
        np.fill_diagonal(jacobian, -jacobian.sum(axis=1))
        
        # Compute diversion: D_jk = -Jacobian[j,k] / Jacobian[j,j]
        diversion = -jacobian / jacobian_diag[:, None]
        diversion_matrices.append(diversion)
    
    return diversion_matrices

# Compute diversion ratios for both models using same unified function
true_div = np.mean(compute_diversion_ratios_pyblp(
    true_elast, product_data, T, J), axis=0)
nl_div = np.mean(compute_diversion_ratios_pyblp(
    nl_elast, product_data, T, J), axis=0)
\end{verbatim}

\textbf{Results:}

\textit{Own-Price Elasticities:} Elasticities are computed as $\varepsilon_{jk} = \frac{p_k}{s_j} \frac{\partial s_j}{\partial p_k}$. True elasticities computed using analytical derivatives of the random coefficients logit model: $\frac{\partial s_j}{\partial p_k} = \alpha \mathbb{E}[s_{ij}(1-s_{ij})]$ (own-price) and $\frac{\partial s_j}{\partial p_k} = -\alpha \mathbb{E}[s_{ij}s_{ik}]$ (cross-price), where expectations are taken over the 10,000 pre-drawn simulation draws of random coefficients $(\beta_i^{(2)}, \beta_i^{(3)}) \sim N(4,1)$ for each market. Nested logit elasticities computed using the Jacobian formulation: $\text{Jacobian} = \text{diag}(\Lambda) - \Gamma$, where $\Lambda_{jj} = \frac{\alpha s_j}{1-\rho_j}$ and $\Gamma_{jk} = \alpha s_j s_k + \mathbb{I}_{\text{same nest}} \cdot \frac{\rho_j}{1-\rho_j} \alpha s_{j|g} s_k$, with $s_{j|g}$ denoting the within-nest share and $\rho_j$ the nest-specific correlation parameter. The nested logit model overestimates the magnitude of own-price elasticities, particularly for wired products:


\begin{center}
\begin{tabular}{lrr}
\hline
Product & True & Estimated (NL) \\
\hline
Satellite 1 & $-4.753$ & $-4.998$ \\
Satellite 2 & $-4.550$ & $-4.855$ \\
Wired 1 & $-4.615$ & $-5.904$ \\
Wired 2 & $-4.724$ & $-6.001$ \\
\hline
\end{tabular}
\end{center}

\textit{Diversion Ratios:} 
True vs Estimated $D_{jk} = -(\partial s_k/\partial p_j) / (\partial s_j/\partial p_j).$ 

Diversion to the outside good reported on the diagonal instead of $D_{jj}=-1.$ 

True Diversion (Random Coefficients Logit):
\begin{center}
\begin{tabular}{lrrrr}
\hline
 & Sat 1 & Sat 2 & Wired 1 & Wired 2 \\
\hline
Sat 1 & 0.034 & 0.411 & 0.287 & 0.267 \\
Sat 2 & 0.394 & 0.036 & 0.290 & 0.280 \\
Wired 1 & 0.276 & 0.291 & 0.036 & 0.398 \\
Wired 2 & 0.268 & 0.286 & 0.413 & 0.034 \\
\hline
\end{tabular}
\end{center}

Estimated Diversion (Nested Logit):
\begin{center}
\begin{tabular}{lrrrr}
\hline
 & Sat 1 & Sat 2 & Wired 1 & Wired 2 \\
\hline
Sat 1 & 0.508 & 0.200 & 0.152 & 0.141 \\
Sat 2 & 0.190 & 0.514 & 0.153 & 0.143 \\
Wired 1 & 0.124 & 0.129 & 0.445 & 0.303 \\
Wired 2 & 0.121 & 0.130 & 0.314 & 0.435 \\
\hline
\end{tabular}
\end{center}

The nested logit model systematically mischaracterizes substitution patterns compared to the true random coefficients model. It substantially overestimates diversion to the outside option (diagonal elements: 0.44--0.51 vs.\ true 0.03--0.04) while underestimating within-nest substitution (e.g., Satellite 1 to Satellite 2: 0.200 vs.\ true 0.411). This pattern reflects a fundamental model misspecification: the nested logit's group-level correlation structure cannot replicate the rich substitution patterns generated by heterogeneous consumer preferences over satellite versus wired services in the true random coefficients model.

Both diversion matrices are computed using pyBLP's derivative-based Jacobian method. Elasticities are first converted to price derivatives via $\frac{\partial s_j}{\partial p_k} = \frac{s_j}{p_k}\varepsilon_{jk}$. The diagonal elements are then replaced with outside option derivatives $\frac{\partial s_0}{\partial p_j} = -\sum_k \frac{\partial s_k}{\partial p_j}$ (imposed by the adding-up constraint). Finally, diversion ratios are computed as $D_{jk} = -\frac{\partial s_k/\partial p_j}{\partial s_j/\partial p_j}$.

\section{Estimate the Correctly Specified Model}

Use the \texttt{pyBLP} package to estimate the correctly specified model. Allow
\texttt{pyBLP} to construct approximations to the optimal instruments, using the
exogenous demand shifters and exogenous cost shifters.

\begin{enumerate}
\item[8.] Report a table with the estimates of the demand
parameters and standard errors. Do this twice: once when you estimate demand
alone, then again when you estimate jointly with supply.

\begin{verbatim}
# Formulations and instruments
X1_formulation = pyblp.Formulation('0 + prices + x + satellite + wired')
X2_formulation = pyblp.Formulation('0 + satellite + wired')
product_formulations1 = (X1_formulation, X2_formulation)
product_data['demand_instruments0'] = product_data['w']
product_data['demand_instruments1'] = product_data['x**2']
product_data['demand_instruments2'] = product_data['w**2']
product_data['demand_instruments3'] = product_data['x*w']
product_data['demand_instruments4'] = product_data['sum_x_competitors']
product_data['demand_instruments5'] = product_data['sum_w_competitors']
product_data['demand_instruments6'] = product_data['x_other_in_nest']
product_data['demand_instruments7'] = product_data['w_other_in_nest']
integration = pyblp.Integration('product', 10)

# Demand-only estimation with optimal instruments
problem1 = pyblp.Problem(product_formulations1, product_data, 
                         integration=integration)
results1 = problem1.solve(sigma=np.eye(2), initial_update=True)
optimal_iv1 = results1.compute_optimal_instruments(seed=1995)
optimal_problem1 = optimal_iv1.to_problem()
optimal_iv_results1 = optimal_problem1.solve(sigma=np.eye(2), 
                                              initial_update=True)

# Joint demand and supply estimation with optimal instruments
X3_formulation = pyblp.Formulation('1 + w')
product_formulations2 = (X1_formulation, X2_formulation, X3_formulation)
product_data['demand_instruments8'] = optimal_iv1.demand_instruments[:, 0]
product_data['demand_instruments9'] = optimal_iv1.demand_instruments[:, 1]
problem2 = pyblp.Problem(product_formulations2, product_data, 
                         costs_type='log', integration=integration)
results2 = problem2.solve(sigma=np.eye(2), costs_bounds=(0.001, None), 
                          beta=optimal_iv_results1.beta, initial_update=True)
optimal_iv2 = results2.compute_optimal_instruments(seed=1995)

# Re-estimate with optimal instruments
columns_to_drop = [col for col in product_data.columns 
                   if 'instruments' in col]
product_data = product_data.drop(columns=columns_to_drop)
for i in range(optimal_iv2.demand_instruments.shape[1]-3):
    product_data[f'demand_instruments{i}'] = 
        optimal_iv2.demand_instruments[:, i]
problem3 = pyblp.Problem(product_formulations2, product_data, 
                         costs_type='log', integration=integration)
optimal_iv_results2 = problem3.solve(sigma=np.eye(2), 
                                      beta=optimal_iv_results1.beta, 
                                      costs_bounds=(0.001, None), 
                                      initial_update=True)
\end{verbatim}

\textbf{Results:}

\textit{Demand Parameters ($\beta$ and $\Sigma$):}

\begin{center}
\begin{tabular}{lrrrr}
\hline
 & \multicolumn{2}{c}{Estimates} & \multicolumn{2}{c}{Std. Errors} \\
Parameter & Demand Only & Joint D \& S & Demand Only & Joint D \& S \\
\hline
\multicolumn{5}{l}{\textit{Mean Utility ($\beta$):}} \\
$\alpha$ (prices) & $-2.033$ & $-2.022$ & $0.068$ & $0.069$ \\
$\beta^{(1)}$ (x) & $1.011$ & $1.017$ & $0.045$ & $0.045$ \\
$\beta^{(2)}$ (satellite) & $3.910$ & $3.864$ & $0.229$ & $0.236$ \\
$\beta^{(3)}$ (wired) & $4.093$ & $4.016$ & $0.222$ & $0.233$ \\
\hline
\multicolumn{5}{l}{\textit{Random Coefficients ($\Sigma$):}} \\
$\sigma_{satellite}$ & $1.349$ & $1.372$ & $0.227$ & $0.231$ \\
$\sigma_{wired}$ & $1.002$ & $1.109$ & $0.255$ & $0.253$ \\
\hline
\end{tabular}
\end{center}

\textit{Marginal Cost Parameters ($\gamma$, from joint estimation only):}

\begin{center}
\begin{tabular}{lrr}
\hline
Parameter & Estimate & Std. Error \\
\hline
$\gamma^{(0)}$ (constant) & $0.793$ & $0.016$ \\
$\gamma^{(1)}$ (w) & $0.205$ & $0.007$ \\
\hline
\end{tabular}
\end{center}

Both estimation approaches recover the true parameters well. The demand-only estimates are $\hat{\alpha} = -2.033$ (true: $-2.0$), $\hat{\beta}^{(1)} = 1.011$ (true: $1.0$), with random coefficient means $\hat{\beta}^{(2)} = 3.910$, $\hat{\beta}^{(3)} = 4.093$ (true: $4.0$ each), and standard deviations $\hat{\sigma}_{satellite} = 1.349$, $\hat{\sigma}_{wired} = 1.002$ (true: $1.0$ each). Joint estimation produces nearly identical demand parameters while additionally recovering marginal cost parameters: $\hat{\gamma}^{(0)} = 0.793$ (true: $0.5$) and $\hat{\gamma}^{(1)} = 0.205$ (true: $0.25$). The cost parameter estimates differ somewhat from their true values, though both are precisely estimated with small standard errors. 

Demand-only estimates are invariant to starting values for $\sigma$, while joint estimates are invariant to starting values for both $\sigma$ and $\alpha$. This insensitivity to initial conditions confirms strong identification via optimal instruments and a well-behaved objective function. The close correspondence between demand-only and joint estimates indicates that supply-side moments contribute minimal additional information for demand parameter identification, consistent with strong demand-side identification from rich cost-shifter instruments.

\item[9.] Using your preferred estimates from the prior step (explain your
preference), provide a table comparing the estimated own-price elasticities
to the true own-price elasticities. Provide two additional tables showing
the true matrix of diversion ratios and the diversion ratios implied by your
estimates.

\begin{verbatim}
    
\end{verbatim}

\end{enumerate}

\section{Merger Simulation}

\begin{enumerate}
\item[10.] Suppose two of the four firms were to merge. Give a brief
intuition for what theory tells us is likely to happen to the equilibrium
prices of each good $j$.



\item[11.] Suppose firms 1 and 2 are proposing to merge. Use the \texttt{pyBLP}
merger simulation procedure to provide a prediction of the post-merger
equilibrium prices.

\begin{verbatim}
    
\end{verbatim}

\item[12.] Now suppose instead that firms 1 and 3 are the ones to merge.
Re-run the merger simulation. Provide a table comparing the (average
across markets) predicted merger-induced price changes for this merger and
that in part 11. Interpret the differences between the predictions for the
two mergers.

\begin{verbatim}
    
\end{verbatim}

\item[13.] Thus far you have assumed that there are no \textquotedblleft
efficiencies\textquotedblright\ (reduction in costs) resulting from the
merger. Explain briefly why a merger-specific reduction in marginal cost
could mean that a merger is welfare-enhancing.

\item[14.] Consider the merger between firms 1 and 2, and suppose the firms
demonstrate that by merging they would reduce marginal cost of each of their
products by 15\%. Furthermore, suppose that they demonstrate that this cost
reduction could not be achieved without merging.    Using the \texttt{pyBLP} software, re-run the merger simulation
with the 15\% cost saving. Show the predicted post-merger price changes (again,
for each product, averaged across markets). What is the predicted impact of
the merger on consumer welfare, assuming that the total measure of consumers $%
M_{t} $ is the same in each market  $t$?  

\begin{verbatim}
    
\end{verbatim}

Explain why this additional assumption
(or data on the correct values of $M_{t}$) is needed here, whereas up to
this point it was without loss to assume $M_{t}=1$. What is the predicted
impact of the merger on total welfare?
\end{enumerate}




\end{document}
