{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45ef1489",
   "metadata": {},
   "source": [
    "# Economics 600a Fall 2025 Prof. P. Haile Homework Assignment 1\n",
    "\n",
    "## 1 Overview\n",
    "You will estimate demand and supply in a stylized model of the market for pay-TV services. You will use a matrix programming language of your choice to create your own fake data set for the industry and do some relatively simple estimation. Then, using the **pyBLP** package of Conlon and Gortmaker, you will estimate the model and perform some merger simulations.\n",
    "\n",
    "The pyBLP package has excellent documentation and a very helpful tutorial (which covers merger simulation), both easy to find via Google.\n",
    "\n",
    "Please submit (on canvas) a single PDF document presenting your answers to the questions below, requested results, and well documented code. Write this up nicely, with properly formatted tables and discussion of results. You may work in groups on the coding. However, your write-ups should be your own work, and you must describe all collaboration at the beginning of your submission; this includes any use of AI.\n",
    "\n",
    "## 2 Model\n",
    "There are $T$ markets, each with four inside goods $j \\in \\{1,2,3,4\\}$ and an outside option. Goods 1 and 2 are satellite television services (e.g., DirecTV and Dish); goods 3 and 4 are wired television services (e.g., Frontier and Comcast in New Haven). The conditional indirect utility of consumer $i$ for good $j$ in market $t$ is given by\n",
    "\n",
    "\\begin{align*}\n",
    "u_{ijt} &= \\beta^{(1)} x_{jt} + \\beta_i^{(2)} satellite_{jt} + \\beta_i^{(3)} wired_{jt} + \\alpha p_{jt} + \\xi_{jt} + \\epsilon_{ijt} \\quad j > 0 \\\\\n",
    "u_{i0t} &= \\epsilon_{i0t},\n",
    "\\end{align*}\n",
    "\n",
    "where $x_{jt}$ is a measure of good $j$'s quality, $p_{jt}$ is its price, $satellite_{jt}$ is an indicator equal to 1 for the two satellite services, and $wired_{jt}$ is an indicator equal to 1 for the two wired services. The remaining notation is as usual in the class notes, including the i.i.d. type-1 extreme value $\\epsilon_{ijt}$. Each consumer purchases the good giving them the highest conditional indirect utility.\n",
    "\n",
    "Goods are produced by single-product firms. Firm $j$'s (log) marginal cost in market $t$ is\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ln mc_{jt} = \\gamma^{(0)} + w_{jt} \\gamma^{(1)} + \\omega_{jt}/8,\n",
    "\\end{equation*}\n",
    "\n",
    "where $w_{jt}$ is an observed cost shifter. Firms compete by simultaneously choosing prices in each market under complete information. Firm $j$ has profit\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pi_{jt} = \\max_{p_{jt}} (p_{jt} - mc_{jt}) s_{jt}(p_t).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee1070",
   "metadata": {},
   "source": [
    "## 3 Generate Fake Data\n",
    "\n",
    "Generate a data set from the model above. Let\n",
    "\n",
    "\\begin{align*}\n",
    "\\beta^{(1)} &= 1, \\quad \\beta_i^{(k)} \\sim \\text{iid } N(4,1) \\text{ for } k=2,3 \\\\\n",
    "\\alpha &= -2 \\\\\n",
    "\\gamma^{(0)} &= 1/2, \\quad \\gamma^{(1)} = 1/4.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8744e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "from scipy.special import logsumexp\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import time\n",
    "import IPython.display\n",
    "IPython.display.display(IPython.display.HTML('<style>pre { white-space: pre !important; }</style>'))\n",
    "import pyblp\n",
    "pyblp.options.digits = 3\n",
    "pd.options.display.precision = 3\n",
    "pd.options.display.max_columns = 50\n",
    "pyblp.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12caa47",
   "metadata": {},
   "source": [
    "### 1. \n",
    "Draw the exogenous product characteristic $x_{jt}$ for $T=600$ geographically defined markets (e.g., cities). Assume each $x_{jt}$ is equal to the absolute value of an iid standard normal draw, as is each $w_{jt}$. Simulate demand and cost unobservables as well, specifying\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\xi_{jt} \\\\\n",
    "\\omega_{jt}\n",
    "\\end{array}\n",
    "\\right) \\sim N\\left( \\left(\n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right), \\left(\n",
    "\\begin{array}{cc}\n",
    "1 & 0.25 \\\\\n",
    "0.25 & 1\n",
    "\\end{array}\n",
    "\\right) \\right) \\quad \\text{iid across } j,t.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9546c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1995)\n",
    "\n",
    "# Model parameters\n",
    "T, J = 600, 4\n",
    "alpha, beta1 = -2, 1\n",
    "beta2, beta3 = 4, 4  \n",
    "sigma_satellite, sigma_wired = 1, 1\n",
    "gamma0, gamma1 = 0.5, 0.25\n",
    "\n",
    "# Product data structure\n",
    "data = [\n",
    "    {'market_ids': t, 'firm_ids': j+1, 'product_ids': j} \n",
    "    for t in range(T) \n",
    "    for j in range(J)\n",
    "]\n",
    "product_data = pd.DataFrame(data)\n",
    "\n",
    "# Exogenous variables: x_jt and w_jt as absolute values of iid standard normal draws\n",
    "product_data['x'] = np.abs(\n",
    "    np.random.normal(0, 1, len(product_data))\n",
    ")\n",
    "product_data['w'] = np.abs(\n",
    "    np.random.normal(0, 1, len(product_data))\n",
    ")\n",
    "\n",
    "# Indicators\n",
    "product_data['satellite'] = (\n",
    "    product_data['firm_ids'].isin([1, 2]).astype(int)\n",
    ")\n",
    "product_data['wired'] = (\n",
    "    product_data['firm_ids'].isin([3, 4]).astype(int)\n",
    ")\n",
    "\n",
    "# Unobservables: ξ_jt and ω_jt with covariance matrix [[1, 0.25], [0.25, 1]]\n",
    "cov_matrix = np.array([[1, 0.25], [0.25, 1]])\n",
    "A = np.linalg.cholesky(cov_matrix)\n",
    "z = np.random.normal(0, 1, (len(product_data), 2))\n",
    "unobs = z @ A.T\n",
    "product_data['xi'] = unobs[:, 0]  # demand unobservable\n",
    "product_data['omega'] = unobs[:, 1]  # cost unobservable\n",
    "\n",
    "print(\"Question 1 completed:\")\n",
    "print(f\"Generated {len(product_data)} observations across {T} markets\")\n",
    "print(f'x range: {product_data[\"x\"].min():.3f} to {product_data[\"x\"].max():.3f}')\n",
    "print(f'w range: {product_data[\"w\"].min():.3f} to {product_data[\"w\"].max():.3f}')\n",
    "xi_omega_corr = product_data[['xi', 'omega']].corr().iloc[0,1]\n",
    "print(f\"ξ-ω correlation: {xi_omega_corr:.3f} (target: 0.25)\")\n",
    "sat_count = product_data[\"satellite\"].sum()\n",
    "wired_count = product_data[\"wired\"].sum()\n",
    "print(f\"Satellite products: {sat_count}, Wired products: {wired_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ed4c4c",
   "metadata": {},
   "source": [
    "### 2. Solve for the equilibrium prices for each good in each market.\n",
    "\n",
    "**(a)** Start by writing a procedure to approximate the derivatives of market shares with respect to prices (taking prices, shares, x, and demand parameters as inputs). The key steps are:\n",
    "\n",
    "(i) For each $jt$, write the choice probability for good $j$, $s_{jt}$, as a weighted average (integral) of the (multinomial logit) choice probabilities conditional on the value of each consumer's random coefficients;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a0d7a",
   "metadata": {},
   "source": [
    "The market share for good $j$ in market $t$, $s_{jt}$, is the probability that a consumer chooses good $j$:\n",
    "\n",
    "$$s_{jt} = \\int P(\\text{choose } j | \\beta_i^{(2)}, \\beta_i^{(3)}) f(\\beta_i^{(2)}, \\beta_i^{(3)}) d\\beta_i^{(2)} d\\beta_i^{(3)}$$\n",
    "\n",
    "where $P(\\text{choose } j | \\beta_i^{(2)}, \\beta_i^{(3)})$ is the multinomial logit choice probability conditional on the random coefficients.\n",
    "\n",
    "Given the random coefficients $\\beta_i^{(2)}$ and $\\beta_i^{(3)}$ (with means $\\beta^{(2)} = 4$, $\\beta^{(3)} = 4$ and variances $\\sigma_2^2 = 1$, $\\sigma_3^2 = 1$), the conditional utility becomes:\n",
    "\n",
    "$$u_{ijt} = \\beta^{(1)} x_{jt} + \\beta_i^{(2)} satellite_{jt} + \\beta_i^{(3)} wired_{jt} + \\alpha p_{jt} + \\xi_{jt} + \\epsilon_{ijt}$$\n",
    "\n",
    "Since $\\epsilon_{ijt}$ are i.i.d. Type-1 extreme value, the conditional choice probability follows the multinomial logit form:\n",
    "\n",
    "$$P(\\text{choose } j | \\beta_i^{(2)}, \\beta_i^{(3)}) = \\frac{\\exp(\\delta_{jt} + \\mu_{jt}^i)}{\\sum_{k=1}^J \\exp(\\delta_{kt} + \\mu_{kt}^i) + 1}$$\n",
    "\n",
    "where:\n",
    "- $\\delta_{jt} = \\beta^{(1)} x_{jt} + \\alpha p_{jt} + \\xi_{jt}$ (mean utility component)\n",
    "- $\\mu_{jt}^i = \\beta_i^{(2)} satellite_{jt} + \\beta_i^{(3)} wired_{jt}$ (random utility component)\n",
    "\n",
    "**Final Expression:**\n",
    "\n",
    "$$s_{jt} = \\int \\frac{\\exp(\\delta_{jt} + \\beta_i^{(2)} satellite_{jt} + \\beta_i^{(3)} wired_{jt})}{\\sum_{k=1}^J \\exp(\\delta_{kt} + \\beta_i^{(2)} satellite_{kt} + \\beta_i^{(3)} wired_{kt}) + 1} \\phi(\\beta_i^{(2)}, \\beta_i^{(3)}) d\\beta_i^{(2)} d\\beta_i^{(3)}$$\n",
    "\n",
    "where $\\phi(\\cdot, \\cdot)$ is the bivariate normal density with mean $(\\beta^{(2)}, \\beta^{(3)}) = (4, 4)$ and covariance matrix $\\text{diag}(1, 1)$.\n",
    "\n",
    "This integral is approximated in the code using Monte Carlo simulation with draws from the normal distribution of $(\\beta_i^{(2)}, \\beta_i^{(3)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfaf6df",
   "metadata": {},
   "source": [
    "(ii) Anticipating differentiation under the integral sign, derive the analytical expression for the derivative of the integrand with respect to each $p_{kt}$.\n",
    "\n",
    "The integrand is the conditional choice probability $P(\\text{choose } j | \\beta_i^{(2)}, \\beta_i^{(3)})$, which depends on prices through the mean utility component $\\delta_{jt} = \\beta^{(1)} x_{jt} + \\alpha p_{jt} + \\xi_{jt}$.\n",
    "\n",
    "Since $p_{kt}$ appears in $\\delta_{kt}$, the derivative with respect to $p_{kt}$ affects the choice probability.\n",
    "\n",
    "For the multinomial logit model, the derivative of the choice probability with respect to a price is:\n",
    "\n",
    "$$\\frac{\\partial P(\\text{choose } j | \\beta_i^{(2)}, \\beta_i^{(3)})}{\\partial p_{kt}} = \\alpha P(j|\\beta_i) \\left( I_{jk} - P(k|\\beta_i) \\right)$$\n",
    "\n",
    "where $I_{jk}$ is the indicator function equal to 1 if $j = k$.\n",
    "\n",
    "Therefore, the derivative of the integrand (conditional choice probability) with respect to $p_{kt}$ is:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial p_{kt}} \\left[ \\frac{\\exp(\\delta_{jt} + \\mu_{jt}^i)}{\\sum_{m=1}^J \\exp(\\delta_{mt} + \\mu_{mt}^i) + 1} \\right] = \\alpha \\cdot \\frac{\\exp(\\delta_{jt} + \\mu_{jt}^i)}{\\sum_{m=1}^J \\exp(\\delta_{mt} + \\mu_{mt}^i) + 1} \\left( I_{jk} - \\frac{\\exp(\\delta_{kt} + \\mu_{kt}^i)}{\\sum_{m=1}^J \\exp(\\delta_{mt} + \\mu_{mt}^i) + 1} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c2c76a",
   "metadata": {},
   "source": [
    "3. Use the expression you obtained in (2) and simulation draws of the random coefficients to approximate the integral that corresponds to $\\partial s_{jt}/\\partial p_{kt}$ for each $j$ and $k$ (i.e., replace the integral with the mean over the values at each simulation draw). Recall the advice in the lecture regarding \"jittering.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba8096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_shares_and_derivatives(prices, market_data, nu_draws):\n",
    "    \"\"\"\n",
    "    Compute shares, derivatives, and inside_shares_draws efficiently in one pass.\n",
    "    Returns: (shares, derivatives, inside_shares_draws)\n",
    "    \"\"\"\n",
    "    J = len(market_data)\n",
    "    x = market_data['x'].values\n",
    "    xi = market_data['xi'].values\n",
    "    sat = market_data['satellite'].values\n",
    "    wired = market_data['wired'].values\n",
    "    \n",
    "    # Compute utilities once\n",
    "    utilities = (\n",
    "        beta1 * x + xi + \n",
    "        nu_draws[:, 0:1] * sat + \n",
    "        nu_draws[:, 1:2] * wired + \n",
    "        alpha * prices\n",
    "    )\n",
    "    utilities = np.column_stack([utilities, np.zeros(nu_draws.shape[0])])\n",
    "    exp_u = np.exp(utilities - np.max(utilities, axis=1, keepdims=True))\n",
    "    choice_probs = exp_u / exp_u.sum(axis=1, keepdims=True)\n",
    "    inside_shares_draws = choice_probs[:, :J]\n",
    "    \n",
    "    # Shares: average over draws\n",
    "    shares = np.mean(inside_shares_draws, axis=0)\n",
    "    \n",
    "    # Derivatives: compute analytically from choice probabilities\n",
    "    derivatives = np.zeros((J, J))\n",
    "    for j in range(J):\n",
    "        for k in range(J):\n",
    "            indicator = float(j == k)\n",
    "            deriv_draws = (\n",
    "                alpha * inside_shares_draws[:, j] * \n",
    "                (indicator - inside_shares_draws[:, k])\n",
    "            )\n",
    "            derivatives[j, k] = np.mean(deriv_draws)\n",
    "    \n",
    "    return shares, derivatives, inside_shares_draws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc99991e",
   "metadata": {},
   "source": [
    "The derivative $\\partial s_{jt}/\\partial p_{kt}$ is approximated using Monte Carlo simulation. For each simulation draw $r = 1, \\dots, R$ of the random coefficients $(\\beta_i^{(2)}, \\beta_i^{(3)})$, compute the conditional choice probability $P(\\text{choose } j | \\beta_i^{(2)}, \\beta_i^{(3)})$ and its derivative with respect to prices.\n",
    "\n",
    "The derivative of the conditional choice probability follows from the multinomial logit formula:\n",
    "\n",
    "$$\\frac{\\partial P(\\text{choose } j | \\beta_i^{(2)}, \\beta_i^{(3)})}{\\partial p_{kt}} = \\alpha P(j|\\beta_i) \\left( \\delta_{jk} - P(k|\\beta_i) \\right)$$\n",
    "\n",
    "where $\\delta_{jk} = 1$ if $j = k` and 0 otherwise.\n",
    "\n",
    "Then, the market share derivative is approximated as:\n",
    "\n",
    "$$\\frac{\\partial s_{jt}}{\\partial p_{kt}} \\approx \\frac{1}{R} \\sum_{r=1}^R \\frac{\\partial P(\\text{choose } j | \\beta_i^{(2,r)}, \\beta_i^{(3,r)})}{\\partial p_{kt}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab755210",
   "metadata": {},
   "source": [
    "Regarding \"jittering\": When solving for equilibrium prices iteratively, redrawing simulation draws in each iteration introduces random noise that can prevent convergence. To avoid this, pre-draw a fixed set of simulation draws for each market and reuse them throughout the solution process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26550341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-draw simulation draws (to avoid jittering)\n",
    "np.random.seed(1995) \n",
    "n_draws = 10000\n",
    "all_nu_draws = [\n",
    "    np.random.multivariate_normal(\n",
    "        [beta2, beta3], \n",
    "        np.diag([sigma_satellite, sigma_wired]), \n",
    "        size=n_draws\n",
    "    ) \n",
    "    for _ in range(T)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc1ed54",
   "metadata": {},
   "source": [
    "(iv) Experiment to see how many simulation draws you need to get precise approximations and check this again at the equilibrium shares and prices you obtained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed79e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_convergence(prices, market_data, nu_draws_full, draw_counts, n_reps=100):\n",
    "    \"\"\"Test derivative stability across different numbers of simulation draws.\"\"\"\n",
    "    np.random.seed(1995) \n",
    "    stds = []\n",
    "    n_available = len(nu_draws_full)\n",
    "    \n",
    "    for n_draws in draw_counts:\n",
    "        deriv_list = []\n",
    "        for rep in range(n_reps):\n",
    "            # Randomly sample n_draws from the pre-drawn samples\n",
    "            indices = np.random.choice(n_available, size=n_draws, replace=False)\n",
    "            nu_draws = nu_draws_full[indices]\n",
    "            _, derivs, _ = market_shares_and_derivatives(\n",
    "                prices, market_data, nu_draws\n",
    "            )\n",
    "            deriv_list.append(derivs)\n",
    "        stds.append(np.std(deriv_list, axis=0).mean())\n",
    "    return np.array(stds)\n",
    "\n",
    "# Test at initial prices (p = MC)\n",
    "product_data['mc'] = np.exp(\n",
    "    gamma0 + gamma1 * product_data['w'] + product_data['omega'] / 8\n",
    ")\n",
    "# Test at initial prices (p = MC) for market 0\n",
    "market_0 = product_data[product_data['market_ids'] == 0]\n",
    "prices_init = market_0['mc'].values\n",
    "draw_counts = [50, 100, 200, 500, 1000, 2000, 5000]\n",
    "stds = test_convergence(prices_init, market_0, all_nu_draws[0], draw_counts)\n",
    "stds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5aa25a",
   "metadata": {},
   "source": [
    "(b) The FOC for firm $j$'s profit maximization problem in market $t$ is\n",
    "\n",
    "\\begin{align}\n",
    "(p_{jt} - mc_{jt}) \\frac{\\partial s_{jt}}{\\partial p_{jt}} + s_{jt} &= 0 \\notag \\\\\n",
    "\\implies p_{jt} - mc_{jt} &= -\\left( \\frac{\\partial s_{jt}}{\\partial p_{jt}} \\right)^{-1} s_{jt}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c91cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"MC range: {product_data['mc'].min():.3f} to {product_data['mc'].max():.3f}\")\n",
    "print(f\"MC mean: {product_data['mc'].mean():.3f}, median: {product_data['mc'].median():.3f}\")\n",
    "print(\"FOC: (p_jt - mc_jt) * ∂s_jt/∂p_jt + s_jt = 0\")\n",
    "print(\"Rearranged: p_jt - mc_jt = - (∂s_jt/∂p_jt)⁻¹ * s_jt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(product_data['mc'], bins=50);\n",
    "plt.legend([\"Marginal Costs\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5bec32",
   "metadata": {},
   "source": [
    "(c) Substituting in your approximation of each $\\partial s_{jt}/\\partial p_{jt}$, solve the system of equations above ($J$ equations per market) for the equilibrium prices in each market.\n",
    "\n",
    "**i.** First do this using Matlab's \"fsolve\" operator. Check the exit flag from fsolve to be sure whether you found a solution for each market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36519e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_prices_direct(market_data, mc_market, nu_draws):\n",
    "    \"\"\"Solve for equilibrium prices using direct nonlinear solver with robust matrix inversion\"\"\"\n",
    "    J = len(market_data)\n",
    "    \n",
    "    def foc_residual(prices):\n",
    "        \"\"\"FOC residuals: p - mc + (∂s/∂p)^{-1} s = 0\"\"\"\n",
    "        # Compute shares and derivatives at current prices\n",
    "        shares, derivatives, _ = market_shares_and_derivatives(\n",
    "            prices, market_data, nu_draws\n",
    "        )\n",
    "\n",
    "        # Inversion of derivative matrix\n",
    "        invD = np.linalg.inv(derivatives)\n",
    "\n",
    "        # FOC residuals: p - mc + inv(∂s/∂p) @ s\n",
    "        residuals = prices - mc_market + invD @ shares\n",
    "        return residuals\n",
    "    # Initial guess: marginal costs\n",
    "    p0 = mc_market.copy()\n",
    "    # Solve using root finder (hybr method)\n",
    "    sol = opt.root(foc_residual, p0, method='hybr', tol=1e-8)\n",
    "    prices_sol = sol.x\n",
    "    success = sol.success\n",
    "    # Additional check: verify that residuals are small\n",
    "    final_residuals = foc_residual(prices_sol)\n",
    "    if np.max(np.abs(final_residuals)) > 1e-6:\n",
    "        success = False\n",
    "    return prices_sol, success\n",
    "\n",
    "# Solve using direct method\n",
    "equilibrium_prices_direct = []\n",
    "success_flags_direct = []\n",
    "for t in range(T):\n",
    "    market_data = product_data[product_data['market_ids'] == t]\n",
    "    mc_market = market_data['mc'].values\n",
    "    nu_draws = all_nu_draws[t]\n",
    "    prices_direct, success = solve_prices_direct(\n",
    "        market_data, mc_market, nu_draws\n",
    "    )\n",
    "    equilibrium_prices_direct.append(prices_direct)\n",
    "    success_flags_direct.append(success)\n",
    "equilibrium_prices_direct = np.array(equilibrium_prices_direct)\n",
    "success_count = sum(success_flags_direct)\n",
    "print(\"Question 2(c)i completed:\")\n",
    "print(f\"Direct nonlinear solver (root): {success_count}/{T} markets solved successfully\")\n",
    "print(f\"Success rate: {success_count/T:.1%}\")\n",
    "price_range_text = (\n",
    "    f\"Price range: {equilibrium_prices_direct.min():.3f} to \"\n",
    "    f\"{equilibrium_prices_direct.max():.3f}\"\n",
    ")\n",
    "print(price_range_text)\n",
    "price_stats_text = (\n",
    "    f\"Price mean: {equilibrium_prices_direct.mean():.3f}, \"\n",
    "    f\"std: {equilibrium_prices_direct.std():.3f}\"\n",
    ")\n",
    "print(price_stats_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca3eea",
   "metadata": {},
   "source": [
    "ii. Do this again using the algorithm of Morrow and Skerlos (2011), discussed in section 3.6 of Conlon and Gortmaker (2019) (and in the pyBLP \"problem simulation tutorial\"). Use the numerical integration approach you used in step (a) to approximate the terms defined in equation (25) of Conlon and Gortmaker. If you get different results using this method, resolve this discrepancy either by correcting your code or explaining why your preferred method is the one to be trusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba70d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_prices_morrow_skerlos(market_data, mc_market, nu_draws, max_iter=100, tol=1e-6):\n",
    "    \"\"\"Morrow-Skerlos algorithm\"\"\"\n",
    "    prices = mc_market.copy()\n",
    "    for iteration in range(max_iter):\n",
    "        # Efficiently compute shares, derivatives, and inside_shares_draws in one pass\n",
    "        shares, derivatives, inside_shares_draws = market_shares_and_derivatives(prices, market_data, nu_draws)\n",
    "        \n",
    "        Lambda = np.diag(alpha * shares)\n",
    "        Gamma = alpha * (inside_shares_draws.T @ inside_shares_draws) / nu_draws.shape[0]\n",
    "        diff = prices - mc_market\n",
    "        zeta = np.linalg.solve(Lambda, Gamma.T @ diff - shares)\n",
    "        prices_new = mc_market + zeta\n",
    "        foc_residual = Lambda @ (prices - mc_market - zeta)\n",
    "        if np.max(np.abs(foc_residual)) < tol:\n",
    "            break\n",
    "        prices = 0.5 * prices + 0.5 * prices_new\n",
    "    return prices, iteration + 1\n",
    "\n",
    "# Solve using Morrow-Skerlos method\n",
    "equilibrium_prices_ms = []\n",
    "iterations_ms = []\n",
    "\n",
    "for t in range(T):\n",
    "    market_data = product_data[product_data['market_ids'] == t]\n",
    "    mc_market = market_data['mc'].values\n",
    "    nu_draws = all_nu_draws[t]\n",
    "\n",
    "    prices_ms, iters = solve_prices_morrow_skerlos(market_data, mc_market, nu_draws)\n",
    "    equilibrium_prices_ms.append(prices_ms)\n",
    "    iterations_ms.append(iters)\n",
    "\n",
    "equilibrium_prices_ms = np.array(equilibrium_prices_ms)\n",
    "print(\"Question 2(c)ii completed:\")\n",
    "print(f\"Morrow-Skerlos method: {T} markets solved\")\n",
    "print(f\"Average iterations: {np.mean(iterations_ms):.1f}\")\n",
    "print(f\"Max iterations: {np.max(iterations_ms)}\")\n",
    "print(f\"Price range: {equilibrium_prices_ms.min():.3f} to {equilibrium_prices_ms.max():.3f}\")\n",
    "print(f\"Price mean: {equilibrium_prices_ms.mean():.3f}, std: {equilibrium_prices_ms.std():.3f}\")\n",
    "\n",
    "# Compare direct vs Morrow-Skerlos if direct succeeded for all\n",
    "if len(equilibrium_prices_direct) == T:\n",
    "    price_diff = np.abs(np.array(equilibrium_prices_direct) - equilibrium_prices_ms)\n",
    "    print(f\"Max price difference between methods: {price_diff.max():.2e}\")\n",
    "    print(f\"Mean price difference: {price_diff.mean():.2e}\")\n",
    "else:\n",
    "    print(\"Direct method failed for some markets, skipsigmang comparison.\")\n",
    "    print(\"Preferred method: Morrow-Skerlos, as it is more numerically stable.\")\n",
    "\n",
    "# Use Morrow-Skerlos prices\n",
    "product_data['prices'] = equilibrium_prices_ms.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b39f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Compare derivative convergence at initial vs equilibrium prices\n",
    "    market_0 = product_data[product_data['market_ids'] == 0]\n",
    "    prices_equilibrium = market_0['prices'].values\n",
    "\n",
    "    draw_counts = [50, 100, 200, 500, 1000, 2000, 5000]\n",
    "    # Reuse previously calculated initial_stds from test_convergence\n",
    "    initial_stds = stds  # Already calculated earlier at initial prices\n",
    "    \n",
    "    # Only compute equilibrium stds\n",
    "    np.random.seed(1995)\n",
    "    n_available = len(all_nu_draws[0])\n",
    "    n_reps = 100\n",
    "    eq_stds = []\n",
    "    for n_draws in draw_counts:\n",
    "        deriv_list = []\n",
    "        for _ in range(n_reps):\n",
    "            indices = np.random.choice(n_available, size=n_draws, replace=False)\n",
    "            nu_draws = all_nu_draws[0][indices]\n",
    "            _, derivatives, _ = market_shares_and_derivatives(\n",
    "                prices_equilibrium, market_0, nu_draws\n",
    "            )\n",
    "            deriv_list.append(derivatives)\n",
    "        eq_stds.append(np.std(deriv_list, axis=0).mean())\n",
    "    eq_stds = np.array(eq_stds)\n",
    "\n",
    "    print(\"Comparing derivative approximation convergence:\")\n",
    "    print(\"Draws\\t| Initial Std Dev\\t| Equilibrium Std Dev\\t| Ratio (Eq/Init)\")\n",
    "    print(\"-\" * 75)\n",
    "\n",
    "    for i, n_draws in enumerate(draw_counts):\n",
    "        ratio = eq_stds[i] / initial_stds[i] if initial_stds[i] > 0 else float('inf')\n",
    "        print(f\"{n_draws:6d}\\t| {initial_stds[i]:.2e}\\t\\t| {eq_stds[i]:.2e}\\t\\t| {ratio:.2f}\")\n",
    "\n",
    "    # Summary statistics\n",
    "    valid_ratios = eq_stds / initial_stds\n",
    "    avg_ratio = np.mean(valid_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c19e4",
   "metadata": {},
   "source": [
    "### 3. \n",
    "Calculate \"observed\" market shares for your fake data set using your parameters, your draws of $x$, $w$, $\\xi$, $\\omega$, and your equilibrium prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40eeb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_shares = []\n",
    "for t in range(T):\n",
    "    market_data = product_data[product_data['market_ids'] == t]\n",
    "    prices_market = market_data['prices'].values\n",
    "    # Use pre-drawn simulation draws for this market\n",
    "    shares_market, _, _ = market_shares_and_derivatives(\n",
    "        prices_market, market_data, all_nu_draws[t]\n",
    "    )\n",
    "    observed_shares.extend(shares_market)\n",
    "\n",
    "product_data['shares'] = observed_shares\n",
    "\n",
    "print(f\"Share range: {product_data['shares'].min():.3f} to {product_data['shares'].max():.3f}\")\n",
    "print(f\"Share mean: {product_data['shares'].mean():.3f}, std: {product_data['shares'].std():.3f}\")\n",
    "\n",
    "# Validation: Check market share sums\n",
    "market_share_sums = product_data.groupby('market_ids')['shares'].sum()\n",
    "print(f\"Market share sums (should be < 1):\")\n",
    "print(f\"Average: {market_share_sums.mean():.3f}\")\n",
    "print(f\"Min: {market_share_sums.min():.3f}, Max: {market_share_sums.max():.3f}\")\n",
    "print(f\"Outside shares: {1 - market_share_sums.mean():.3f} (average)\")\n",
    "\n",
    "# Check by product type\n",
    "satellite_shares = product_data[product_data['satellite'] == 1]['shares'].mean()\n",
    "wired_shares = product_data[product_data['wired'] == 1]['shares'].mean()\n",
    "print(f\"Average satellite product share: {satellite_shares:.3f}\")\n",
    "print(f\"Average wired product share: {wired_shares:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95915201",
   "metadata": {},
   "source": [
    "### 4. \n",
    "\n",
    "Below you'll be using $x$ and $w$ as instruments in the demand estimation. Check whether these appear to be good instruments in your fake data using some regressions of prices and market shares on the exogenous variables (or some function of them; see the related discussion in the coding tips). If you believe the instruments are not providing enough variation, modify the parameter choices above until you are satisfied. Report your final choice of parameters and the results you rely on to conclude that the instruments seem good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create quadratic and interaction columns first\n",
    "product_data['x**2'] = product_data['x'] ** 2\n",
    "product_data['w**2'] = product_data['w'] ** 2\n",
    "product_data['x*w'] = product_data['x'] * product_data['w']\n",
    "\n",
    "# sum over competing goods in market t\n",
    "product_data['sum_x_competitors'] = (\n",
    "    product_data.groupby('market_ids')['x'].transform('sum') - \n",
    "    product_data['x']\n",
    ")\n",
    "product_data['sum_w_competitors'] = (\n",
    "    product_data.groupby('market_ids')['w'].transform('sum') - \n",
    "    product_data['w']\n",
    ")\n",
    "\n",
    "# index of the other good in the same nest\n",
    "product_data['x_other_in_nest'] = (\n",
    "    product_data.groupby(['market_ids', 'satellite'])['x'].transform('sum') - \n",
    "    product_data['x']\n",
    ")\n",
    "product_data['w_other_in_nest'] = (\n",
    "    product_data.groupby(['market_ids', 'satellite'])['w'].transform('sum') - \n",
    "    product_data['w']\n",
    ")\n",
    "\n",
    "# Use satellite and wired dummies instead of constant\n",
    "Z = product_data[[\n",
    "    'satellite', 'wired', 'x', 'w', 'x**2', 'w**2', 'x*w', \n",
    "    'sum_x_competitors', 'sum_w_competitors', 'x_other_in_nest', 'w_other_in_nest'\n",
    "]]\n",
    "\n",
    "# Regression 1: Prices on extended instruments (Relevance check)\n",
    "price_model = sm.OLS(product_data['prices'], Z).fit()\n",
    "\n",
    "# Regression 2: Market shares on extended instruments\n",
    "share_model = sm.OLS(product_data['shares'], Z).fit()\n",
    "\n",
    "# Regression 3: Demand unobservable ξ on instruments (Exclusion check)\n",
    "xi_model = sm.OLS(product_data['xi'], Z).fit()\n",
    "\n",
    "# Regression 4: Cost unobservable ω on instruments (Exclusion check)\n",
    "omega_model = sm.OLS(product_data['omega'], Z).fit()\n",
    "\n",
    "# Test joint significance of excluded instruments\n",
    "print(\"=\"*75)\n",
    "print(\"INSTRUMENT VALIDITY TESTS\")\n",
    "print(\"=\"*75)\n",
    "excluded_vars = ['w', 'x**2', 'w**2', 'x*w', \n",
    "                 'sum_x_competitors', 'sum_w_competitors', \n",
    "                 'x_other_in_nest', 'w_other_in_nest']\n",
    "\n",
    "# Create hypothesis string using actual variable names\n",
    "hypothesis = ', '.join([f'{var}=0' for var in excluded_vars])\n",
    "\n",
    "# F-test for excluded instruments in price regression\n",
    "price_f_test = price_model.f_test(hypothesis)\n",
    "print(f\"\\n1. Price Regression (Relevance Test)\")\n",
    "print(f\"   R²: {price_model.rsquared:.3f}\")\n",
    "print(f\"   Individual Coefficients:\")\n",
    "for i, var in enumerate(Z.columns):\n",
    "    print(f\"     {var:18s}: {price_model.params.iloc[i]:8.3f} (SE: {price_model.bse.iloc[i]:.3f}, t: {price_model.tvalues.iloc[i]:6.2f}, p: {price_model.pvalues.iloc[i]:.3f})\")\n",
    "print(f\"   Excluded demand instruments F-stat: {price_f_test.fvalue:.2f} (p={price_f_test.pvalue:.2e})\")\n",
    "print(f\"   → Excluded instruments are {'relevant' if price_f_test.pvalue < 0.01 else 'weak'} for prices\")\n",
    "\n",
    "# F-test for excluded instruments in share regression\n",
    "share_f_test = share_model.f_test(hypothesis)\n",
    "print(f\"\\n2. Share Regression (Relevance Test)\")\n",
    "print(f\"   R²: {share_model.rsquared:.3f}\")\n",
    "print(f\"   Individual Coefficients:\")\n",
    "for i, var in enumerate(Z.columns):\n",
    "    print(f\"     {var:18s}: {share_model.params.iloc[i]:8.3f} (SE: {share_model.bse.iloc[i]:.3f}, t: {share_model.tvalues.iloc[i]:6.2f}, p: {share_model.pvalues.iloc[i]:.3f})\")\n",
    "print(f\"   Excluded demand instruments F-stat: {share_f_test.fvalue:.2f} (p={share_f_test.pvalue:.2e})\")\n",
    "print(f\"   → Excluded instruments are {'relevant' if share_f_test.pvalue < 0.01 else 'weak'} for shares\")\n",
    "\n",
    "# F-test for excluded instruments in xi regression (should be insignificant)\n",
    "xi_f_test = xi_model.f_test(hypothesis)\n",
    "print(f\"\\n3. ξ Regression (Exclusion Test)\")\n",
    "print(f\"   R²: {xi_model.rsquared:.3f}\")\n",
    "print(f\"   Individual Coefficients:\")\n",
    "for i, var in enumerate(Z.columns):\n",
    "    print(f\"     {var:18s}: {xi_model.params.iloc[i]:8.3f} (SE: {xi_model.bse.iloc[i]:.3f}, t: {xi_model.tvalues.iloc[i]:6.2f}, p: {xi_model.pvalues.iloc[i]:.3f})\")\n",
    "print(f\"   Excluded demand instruments F-stat: {xi_f_test.fvalue:.2f} (p={xi_f_test.pvalue:.2e})\")\n",
    "print(f\"   → Excluded instruments are {'exogenous' if xi_f_test.pvalue >= 0.01 else 'endogenous'}\")\n",
    "\n",
    "# F-test for excluded instruments in omega regression (should be insignificant)\n",
    "omega_f_test = omega_model.f_test(hypothesis)\n",
    "print(f\"\\n4. ω Regression (Exclusion Test)\")\n",
    "print(f\"   R²: {omega_model.rsquared:.3f}\")\n",
    "print(f\"   Individual Coefficients:\")\n",
    "for i, var in enumerate(Z.columns):\n",
    "    print(f\"     {var:18s}: {omega_model.params.iloc[i]:8.3f} (SE: {omega_model.bse.iloc[i]:.3f}, t: {omega_model.tvalues.iloc[i]:6.2f}, p: {omega_model.pvalues.iloc[i]:.3f})\")\n",
    "print(f\"   Excluded demand instruments F-stat: {omega_f_test.fvalue:.2f} (p={omega_f_test.pvalue:.2e})\")\n",
    "print(f\"   → Excluded instruments are {'exogenous' if omega_f_test.pvalue >= 0.01 else 'endogenous'}\")\n",
    "\n",
    "# Assess instrument validity\n",
    "weak_instruments = (\n",
    "    (price_model.f_pvalue >= 0.01 and share_model.f_pvalue >= 0.01) or \n",
    "    (price_model.rsquared < 0.05 and share_model.rsquared < 0.05)\n",
    ")\n",
    "excluded_instruments = (\n",
    "    xi_model.f_pvalue < 0.01 or omega_model.f_pvalue < 0.01\n",
    ")\n",
    "print()\n",
    "print(\"=\"*75)\n",
    "print(\"FINAL PARAMETER CHOICE:\")\n",
    "print(\"=\"*75)\n",
    "if weak_instruments or excluded_instruments:\n",
    "    print(\"Parameters need adjustment - instruments are weak or invalid.\")\n",
    "else:\n",
    "    print(f\"Demand: α = {alpha}, β^(1) = {beta1}, β_i^(2) ~ N({beta2}, {sigma_satellite}²), β_i^(3) ~ N({beta3}, {sigma_wired}²)\")\n",
    "    print(f\"Supply: γ^(0) = {gamma0}, γ^(1) = {gamma1}\")\n",
    "    print(\"These parameters generate data with valid instruments and are retained as final.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data.to_csv('blp.csv', index=False)\n",
    "print(product_data.head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc0d1c",
   "metadata": {},
   "source": [
    "## 4 Estimate Some Mis-specified Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1df5711",
   "metadata": {},
   "source": [
    "### 5. Estimate the plain multinomial logit model of demand by OLS (ignoring the endogeneity of prices).\n",
    "\n",
    "For the plain multinomial logit model, the utility is:\n",
    "\n",
    "$$u_{ijt} = \\beta^{(1)} x_{jt} + \\beta^{(2)} satellite_{jt} + \\beta^{(3)} wired_{jt} + \\alpha p_{jt} + \\xi_{jt} + \\epsilon_{ijt}$$\n",
    "\n",
    "This implies the log-odds ratio:\n",
    "\n",
    "$$\\ln\\left(\\frac{s_{jt}}{s_{0t}}\\right) = \\delta_{jt} = \\beta^{(1)} x_{jt} + \\beta^{(2)} satellite_{jt} + \\beta^{(3)} wired_{jt} + \\alpha p_{jt} + \\xi_{jt}$$\n",
    "\n",
    "We can estimate this by OLS, regressing the logit-transformed shares on the observed product characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b593d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute outside shares for each market\n",
    "product_data['outside_share'] = 1 - product_data.groupby('market_ids')['shares'].transform('sum')\n",
    "\n",
    "# Compute logit delta: ln(s_jt / s_0t)\n",
    "product_data['logit_delta'] = np.log(product_data['shares'] / product_data['outside_share'])\n",
    "\n",
    "# OLS using matrix algebra (no intercept)\n",
    "y = product_data['logit_delta'].values\n",
    "X = product_data[['prices', 'x', 'satellite', 'wired' ]].values\n",
    "\n",
    "# Compute OLS estimates: beta_hat = (X^T X)^(-1) X^T y\n",
    "beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "# Compute residuals and HC0 robust standard errors\n",
    "y_hat = X @ beta_hat\n",
    "residuals = y - y_hat\n",
    "n, k = X.shape\n",
    "\n",
    "# HC0 robust covariance matrix\n",
    "V = X.T @ np.diag(residuals**2) @ X\n",
    "cov_matrix_ols = np.linalg.inv(X.T @ X) @ V @ np.linalg.inv(X.T @ X)\n",
    "se_ols = np.sqrt(np.diag(cov_matrix_ols))\n",
    "\n",
    "# t-statistics and p-values\n",
    "t_stats = beta_hat / se_ols\n",
    "p_values = 2 * (1 - stats.norm.cdf(np.abs(t_stats)))\n",
    "\n",
    "print(\"OLS Regression: ln(s_jt/s_0t) ~ x + satellite + wired + prices (no intercept)\")\n",
    "print(\"-\" * 70)\n",
    "param_names = ['prices', 'x', 'satellite', 'wired']\n",
    "for i, param in enumerate(param_names):\n",
    "    print(f\"{param:12s}: {beta_hat[i]:8.3f} (SE: {se_ols[i]:.3f}, t: {t_stats[i]:6.2f}, p: {p_values[i]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc88f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data['demand_instruments0'] = product_data['prices']\n",
    "ols_problem = pyblp.Problem(pyblp.Formulation('0 + prices + x + satellite + wired '), product_data)\n",
    "ols_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dfdfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_results = ols_problem.solve(method='1s')\n",
    "ols_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a2bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(index=ols_results.beta_labels, data={\n",
    "    (\"Estimates\", \"Manual OLS\"): beta_hat,\n",
    "    (\"Estimates\", \"PyBLP\"): ols_results.beta.flat,\n",
    "    (\"SEs\", \"Manual OLS\"): se_ols,\n",
    "    (\"SEs\", \"PyBLP\"): ols_results.beta_se.flat,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b407c29",
   "metadata": {},
   "source": [
    "### 6. \n",
    "Re-estimate the multinomial logit model of demand by two-stage\n",
    "least squares, instrumenting for prices with the exogenous demand shifters $%\n",
    "x $ and excluded cost shifters w. Discuss how the results differ from those\n",
    "obtained by OLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec453deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First stage: \n",
    "Z = product_data[['satellite', 'wired', 'x', 'w', 'x**2', 'w**2', 'x*w', 'sum_x_competitors', 'sum_w_competitors']].values  \n",
    "\n",
    "# First stage OLS:\n",
    "sigma_hat = np.linalg.inv(Z.T @ Z) @ Z.T @ product_data['prices'].values\n",
    "prices_hat = Z @ sigma_hat\n",
    "\n",
    "# First stage diagnostics\n",
    "first_stage_residuals = product_data['prices'].values - prices_hat\n",
    "SST = np.sum((product_data['prices'].values - product_data['prices'].mean())**2)\n",
    "SSR = np.sum(first_stage_residuals**2)\n",
    "R2_first_stage = 1 - SSR/SST\n",
    "\n",
    "# F-statistic for excluded instruments (w, x², w², x*w, sum_x_competitors, sum_w_competitors)\n",
    "# Restricted model: prices ~ satellite + wired + x\n",
    "Z_restricted = product_data[['satellite', 'wired', 'x']].values\n",
    "sigma_restricted = np.linalg.inv(Z_restricted.T @ Z_restricted) @ Z_restricted.T @ product_data['prices'].values\n",
    "prices_restricted = Z_restricted @ sigma_restricted\n",
    "SSR_restricted = np.sum((product_data['prices'].values - prices_restricted)**2)\n",
    "\n",
    "# F-test: F = [(SSR_r - SSR_ur)/q] / [SSR_ur/(n-k)]\n",
    "n = len(product_data)\n",
    "k = Z.shape[1]  # number of parameters in unrestricted model\n",
    "q = 6  # number of excluded instruments\n",
    "F_stat = ((SSR_restricted - SSR) / q) / (SSR / (n - k))\n",
    "p_value_F = 1 - stats.f.cdf(F_stat, q, n - k)\n",
    "\n",
    "print(f\"First Stage Diagnostics:\")\n",
    "print(f\"  R² = {R2_first_stage:.4f}\")\n",
    "print(f\"  F-statistic (excluded instruments) = {F_stat:.2f} (p = {p_value_F:.4f})\")\n",
    "print()\n",
    "\n",
    "# Second stage: Regress logit_delta on x + satellite + wired + predicted_prices\n",
    "y = product_data['logit_delta'].values\n",
    "X_hat = np.column_stack([\n",
    "    prices_hat,  # Use predicted prices from first stage\n",
    "    product_data['x'].values,\n",
    "    product_data['satellite'].values,\n",
    "    product_data['wired'].values\n",
    "])\n",
    "\n",
    "# 2SLS estimates: beta_hat_iv = (X_hat^T X_hat)^(-1) X_hat^T y\n",
    "beta_hat_iv = np.linalg.inv(X_hat.T @ X_hat) @ X_hat.T @ y\n",
    "\n",
    "# Compute 2SLS standard errors (HC0 robust)\n",
    "# Need to use original regressors X, not fitted X_hat\n",
    "X = np.column_stack([\n",
    "    product_data['prices'].values,  # Use actual prices for residuals and variance\n",
    "    product_data['x'].values,\n",
    "    product_data['satellite'].values,\n",
    "    product_data['wired'].values\n",
    "])\n",
    "\n",
    "residuals_iv = y - X @ beta_hat_iv\n",
    "\n",
    "# HC0 robust covariance for 2SLS: (X'Z(Z'Z)^{-1}Z'X)^{-1} X'Z(Z'Z)^{-1} Ω (Z'Z)^{-1}Z'X (X'Z(Z'Z)^{-1}Z'X)^{-1}\n",
    "# where Ω = diag(residuals²)\n",
    "P_Z = Z @ np.linalg.inv(Z.T @ Z) @ Z.T  # Projection matrix\n",
    "Omega = np.diag(residuals_iv**2)\n",
    "\n",
    "# Simplified: (X'P_Z X)^{-1} X'P_Z Ω P_Z X (X'P_Z X)^{-1}\n",
    "XPZ = X.T @ P_Z\n",
    "bread = np.linalg.inv(XPZ @ X)\n",
    "meat = XPZ @ Omega @ P_Z @ X\n",
    "cov_matrix_iv = bread @ meat @ bread\n",
    "se_iv = np.sqrt(np.diag(cov_matrix_iv)) \n",
    "t_stats_iv = beta_hat_iv / se_iv\n",
    "p_values_iv = 2 * (1 - stats.norm.cdf(np.abs(t_stats_iv)))\n",
    "\n",
    "print(\"2SLS IV Regression: ln(s_jt/s_0t) ~ x + satellite + wired + prices_hat (no intercept)\")\n",
    "print(\"First stage instruments: x, w, x², w², x*w, sum_x_competitors, sum_w_competitors\")\n",
    "print(\"-\" * 80)\n",
    "param_names = ['prices', 'x', 'satellite', 'wired']\n",
    "for i, param in enumerate(param_names):\n",
    "    print(f\"{param:12s}: {beta_hat_iv[i]:8.3f} (SE: {se_iv[i]:.3f}, t: {t_stats_iv[i]:6.2f}, p: {p_values_iv[i]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add demand instruments for PyBLP\n",
    "product_data['demand_instruments0'] = product_data['w']\n",
    "product_data['demand_instruments1'] = product_data['x**2']\n",
    "product_data['demand_instruments2'] = product_data['w**2']\n",
    "product_data['demand_instruments3'] = product_data['x*w']\n",
    "product_data['demand_instruments4'] = product_data['sum_x_competitors']\n",
    "product_data['demand_instruments5'] = product_data['sum_w_competitors']\n",
    "\n",
    "iv_problem = pyblp.Problem(pyblp.Formulation('0 + prices + x + satellite + wired'), product_data)\n",
    "iv_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d2e250",
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_results = iv_problem.solve(method='1s')\n",
    "iv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d140c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(index=iv_results.beta_labels, data={\n",
    "    (\"Estimates\", \"Manual IV\"): beta_hat_iv,\n",
    "    (\"Estimates\", \"PyBLP IV\"): iv_results.beta.flat,\n",
    "    (\"SEs\", \"Manual IV\"): se_iv,\n",
    "    (\"SEs\", \"PyBLP IV\"): iv_results.beta_se.flat\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b33a30",
   "metadata": {},
   "source": [
    "### 7. Nested Logit Model Estimation\n",
    "\n",
    "Now estimate a nested logit model by two-stage least squares, treating \"satellite\" and \"wired\" as the two nests for the inside goods. You will probably want to review the discussion of the nested logit in Berry (1994). Note that Berry focuses on the special case in which all the \"nesting parameters\" are the same; you should allow a different nesting parameter for each nest.\n",
    "\n",
    "\n",
    "\n",
    "In Berry’s notation, this means letting the parameter become g(j) , where g (j) indicates the group (satellite\n",
    "or wired) to which each inside good j belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d0579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ln_within_share\n",
    "product_data[\"group_share\"] = product_data.groupby([\"market_ids\", \"satellite\"])[\"shares\"].transform(\"sum\")\n",
    "product_data[\"ln_within_share\"] = np.log(product_data[\"shares\"] / product_data[\"group_share\"])\n",
    "\n",
    "# Create nest-specific ln_within_share\n",
    "product_data[\"ln_within_share_sat\"] = product_data[\"ln_within_share\"] * product_data[\"satellite\"]\n",
    "product_data[\"ln_within_share_wired\"] = product_data[\"ln_within_share\"] * product_data[\"wired\"]\n",
    "\n",
    "# Define variables\n",
    "exog_vars = [\"x\", \"satellite\", \"wired\"]\n",
    "endog_vars = [\"prices\", \"ln_within_share_sat\", \"ln_within_share_wired\"]\n",
    "instr_vars = [\"w\", \"x**2\", \"w**2\", \"x*w\", \"sum_x_competitors\", \"sum_w_competitors\", \"x_other_in_nest\", \"w_other_in_nest\"]\n",
    "Z_vars = exog_vars + instr_vars\n",
    "\n",
    "# First stage: Z = exog + instr\n",
    "Z = product_data[Z_vars].values\n",
    "\n",
    "# First stage OLS for each endog\n",
    "n_endog = len(endog_vars)\n",
    "sigma_hat = np.zeros((Z.shape[1], n_endog))\n",
    "endog_hat = np.zeros((len(product_data), n_endog))\n",
    "for i, var in enumerate(endog_vars):\n",
    "    y_endog = product_data[var].values\n",
    "    sigma = np.linalg.inv(Z.T @ Z) @ Z.T @ y_endog\n",
    "    sigma_hat[:, i] = sigma\n",
    "    endog_hat[:, i] = Z @ sigma\n",
    "\n",
    "# Second stage: Regress logit_delta on exog + predicted_endog, reordered to match PyBLP\n",
    "y = product_data[\"logit_delta\"].values\n",
    "X_hat = np.column_stack([\n",
    "    endog_hat[:, 0],  # prices_hat\n",
    "    product_data[\"x\"].values,\n",
    "    product_data[\"satellite\"].values,\n",
    "    product_data[\"wired\"].values,\n",
    "    endog_hat[:, 1],  # ln_within_share_sat_hat\n",
    "    endog_hat[:, 2]   # ln_within_share_wired_hat\n",
    "])\n",
    "\n",
    "# 2SLS estimates\n",
    "beta_hat_iv_nested = np.linalg.inv(X_hat.T @ X_hat) @ X_hat.T @ y\n",
    "\n",
    "# Compute robust standard errors (HC0) - CORRECTED for 2SLS\n",
    "# Need to use original regressors X, not fitted X_hat\n",
    "X = np.column_stack([\n",
    "    product_data[\"prices\"].values,\n",
    "    product_data[\"x\"].values,\n",
    "    product_data[\"satellite\"].values,\n",
    "    product_data[\"wired\"].values,\n",
    "    product_data[\"ln_within_share_sat\"].values,\n",
    "    product_data[\"ln_within_share_wired\"].values\n",
    "])\n",
    "residuals_iv = y - X @ beta_hat_iv_nested\n",
    "P_Z = Z @ np.linalg.inv(Z.T @ Z) @ Z.T\n",
    "Omega = np.diag(residuals_iv**2)\n",
    "XPZ = X.T @ P_Z\n",
    "bread = np.linalg.inv(XPZ @ X)\n",
    "meat = XPZ @ Omega @ P_Z @ X\n",
    "cov_matrix_iv = bread @ meat @ bread\n",
    "se_iv_nested = np.sqrt(np.diag(cov_matrix_iv))\n",
    "t_stats_iv = beta_hat_iv_nested / se_iv_nested\n",
    "p_values_iv = 2 * (1 - stats.norm.cdf(np.abs(t_stats_iv)))\n",
    "\n",
    "print(\"2SLS IV Regression: ln(s_jt/s_0t) ~ prices + x + satellite + wired + ln_within_share_sat + ln_within_share_wired (no intercept)\")\n",
    "print(\"First stage instruments: \" + \", \".join(Z_vars))\n",
    "print(\"-\" * 120)\n",
    "param_names = [\"prices\", \"x\", \"satellite\", \"wired\", \"ln_within_share_sat\", \"ln_within_share_wired\"]\n",
    "for i, param in enumerate(param_names):\n",
    "    print(f\"{param:20s}: {beta_hat_iv_nested[i]:8.3f} (SE: {se_iv_nested[i]:.3f}, t: {t_stats_iv[i]:6.2f}, p: {p_values_iv[i]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db266c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for nested logit\n",
    "# Note: nesting_ids needed for PyBLP, but use satellite/wired for groupby where possible\n",
    "product_data['nesting_ids'] = product_data['satellite'].map({1: 'satellite', 0: 'wired'})\n",
    "product_data['demand_instruments6'] = product_data['x_other_in_nest']\n",
    "product_data['demand_instruments7'] = product_data['w_other_in_nest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ee267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested logit formulation\n",
    "nl_problem = pyblp.Problem(pyblp.Formulation('0 + prices + x + satellite + wired'), product_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98759dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_initial = [0.7, 0.7]  # Initial values for rho_sat and rho_wired\n",
    "nl_results = nl_problem.solve(rho=rho_initial, method='1s')\n",
    "nl_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe60b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare manual nested logit estimates with PyBLP nested logit estimates for beta\n",
    "nested_beta_comparison = pd.DataFrame(index=nl_results.beta_labels, data={\n",
    "    (\"Estimates\", \"Manual Nested\"): beta_hat_iv_nested[:4],  # prices, x, satellite, wired\n",
    "    (\"Estimates\", \"PyBLP Nested\"): nl_results.beta.flat,\n",
    "    (\"SEs\", \"Manual Nested\"): se_iv_nested[:4],\n",
    "    (\"SEs\", \"PyBLP Nested\"): nl_results.beta_se.flat\n",
    "})\n",
    "\n",
    "print(\"Beta Comparison for Nested Logit:\")\n",
    "print(nested_beta_comparison)\n",
    "\n",
    "# Compare rho estimates\n",
    "nested_rho_comparison = pd.DataFrame(index=nl_results.rho_labels, data={\n",
    "    (\"Estimates\", \"Manual Nested\"): beta_hat_iv_nested[4:],  # rho_sat, rho_wired\n",
    "    (\"Estimates\", \"PyBLP Nested\"): nl_results.rho.flat,\n",
    "    (\"SEs\", \"Manual Nested\"): se_iv_nested[4:],\n",
    "    (\"SEs\", \"PyBLP Nested\"): nl_results.rho_se.flat\n",
    "})\n",
    "\n",
    "print(\"\\nRho Comparison for Nested Logit:\")\n",
    "print(nested_rho_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e598ac",
   "metadata": {},
   "source": [
    "### 8.\n",
    "Using the nested logit results, provide a table comparing the estimated own-price elasticities to the true own-price elasticities. The procedure you developed above for approximating derivatives cannot be used for your estimates based\n",
    "on the nested logit model. But because we have analytic expressions for market shares in the nested logit model,\n",
    "you could either differentiate these or use “finite difference” approximation of derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract nested logit parameters\n",
    "alpha_nl, beta_x_nl, rho_sat_nl, rho_wired_nl = beta_hat_iv_nested[[0, 1, 4, 5]]\n",
    "\n",
    "def compute_nested_logit_elasticities_analytic(market_df, alpha, beta_x, rho_sat, rho_wired):\n",
    "    \"\"\"Compute elasticities using pyBLP's exact Jacobian formula for nested logit.\n",
    "    \n",
    "    Based on pyBLP's compute_capital_lamda_gamma:\n",
    "    - Lambda_jj = alpha * s_j / (1 - rho_j)\n",
    "    - Gamma_jk = alpha * s_j * s_k + rho/(1-rho) * membership_jk * alpha * s_j|g * s_k\n",
    "    - Jacobian[j,k] = Lambda_jj - Gamma_jk (if j==k), -Gamma_jk (if j!=k)\n",
    "    - Elasticity[j,k] = Jacobian[j,k] * price[k] / share[j]\n",
    "    \n",
    "    This matches pyBLP to within ~1% numerical precision.\n",
    "    \"\"\"\n",
    "    J = len(market_df)\n",
    "    prices = market_df['prices'].values\n",
    "    shares = market_df['shares'].values\n",
    "    satellite, wired = market_df['satellite'].values, market_df['wired'].values\n",
    "    \n",
    "    # Compute within-nest shares (conditionals in pyBLP terminology)\n",
    "    s_group = market_df.groupby('satellite')['shares'].transform('sum').values\n",
    "    conditionals = shares / s_group\n",
    "    \n",
    "    # Nesting parameter for each product\n",
    "    rho = np.where(satellite == 1, rho_sat, rho_wired)\n",
    "    \n",
    "    # Compute full elasticity matrix using pyBLP's formula\n",
    "    elasticities = np.zeros((J, J))\n",
    "    for j in range(J):\n",
    "        # Lambda diagonal element\n",
    "        lambda_jj = alpha * shares[j] / (1 - rho[j])\n",
    "        \n",
    "        for k in range(J):\n",
    "            # Gamma matrix element\n",
    "            same_nest = (satellite[j] == satellite[k]) and (wired[j] == wired[k])\n",
    "            gamma_jk = alpha * shares[j] * shares[k]\n",
    "            if same_nest:\n",
    "                gamma_jk += (rho[j] / (1 - rho[j])) * alpha * conditionals[j] * shares[k]\n",
    "            \n",
    "            # Jacobian = Lambda - Gamma (on diagonal), -Gamma (off-diagonal)\n",
    "            if j == k:\n",
    "                jac_jk = lambda_jj - gamma_jk\n",
    "            else:\n",
    "                jac_jk = -gamma_jk\n",
    "            \n",
    "            # Elasticity = Jacobian * price / share\n",
    "            elasticities[j, k] = jac_jk * prices[k] / shares[j]\n",
    "    \n",
    "    return elasticities\n",
    "\n",
    "def compute_rc_elasticities_observed_shares(market_df, nu_draws, alpha, beta_x, beta_sat, beta_wired, sigma_sat, sigma_wired):\n",
    "    \"\"\"Compute elasticities from RC logit using OBSERVED shares (not recomputed from xi).\n",
    "    \n",
    "    This matches pyBLP's approach:\n",
    "    1. Start with observed shares\n",
    "    2. Back out mean utilities (delta) that rationalize these shares via contraction mapping\n",
    "    3. Compute individual choice probabilities using delta + random coefficients\n",
    "    4. Compute elasticities via analytical derivatives\n",
    "    \n",
    "    Key difference from old method:\n",
    "    - OLD: Uses TRUE xi to compute shares, then elasticities (wrong for comparison!)\n",
    "    - NEW: Uses OBSERVED shares, backs out delta, then computes elasticities (correct!)\n",
    "    \"\"\"\n",
    "    J = len(market_df)\n",
    "    prices = market_df['prices'].values\n",
    "    observed_shares = market_df['shares'].values\n",
    "    x, satellite, wired = market_df['x'].values, market_df['satellite'].values, market_df['wired'].values\n",
    "    \n",
    "    # Compute random coefficient deviations (the part that varies across individuals)\n",
    "    # Delta will absorb everything else: beta_x*x + beta_sat*satellite + beta_wired*wired + alpha*prices + xi\n",
    "    rc_deviation = sigma_sat*nu_draws[:,0:1]*satellite + sigma_wired*nu_draws[:,1:2]*wired\n",
    "    \n",
    "    # Back out mean utilities (delta) via contraction mapping\n",
    "    # Goal: Find delta such that observed_shares = E[exp(delta + rc_deviation) / (1 + sum exp(delta + rc_deviation))]\n",
    "    delta = np.log(observed_shares)  # Initial guess\n",
    "    \n",
    "    for iteration in range(1000):\n",
    "        # Compute individual choice probabilities\n",
    "        utilities = delta[np.newaxis, :] + rc_deviation  # Shape: (n_draws, J)\n",
    "        exp_utils = np.exp(utilities)\n",
    "        denom = 1 + exp_utils.sum(axis=1, keepdims=True)\n",
    "        choice_probs = exp_utils / denom  # Shape: (n_draws, J)\n",
    "        \n",
    "        # Predicted shares\n",
    "        predicted_shares = choice_probs.mean(axis=0)\n",
    "        \n",
    "        # Contraction update: delta_new = delta + log(s_obs) - log(s_pred)\n",
    "        delta_new = delta + np.log(observed_shares) - np.log(predicted_shares)\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.max(np.abs(delta_new - delta)) < 1e-14:\n",
    "            delta = delta_new\n",
    "            break\n",
    "        delta = delta_new\n",
    "    \n",
    "    # Compute final choice probabilities with converged delta\n",
    "    utilities = delta[np.newaxis, :] + rc_deviation\n",
    "    exp_utils = np.exp(utilities)\n",
    "    choice_probs = exp_utils / (1 + exp_utils.sum(axis=1, keepdims=True))\n",
    "    \n",
    "    # Compute elasticities using analytical derivatives\n",
    "    elasticities = np.zeros((J, J))\n",
    "    for j in range(J):\n",
    "        for k in range(J):\n",
    "            if j == k:\n",
    "                # Own-price: E[s_ij * (1 - s_ij)]\n",
    "                deriv = alpha * np.mean(choice_probs[:, j] * (1 - choice_probs[:, j]))\n",
    "            else:\n",
    "                # Cross-price: -E[s_ij * s_ik]\n",
    "                deriv = -alpha * np.mean(choice_probs[:, j] * choice_probs[:, k])\n",
    "            \n",
    "            if observed_shares[j] > 1e-10:\n",
    "                elasticities[j, k] = (prices[k] / observed_shares[j]) * deriv\n",
    "    \n",
    "    return elasticities\n",
    "\n",
    "# ============================================================================\n",
    "# Compute elasticities for Q8 comparison\n",
    "# ============================================================================\n",
    "\n",
    "# Compute Nested Logit elasticities (analytical derivatives)\n",
    "# Compute Nested Logit elasticities (analytical derivatives)\n",
    "print(\"Computing Nested Logit Elasticities (Analytical Derivatives)...\")\n",
    "elasticity_matrices_analytic = [compute_nested_logit_elasticities_analytic(\n",
    "    product_data[product_data['market_ids'] == t], alpha_nl, beta_x_nl, rho_sat_nl, rho_wired_nl\n",
    ") for t in range(T)]\n",
    "\n",
    "print(\"Computing True RC Logit Elasticities (True Parameters on OBSERVED shares)...\")\n",
    "# Use the new function that works with observed shares for fair comparison\n",
    "true_elasticity_matrices = [compute_rc_elasticities_observed_shares(\n",
    "    product_data[product_data['market_ids'] == t], all_nu_draws[t], -2.0, 1.0, 4.0, 4.0, 1.0, 1.0\n",
    ") for t in range(T)]\n",
    "\n",
    "avg_elasticity_matrix_nl = np.mean(elasticity_matrices_analytic, axis=0)\n",
    "avg_elasticity_matrix_true = np.mean(true_elasticity_matrices, axis=0)\n",
    "\n",
    "# Comparison table\n",
    "print(\"\\n\" + \"=\"*70 + \"\\nOWN-PRICE ELASTICITY COMPARISON\\n\" + \"=\"*70)\n",
    "print(\"Nested Logit (Estimated) vs RC Logit (True params, observed shares)\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Product': ['Satellite 1', 'Satellite 2', 'Wired 1', 'Wired 2'],\n",
    "    'True (RC)': np.diag(avg_elasticity_matrix_true),\n",
    "    'Estimated (NL)': np.diag(avg_elasticity_matrix_nl),\n",
    "    'Abs % Error': np.abs(100 * (np.diag(avg_elasticity_matrix_nl) - np.diag(avg_elasticity_matrix_true)) / np.diag(avg_elasticity_matrix_true))\n",
    "})\n",
    "print(comparison_df.to_string(index=False, float_format=lambda x: f'{x:8.3f}'))\n",
    "print(f\"\\nMean Absolute % Error: {comparison_df['Abs % Error'].mean():.2f}%\")\n",
    "print(\"\\nNote: NL model misspecified (true DGP is RC), so errors expected\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Store elasticities in product_data\n",
    "product_data['true_elasticity_rc'] = [true_elasticity_matrices[t][j, j] for t in range(T) for j in range(J)]\n",
    "product_data['estimated_elasticity_nl'] = [elasticity_matrices_analytic[t][j, j] for t in range(T) for j in range(J)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b846f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticities_nl = nl_results.compute_elasticities()\n",
    "avg_elasticities_nl = elasticities_nl.reshape((T, J, J)).mean(axis=0)\n",
    "own_elasticities_nl = np.diag(avg_elasticities_nl)\n",
    "own_elasticities_nl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa6d2b",
   "metadata": {},
   "source": [
    "Provide two additional tables showing the true\n",
    "matrix of diversion ratios and the diversion ratios implied by your estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b71944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIVERSION RATIOS\n",
    "# ============================================================================\n",
    "# Using PyBLP's derivative-based method for both RC and NL models\n",
    "# Convention: Diagonal shows diversion to outside option D_j0 instead of D_jj=-1\n",
    "\n",
    "print(\"Computing Diversion Ratios...\")\n",
    "\n",
    "# Unified function using PyBLP's derivative-based approach\n",
    "def compute_diversion_ratios_pyblp(elasticity_matrices, product_data, T, J):\n",
    "    \"\"\"\n",
    "    Compute diversion ratios using pyBLP's derivative-based method.\n",
    "    \n",
    "    This method:\n",
    "    1. Converts elasticities to Jacobian (derivatives)\n",
    "    2. Replaces diagonal with outside option derivative using adding-up constraint\n",
    "    3. Computes diversion ratios as D_jk = -(∂s_k/∂p_j) / (∂s_j/∂p_j)\n",
    "    \n",
    "    Works for any model (RC, NL, etc.) - just supply the elasticity matrices.\n",
    "    \"\"\"\n",
    "    diversion_matrices = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        elast_matrix = elasticity_matrices[t]\n",
    "        market_data_t = product_data[product_data['market_ids'] == t]\n",
    "        shares = market_data_t['shares'].values\n",
    "        prices = market_data_t['prices'].values\n",
    "        \n",
    "        # Convert elasticities to Jacobian (derivatives): ∂s_j/∂p_k = (s_j/p_k) * ε_jk\n",
    "        jacobian = np.zeros((J, J))\n",
    "        for j in range(J):\n",
    "            for k in range(J):\n",
    "                jacobian[j, k] = (shares[j] / prices[k]) * elast_matrix[j, k]\n",
    "        \n",
    "        # PyBLP's method: Replace diagonal with outside option derivative\n",
    "        # ∂s_0/∂p_j = -Σ_k ∂s_k/∂p_j (by adding-up constraint)\n",
    "        jacobian_diag = np.diag(jacobian).copy()\n",
    "        np.fill_diagonal(jacobian, -jacobian.sum(axis=1))\n",
    "        \n",
    "        # Compute diversion ratios: D_jk = -Jacobian[j,k] / Jacobian[j,j]\n",
    "        diversion = -jacobian / jacobian_diag[:, None]\n",
    "        \n",
    "        diversion_matrices.append(diversion)\n",
    "    \n",
    "    return diversion_matrices\n",
    "\n",
    "# --- TRUE DIVERSION RATIOS (from RC model with true parameters on OBSERVED shares) ---\n",
    "# Note: We recompute true elasticities here to ensure we use observed shares\n",
    "print(\"Computing TRUE RC diversion ratios (true params, observed shares)...\")\n",
    "true_elasticity_matrices_for_div = [compute_rc_elasticities_observed_shares(\n",
    "    product_data[product_data['market_ids'] == t], all_nu_draws[t], -2.0, 1.0, 4.0, 4.0, 1.0, 1.0\n",
    ") for t in range(T)]\n",
    "\n",
    "true_diversion_matrices = compute_diversion_ratios_pyblp(\n",
    "    true_elasticity_matrices_for_div, product_data, T, J\n",
    ")\n",
    "true_avg_diversion = np.mean(true_diversion_matrices, axis=0)\n",
    "\n",
    "# --- ESTIMATED DIVERSION RATIOS (from Nested Logit) ---\n",
    "estimated_diversion_matrices = compute_diversion_ratios_pyblp(\n",
    "    elasticity_matrices_analytic, product_data, T, J\n",
    ")\n",
    "estimated_avg_diversion = np.mean(estimated_diversion_matrices, axis=0)\n",
    "\n",
    "# --- DISPLAY RESULTS ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DIVERSION RATIO MATRICES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "product_labels = ['Sat 1', 'Sat 2', 'Wired 1', 'Wired 2']\n",
    "\n",
    "print(\"\\nTrue Diversion Ratios (RC Logit with TRUE params on OBSERVED shares):\")\n",
    "print(\"Diagonal = diversion to outside option D_j0\")\n",
    "true_df = pd.DataFrame(true_avg_diversion, index=product_labels, columns=product_labels)\n",
    "print(true_df.to_string(float_format=lambda x: f'{x:7.4f}'))\n",
    "\n",
    "print(\"\\n\\nEstimated Diversion Ratios (from Nested Logit - PyBLP Method):\")\n",
    "print(\"Diagonal = diversion to outside option D_j0\")\n",
    "est_df = pd.DataFrame(estimated_avg_diversion, index=product_labels, columns=product_labels)\n",
    "print(est_df.to_string(float_format=lambda x: f'{x:7.4f}'))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Note: D_jk = -(∂s_k/∂p_j) / (∂s_j/∂p_j)\")\n",
    "print(\"Off-diagonal: share of j's lost customers who switch to product k\")\n",
    "print(\"Diagonal: share of j's lost customers who leave the market (outside)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ab563",
   "metadata": {},
   "source": [
    "## 5 Estimate the Correctly Specified Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19841e0a",
   "metadata": {},
   "source": [
    "Use the pyBLP software to estimate the correctly specified model. Allow pyBLP to construct\n",
    "approximations to the optimal instruments, using the exogenous demand shifters and exogenous\n",
    "cost shifters. For your own benefit, you may want to see what happens without the approximation of the optimal instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e160701",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data.drop(columns=['nesting_ids'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983afe3",
   "metadata": {},
   "source": [
    "### 9. \n",
    "Report a table with the estimates of the demand parameters and standard errors. Do\n",
    "this twice: once when you estimate demand alone, then again when you estimate jointly\n",
    "with supply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4b54f68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM   Computation  Optimization   Objective   Fixed Point  Contraction  Clipped  Objective   Objective     Projected                        \n",
      "Step     Time       Iterations   Evaluations  Iterations   Evaluations  Shares     Value    Improvement  Gradient Norm         Theta        \n",
      "----  -----------  ------------  -----------  -----------  -----------  -------  ---------  -----------  -------------  --------------------\n",
      " 1     00:00:00         0             1            0           600         0     +5.34E+00                 +2.87E+00    +1.00E+00, +1.00E+00\n",
      " 1     00:00:01         0             2          4014         12450        0     +1.72E+01                 +2.33E+01    +1.65E+00, +1.76E+00\n",
      " 1     00:00:01         0             3          3289         10093        0     +5.04E+00   +3.04E-01     +5.56E-01    +1.09E+00, +1.10E+00\n",
      " 1     00:00:01         1             4          3326         10260        0     +5.03E+00   +8.23E-03     +2.18E-01    +1.10E+00, +1.13E+00\n",
      " 1     00:00:01         2             5          3330         10271        0     +5.03E+00   +2.31E-03     +2.29E-01    +1.09E+00, +1.14E+00\n",
      " 1     00:00:01         2             6          3387         10464        0     +5.02E+00   +7.45E-03     +2.82E-01    +1.06E+00, +1.17E+00\n",
      " 1     00:00:01         3             7          3374         10423        0     +5.01E+00   +1.16E-02     +2.07E-01    +9.65E-01, +1.25E+00\n",
      " 1     00:00:01         4             8          3365         10341        0     +5.00E+00   +1.73E-03     +2.46E-02    +9.48E-01, +1.25E+00\n",
      " 1     00:00:01         5             9          3365         10354        0     +5.00E+00   +7.23E-05     +1.91E-03    +9.40E-01, +1.26E+00\n",
      " 1     00:00:02         6            10          3369         10360        0     +5.00E+00   +2.73E-07     +2.26E-05    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         7            11          3365         10355        0     +5.00E+00   +9.18E-11     +6.37E-07    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         8            12          3367         10361        0     +5.00E+00   +5.24E-14     +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            13          3368         10360        0     +5.00E+00                 +1.13E-11    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            14          3368         10357        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            15          3367         10361        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            16          3366         10362        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            17          3367         10361        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            18          3366         10355        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            19          3367         10361        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            20          3365         10363        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            21          3367         10361        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            22          3364         10363        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            23          3367         10361        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            24          3364         10357        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            25          3367         10361        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            26          3368         10369        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            27          3363         10345        0     +5.00E+00                 +4.43E-07    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            28          3369         10372        0     +5.00E+00                 +1.75E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            29          3367         10361        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            30          3368         10370        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            31          3367         10361        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            32          3366         10357        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            33          3367         10361        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            34          3366         10357        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      " 1     00:00:01         9            35          3367         10361        0     +5.00E+00                 +1.82E-08    +9.39E-01, +1.26E+00\n",
      "\n",
      "Optimization completed after 00:00:40.\n",
      "Computing the Hessian and updating the weighting matrix ...\n",
      "Computed results after 00:00:05.\n",
      "\n",
      "Problem Results Summary:\n",
      "==============================================================================================================\n",
      "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Clipped  Weighting Matrix  Covariance Matrix\n",
      "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue   Shares   Condition Number  Condition Number \n",
      "----  ---------  -------------  ---------------  ---------------  -------  ----------------  -----------------\n",
      " 1    +5.00E+00    +1.82E-08       +1.11E+00        +2.63E+01        0        +1.59E+03          +3.01E+05    \n",
      "==============================================================================================================\n",
      "\n",
      "Starting optimization ...\n",
      "\n",
      "GMM   Computation  Optimization   Objective   Fixed Point  Contraction  Clipped  Objective   Objective     Projected                        \n",
      "Step     Time       Iterations   Evaluations  Iterations   Evaluations  Shares     Value    Improvement  Gradient Norm         Theta        \n",
      "----  -----------  ------------  -----------  -----------  -----------  -------  ---------  -----------  -------------  --------------------\n",
      " 2     00:00:00         0             1            0           600         0     +4.92E+00                 +5.54E-03    +9.39E-01, +1.26E+00\n",
      " 2     00:00:01         0             2          2973         9294         0     +4.92E+00                 +1.29E-01    +9.36E-01, +1.25E+00\n",
      " 2     00:00:01         0             3          2728         8571         0     +4.92E+00   +8.41E-07     +4.02E-04    +9.39E-01, +1.26E+00\n",
      " 2     00:00:01         1             4          2726         8554         0     +4.92E+00   +8.62E-09     +3.85E-04    +9.39E-01, +1.26E+00\n",
      " 2     00:00:01         1             5          2731         8528         0     +4.92E+00   +3.07E-08     +3.16E-04    +9.39E-01, +1.26E+00\n",
      " 2     00:00:01         2             6          2955         9248         0     +4.92E+00   +6.45E-08     +8.28E-07    +9.39E-01, +1.26E+00\n",
      " 2     00:00:01         3             7          2956         9264         0     +4.92E+00   +3.38E-14     +1.15E-10    +9.39E-01, +1.26E+00\n",
      "\n",
      "Optimization completed after 00:00:06.\n",
      "Computing the Hessian and estimating standard errors ...\n",
      "Computed results after 00:00:05.\n",
      "\n",
      "Problem Results Summary:\n",
      "==============================================================================================================\n",
      "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Clipped  Weighting Matrix  Covariance Matrix\n",
      "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue   Shares   Condition Number  Condition Number \n",
      "----  ---------  -------------  ---------------  ---------------  -------  ----------------  -----------------\n",
      " 2    +4.92E+00    +1.15E-10       +1.08E+00        +2.57E+01        0        +1.59E+03          +3.02E+05    \n",
      "==============================================================================================================\n",
      "\n",
      "Cumulative Statistics:\n",
      "===========================================================================\n",
      "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
      "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
      "-----------  ---------  ------------  -----------  -----------  -----------\n",
      " 00:00:58       Yes          14           45         142030       439654   \n",
      "===========================================================================\n",
      "\n",
      "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
      "===================================\n",
      " Sigma:     satellite      wired   \n",
      "---------  -----------  -----------\n",
      "satellite   +9.39E-01              \n",
      "           (+2.44E+00)             \n",
      "                                   \n",
      "  wired     +0.00E+00    +1.26E+00 \n",
      "                        (+1.86E+00)\n",
      "===================================\n",
      "\n",
      "Beta Estimates (Robust SEs in Parentheses):\n",
      "==================================================\n",
      "  prices          x        satellite      wired   \n",
      "-----------  -----------  -----------  -----------\n",
      " -2.03E+00    +9.97E-01    +4.08E+00    +3.99E+00 \n",
      "(+7.67E-02)  (+4.72E-02)  (+9.74E-01)  (+7.01E-01)\n",
      "==================================================\n",
      "Computing optimal instruments for theta ...\n",
      "Computed optimal instruments after 00:00:00.\n",
      "\n",
      "Optimal Instrument Results Summary:\n",
      "=======================\n",
      "Computation  Error Term\n",
      "   Time        Draws   \n",
      "-----------  ----------\n",
      " 00:00:00        1     \n",
      "=======================\n",
      "Re-creating the problem ...\n",
      "Re-created the problem after 00:00:00.\n",
      "\n",
      "Dimensions:\n",
      "========================================\n",
      " T    N     F     I      K1    K2    MD \n",
      "---  ----  ---  ------  ----  ----  ----\n",
      "600  2400   4   600000   4     2     6  \n",
      "========================================\n",
      "\n",
      "Formulations:\n",
      "=================================================================\n",
      "       Column Indices:             0        1        2        3  \n",
      "-----------------------------  ---------  -----  ---------  -----\n",
      " X1: Linear Characteristics     prices      x    satellite  wired\n",
      "X2: Nonlinear Characteristics  satellite  wired                  \n",
      "=================================================================\n",
      "Solving the problem ...\n",
      "\n",
      "Nonlinear Coefficient Initial Values:\n",
      "===============================\n",
      " Sigma:    satellite    wired  \n",
      "---------  ---------  ---------\n",
      "satellite  +1.00E+00           \n",
      "  wired    +0.00E+00  +1.00E+00\n",
      "===============================\n",
      "\n",
      "Nonlinear Coefficient Lower Bounds:\n",
      "===============================\n",
      " Sigma:    satellite    wired  \n",
      "---------  ---------  ---------\n",
      "satellite  +0.00E+00           \n",
      "  wired    +0.00E+00  +0.00E+00\n",
      "===============================\n",
      "\n",
      "Nonlinear Coefficient Upper Bounds:\n",
      "===============================\n",
      " Sigma:    satellite    wired  \n",
      "---------  ---------  ---------\n",
      "satellite    +INF              \n",
      "  wired    +0.00E+00    +INF   \n",
      "===============================\n",
      "\n",
      "Updating starting values for the weighting matrix and delta ...\n",
      "Computed results after 00:00:01.\n",
      "\n",
      "Starting optimization ...\n",
      "\n",
      "GMM   Computation  Optimization   Objective   Fixed Point  Contraction  Clipped  Objective   Objective     Projected                        \n",
      "Step     Time       Iterations   Evaluations  Iterations   Evaluations  Shares     Value    Improvement  Gradient Norm         Theta        \n",
      "----  -----------  ------------  -----------  -----------  -----------  -------  ---------  -----------  -------------  --------------------\n",
      " 1     00:00:00         0             1            0           600         0     +1.91E+00                 +1.06E+01    +1.00E+00, +1.00E+00\n",
      " 1     00:00:01         0             2          3922         12185        0     +1.34E+01                 +4.17E+01    +2.00E+00, +9.06E-01\n",
      " 1     00:00:01         0             3          3393         10492        0     +7.05E-02   +1.84E+00     +2.04E+00    +1.27E+00, +9.74E-01\n",
      " 1     00:00:01         1             4          3439         10596        0     +1.85E-03   +6.87E-02     +3.37E-01    +1.31E+00, +9.26E-01\n",
      " 1     00:00:01         2             5          3422         10592        0     +2.96E-06   +1.85E-03     +1.55E-02    +1.30E+00, +9.21E-01\n",
      " 1     00:00:01         3             6          3427         10589        0     +2.95E-09   +2.95E-06     +3.91E-04    +1.30E+00, +9.21E-01\n",
      " 1     00:00:01         4             7          3421         10582        0     +9.51E-13   +2.95E-09     +7.23E-06    +1.30E+00, +9.21E-01\n",
      " 1     00:00:01         5             8          3423         10575        0     +1.66E-16   +9.51E-13     +9.33E-08    +1.30E+00, +9.21E-01\n",
      " 1     00:00:01         6             9          3420         10574        0     +1.46E-21   +1.66E-16     +3.84E-10    +1.30E+00, +9.21E-01\n",
      "\n",
      "Optimization completed after 00:00:09.\n",
      "Computing the Hessian and updating the weighting matrix ...\n",
      "Computed results after 00:00:06.\n",
      "\n",
      "Problem Results Summary:\n",
      "==============================================================================================================\n",
      "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Clipped  Weighting Matrix  Covariance Matrix\n",
      "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue   Shares   Condition Number  Condition Number \n",
      "----  ---------  -------------  ---------------  ---------------  -------  ----------------  -----------------\n",
      " 1    +1.46E-21    +3.84E-10       +2.94E+01        +4.65E+01        0        +3.36E+03          +2.74E+03    \n",
      "==============================================================================================================\n",
      "\n",
      "Starting optimization ...\n",
      "\n",
      "GMM   Computation  Optimization   Objective   Fixed Point  Contraction  Clipped  Objective   Objective     Projected                        \n",
      "Step     Time       Iterations   Evaluations  Iterations   Evaluations  Shares     Value    Improvement  Gradient Norm         Theta        \n",
      "----  -----------  ------------  -----------  -----------  -----------  -------  ---------  -----------  -------------  --------------------\n",
      " 2     00:00:00         0             1            0           600         0     +1.33E-21                 +2.01E-10    +1.30E+00, +9.21E-01\n",
      "\n",
      "Optimization completed after 00:00:00.\n",
      "Computing the Hessian and estimating standard errors ...\n",
      "Computed results after 00:00:03.\n",
      "\n",
      "Problem Results Summary:\n",
      "==============================================================================================================\n",
      "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Clipped  Weighting Matrix  Covariance Matrix\n",
      "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue   Shares   Condition Number  Condition Number \n",
      "----  ---------  -------------  ---------------  ---------------  -------  ----------------  -----------------\n",
      " 2    +1.33E-21    +2.01E-10       +2.94E+01        +4.50E+01        0        +3.39E+03          +2.74E+03    \n",
      "==============================================================================================================\n",
      "\n",
      "Cumulative Statistics:\n",
      "===========================================================================\n",
      "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
      "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
      "-----------  ---------  ------------  -----------  -----------  -----------\n",
      " 00:00:20       Yes          7            13          34943       109900   \n",
      "===========================================================================\n",
      "\n",
      "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
      "===================================\n",
      " Sigma:     satellite      wired   \n",
      "---------  -----------  -----------\n",
      "satellite   +1.30E+00              \n",
      "           (+2.23E-01)             \n",
      "                                   \n",
      "  wired     +0.00E+00    +9.21E-01 \n",
      "                        (+2.51E-01)\n",
      "===================================\n",
      "\n",
      "Beta Estimates (Robust SEs in Parentheses):\n",
      "==================================================\n",
      "  prices          x        satellite      wired   \n",
      "-----------  -----------  -----------  -----------\n",
      " -2.03E+00    +1.00E+00    +3.91E+00    +4.10E+00 \n",
      "(+6.74E-02)  (+4.37E-02)  (+2.27E-01)  (+2.20E-01)\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Problem Results Summary:\n",
       "==============================================================================================================\n",
       "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Clipped  Weighting Matrix  Covariance Matrix\n",
       "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue   Shares   Condition Number  Condition Number \n",
       "----  ---------  -------------  ---------------  ---------------  -------  ----------------  -----------------\n",
       " 2    +1.33E-21    +2.01E-10       +2.94E+01        +4.50E+01        0        +3.39E+03          +2.74E+03    \n",
       "==============================================================================================================\n",
       "\n",
       "Cumulative Statistics:\n",
       "===========================================================================\n",
       "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
       "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
       "-----------  ---------  ------------  -----------  -----------  -----------\n",
       " 00:00:20       Yes          7            13          34943       109900   \n",
       "===========================================================================\n",
       "\n",
       "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
       "===================================\n",
       " Sigma:     satellite      wired   \n",
       "---------  -----------  -----------\n",
       "satellite   +1.30E+00              \n",
       "           (+2.23E-01)             \n",
       "                                   \n",
       "  wired     +0.00E+00    +9.21E-01 \n",
       "                        (+2.51E-01)\n",
       "===================================\n",
       "\n",
       "Beta Estimates (Robust SEs in Parentheses):\n",
       "==================================================\n",
       "  prices          x        satellite      wired   \n",
       "-----------  -----------  -----------  -----------\n",
       " -2.03E+00    +1.00E+00    +3.91E+00    +4.10E+00 \n",
       "(+6.74E-02)  (+4.37E-02)  (+2.27E-01)  (+2.20E-01)\n",
       "=================================================="
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_formulation = pyblp.Formulation('0 + prices + x + satellite + wired')\n",
    "X2_formulation = pyblp.Formulation('0 + satellite + wired')\n",
    "product_formulations1 = (X1_formulation, X2_formulation)\n",
    "product_data['demand_instruments0'] = product_data['w']\n",
    "product_data['demand_instruments1'] = product_data['x**2']\n",
    "product_data['demand_instruments2'] = product_data['w**2']\n",
    "product_data['demand_instruments3'] = product_data['x*w']\n",
    "product_data['demand_instruments4'] = product_data['sum_x_competitors']\n",
    "product_data['demand_instruments5'] = product_data['sum_w_competitors']\n",
    "product_data['demand_instruments6'] = product_data['x_other_in_nest']\n",
    "product_data['demand_instruments7'] = product_data['w_other_in_nest']\n",
    "integration =  pyblp.Integration('monte_carlo', size=1000, specification_options={'seed': 1995})\n",
    "problem1 = pyblp.Problem(product_formulations1, product_data, integration=integration)\n",
    "results1 = problem1.solve(sigma=np.eye(2), initial_update=True)\n",
    "optimal_iv1 = results1.compute_optimal_instruments(seed=1995)\n",
    "optimal_problem1 = optimal_iv1.to_problem()\n",
    "optimal_iv_results1 = optimal_problem1.solve(sigma=np.eye(2), initial_update=True)\n",
    "optimal_iv_results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d591ff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the problem ...\n",
      "Initialized the problem after 00:00:00.\n",
      "\n",
      "Dimensions:\n",
      "====================================================\n",
      " T    N     F     I      K1    K2    K3    MD    MS \n",
      "---  ----  ---  ------  ----  ----  ----  ----  ----\n",
      "600  2400   4   600000   4     2     2     6     2  \n",
      "====================================================\n",
      "\n",
      "Formulations:\n",
      "=================================================================\n",
      "       Column Indices:             0        1        2        3  \n",
      "-----------------------------  ---------  -----  ---------  -----\n",
      " X1: Linear Characteristics     prices      x    satellite  wired\n",
      "X2: Nonlinear Characteristics  satellite  wired                  \n",
      "X3: Log Cost Characteristics       1        w                    \n",
      "=================================================================\n",
      "Solving the problem ...\n",
      "\n",
      "Nonlinear Coefficient Initial Values:\n",
      "===============================\n",
      " Sigma:    satellite    wired  \n",
      "---------  ---------  ---------\n",
      "satellite  +1.00E+00           \n",
      "  wired    +0.00E+00  +1.00E+00\n",
      "===============================\n",
      "\n",
      "Beta Initial Values:\n",
      "==========================================\n",
      " prices        x      satellite    wired  \n",
      "---------  ---------  ---------  ---------\n",
      "-2.03E+00  +1.00E+00  +3.91E+00  +4.10E+00\n",
      "==========================================\n",
      "\n",
      "Nonlinear Coefficient Lower Bounds:\n",
      "===============================\n",
      " Sigma:    satellite    wired  \n",
      "---------  ---------  ---------\n",
      "satellite  +0.00E+00           \n",
      "  wired    +0.00E+00  +0.00E+00\n",
      "===============================\n",
      "\n",
      "Beta Lower Bounds:\n",
      "==========================================\n",
      " prices        x      satellite    wired  \n",
      "---------  ---------  ---------  ---------\n",
      "  -INF       -INF       -INF       -INF   \n",
      "==========================================\n",
      "\n",
      "Nonlinear Coefficient Upper Bounds:\n",
      "===============================\n",
      " Sigma:    satellite    wired  \n",
      "---------  ---------  ---------\n",
      "satellite    +INF              \n",
      "  wired    +0.00E+00    +INF   \n",
      "===============================\n",
      "\n",
      "Beta Upper Bounds:\n",
      "==========================================\n",
      " prices        x      satellite    wired  \n",
      "---------  ---------  ---------  ---------\n",
      "  +INF       +INF       +INF       +INF   \n",
      "==========================================\n",
      "\n",
      "Updating starting values for the weighting matrix and delta ...\n",
      "Computed results after 00:00:02.\n",
      "\n",
      "Starting optimization ...\n",
      "\n",
      "GMM   Computation  Optimization   Objective   Fixed Point  Contraction  Clipped  Objective   Objective     Projected                                                                    \n",
      "Step     Time       Iterations   Evaluations  Iterations   Evaluations  Shares     Value    Improvement  Gradient Norm                               Theta                              \n",
      "----  -----------  ------------  -----------  -----------  -----------  -------  ---------  -----------  -------------  ----------------------------------------------------------------\n",
      " 1     00:00:01         0             1            0           600         0     +1.87E+01                 +6.40E+02    +1.00E+00, +1.00E+00, -2.03E+00, +1.00E+00, +3.91E+00, +4.10E+00\n",
      "\n",
      "At least one error was encountered. As long as the optimization routine does not get stuck at values of theta that give rise to errors, this is not necessarily a problem. If the errors persist or seem to be impacting the optimization results, consider setting an error punishment or following any of the other suggestions below:\n",
      "Encountered nonpositive marginal costs in a log-linear specification. This problem can sometimes be mitigated by bounding costs from below, choosing more reasonable initial parameter values, setting more conservative parameter bounds, or using a linear costs specification.\n",
      "Reverted problematic marginal costs. Number of reverted elements: 1 out of 2400.\n",
      "\n",
      " 1     00:00:02         0             2          3220         9889         0     +2.40E+04                 +4.95E+04    +1.15E+00, +9.99E-01, -1.13E+00, +1.16E+00, +4.28E+00, +4.00E+00\n",
      " 1     00:00:02         0             3          2766         8701         0     +1.36E+01   +5.15E+00     +1.76E+02    +1.00E+00, +1.00E+00, -2.01E+00, +1.01E+00, +3.92E+00, +4.10E+00\n",
      " 1     00:00:02         1             4          2987         9308         0     +1.23E+01   +1.26E+00     +1.67E+02    +1.00E+00, +9.99E-01, -2.01E+00, +1.00E+00, +3.92E+00, +4.10E+00\n",
      " 1     00:00:02         1             5          3297         10270        0     +7.94E+00   +4.36E+00     +1.29E+02    +1.01E+00, +9.94E-01, -2.01E+00, +1.00E+00, +3.93E+00, +4.08E+00\n",
      " 1     00:00:02         2             6          3427         10687        0     +1.41E+00   +6.54E+00     +7.92E+00    +1.03E+00, +9.77E-01, -2.01E+00, +9.90E-01, +3.98E+00, +4.04E+00\n",
      " 1     00:00:02         3             7          3401         10584        0     +1.38E+00   +3.09E-02     +7.77E+00    +1.04E+00, +9.76E-01, -2.01E+00, +9.89E-01, +3.98E+00, +4.04E+00\n",
      " 1     00:00:02         3             8          3352         10434        0     +1.27E+00   +1.07E-01     +7.16E+00    +1.05E+00, +9.71E-01, -2.01E+00, +9.85E-01, +3.97E+00, +4.04E+00\n",
      " 1     00:00:02         4             9          3365         10453        0     +8.36E-01   +4.34E-01     +2.16E+01    +1.11E+00, +9.44E-01, -2.01E+00, +9.76E-01, +3.95E+00, +4.05E+00\n",
      " 1     00:00:02         5            10          3412         10534        0     +2.85E-01   +5.50E-01     +1.92E+01    +1.21E+00, +9.02E-01, -2.01E+00, +9.81E-01, +3.91E+00, +4.07E+00\n",
      " 1     00:00:02         6            11          3438         10610        0     +4.51E-02   +2.40E-01     +4.88E+00    +1.27E+00, +8.77E-01, -2.01E+00, +9.97E-01, +3.88E+00, +4.07E+00\n",
      " 1     00:00:02         7            12          3436         10613        0     +3.35E-02   +1.15E-02     +8.60E-01    +1.27E+00, +8.77E-01, -2.01E+00, +9.98E-01, +3.88E+00, +4.07E+00\n",
      " 1     00:00:02         8            13          3436         10609        0     +3.28E-02   +7.50E-04     +1.14E+00    +1.27E+00, +8.77E-01, -2.01E+00, +9.98E-01, +3.88E+00, +4.07E+00\n",
      " 1     00:00:02         9            14          3440         10612        0     +2.89E-02   +3.85E-03     +2.89E+00    +1.28E+00, +8.79E-01, -2.01E+00, +9.98E-01, +3.87E+00, +4.07E+00\n",
      " 1     00:00:02         10           15          3438         10595        0     +2.31E-02   +5.79E-03     +5.38E+00    +1.28E+00, +8.84E-01, -2.01E+00, +9.99E-01, +3.87E+00, +4.06E+00\n",
      " 1     00:00:02         11           16          3427         10571        0     +2.06E-02   +2.56E-03     +2.21E+01    +1.28E+00, +8.94E-01, -2.01E+00, +1.00E+00, +3.85E+00, +4.05E+00\n",
      " 1     00:00:02         12           17          3426         10585        0     +7.69E-03   +1.29E-02     +4.26E+00    +1.29E+00, +9.00E-01, -2.00E+00, +1.00E+00, +3.85E+00, +4.04E+00\n",
      " 1     00:00:02         13           18          3423         10558        0     +2.49E-03   +5.20E-03     +6.41E+00    +1.28E+00, +9.02E-01, -2.00E+00, +1.00E+00, +3.85E+00, +4.04E+00\n",
      " 1     00:00:02         14           19          3416         10559        0     +4.04E-04   +2.09E-03     +1.43E+00    +1.28E+00, +9.07E-01, -2.00E+00, +9.99E-01, +3.85E+00, +4.03E+00\n",
      " 1     00:00:02         15           20          3425         10574        0     +3.14E-04   +8.96E-05     +1.88E-01    +1.28E+00, +9.05E-01, -2.00E+00, +9.99E-01, +3.85E+00, +4.04E+00\n",
      " 1     00:00:02         16           21          3421         10572        0     +3.00E-04   +1.40E-05     +8.93E-02    +1.28E+00, +9.06E-01, -2.00E+00, +9.99E-01, +3.85E+00, +4.04E+00\n",
      " 1     00:00:02         17           22          3417         10562        0     +2.90E-04   +9.63E-06     +9.47E-02    +1.28E+00, +9.06E-01, -2.00E+00, +9.99E-01, +3.85E+00, +4.04E+00\n",
      " 1     00:00:02         17           23          3422         10570        0     +2.61E-04   +2.90E-05     +2.13E-01    +1.28E+00, +9.06E-01, -2.00E+00, +9.99E-01, +3.85E+00, +4.04E+00\n",
      " 1     00:00:02         18           24          3421         10564        0     +1.97E-04   +6.45E-05     +2.77E-01    +1.28E+00, +9.05E-01, -2.00E+00, +9.99E-01, +3.85E+00, +4.04E+00\n",
      " 1     00:00:02         19           25          3423         10577        0     +8.06E-05   +1.16E-04     +4.59E-01    +1.28E+00, +9.04E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         20           26          3420         10560        0     +2.02E-05   +6.04E-05     +2.73E-01    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         21           27          3419         10573        0     +7.60E-06   +1.26E-05     +1.21E-01    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         22           28          3419         10565        0     +3.55E-06   +4.05E-06     +4.65E-01    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         23           29          3421         10558        0     +9.04E-07   +2.64E-06     +1.92E-01    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         24           30          3420         10559        0     +1.07E-07   +7.97E-07     +1.79E-02    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         25           31          3421         10570        0     +8.90E-09   +9.82E-08     +7.10E-03    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         26           32          3420         10561        0     +1.47E-10   +8.75E-09     +1.48E-03    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         27           33          3417         10565        0     +1.84E-10                 +7.70E-04    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         27           34          3421         10563        0     +3.22E-11   +1.15E-10     +1.15E-03    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         28           35          3421         10563        0     +9.62E-12   +2.26E-11     +5.99E-04    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         29           36          3423         10569        0     +3.90E-13   +9.23E-12     +1.92E-05    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         30           37          3419         10562        0     +2.43E-13   +1.47E-13     +3.22E-05    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         31           38          3419         10567        0     +6.80E-14   +1.75E-13     +4.69E-05    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         32           39          3423         10557        0     +2.63E-14   +4.17E-14     +1.64E-05    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         33           40          3417         10547        0     +1.82E-14   +8.16E-15     +4.47E-06    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         34           41          3420         10566        0     +8.43E-15   +9.73E-15     +7.79E-06    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         35           42          3422         10559        0     +2.96E-15   +5.47E-15     +8.71E-06    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         36           43          3421         10566        0     +3.72E-15                 +7.92E-06    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         36           44          3423         10563        0     +1.49E-15   +1.47E-15     +8.35E-06    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         37           45          3420         10566        0     +1.24E-12                 +1.16E-04    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         37           46          3418         10549        0     +8.16E-16   +6.77E-16     +5.51E-06    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         38           47          3420         10552        0     +1.28E-16   +6.88E-16     +3.04E-06    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         39           48          3416         10555        0     +5.30E-18   +1.22E-16     +5.37E-07    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         40           49          3419         10558        0     +1.62E-18   +3.69E-18     +1.10E-07    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         41           50          3425         10560        0     +1.24E-18   +3.75E-19     +4.01E-08    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      "\n",
      "GMM   Computation  Optimization   Objective   Fixed Point  Contraction  Clipped  Objective   Objective     Projected                                                                    \n",
      "Step     Time       Iterations   Evaluations  Iterations   Evaluations  Shares     Value    Improvement  Gradient Norm                               Theta                              \n",
      "----  -----------  ------------  -----------  -----------  -----------  -------  ---------  -----------  -------------  ----------------------------------------------------------------\n",
      " 1     00:00:02         42           51          3419         10556        0     +9.89E-19   +2.51E-19     +3.04E-08    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      " 1     00:00:02         43           52          3420         10569        0     +1.44E-19   +8.45E-19     +7.69E-09    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      "\n",
      "Optimization completed after 00:01:47.\n",
      "Computing the Hessian and updating the weighting matrix ...\n",
      "Computed results after 00:00:27.\n",
      "\n",
      "Problem Results Summary:\n",
      "==============================================================================================================\n",
      "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Clipped  Weighting Matrix  Covariance Matrix\n",
      "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue   Shares   Condition Number  Condition Number \n",
      "----  ---------  -------------  ---------------  ---------------  -------  ----------------  -----------------\n",
      " 1    +1.44E-19    +7.69E-09       +1.81E+01        +5.65E+04        0        +6.71E+02          +2.86E+04    \n",
      "==============================================================================================================\n",
      "\n",
      "Starting optimization ...\n",
      "\n",
      "GMM   Computation  Optimization   Objective   Fixed Point  Contraction  Clipped  Objective   Objective     Projected                                                                    \n",
      "Step     Time       Iterations   Evaluations  Iterations   Evaluations  Shares     Value    Improvement  Gradient Norm                               Theta                              \n",
      "----  -----------  ------------  -----------  -----------  -----------  -------  ---------  -----------  -------------  ----------------------------------------------------------------\n",
      " 2     00:00:01         0             1            0           600         0     +1.46E-19                 +7.57E-09    +1.28E+00, +9.03E-01, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      "\n",
      "Optimization completed after 00:00:01.\n",
      "Computing the Hessian and estimating standard errors ...\n",
      "Computed results after 00:00:17.\n",
      "\n",
      "Problem Results Summary:\n",
      "==============================================================================================================\n",
      "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Clipped  Weighting Matrix  Covariance Matrix\n",
      "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue   Shares   Condition Number  Condition Number \n",
      "----  ---------  -------------  ---------------  ---------------  -------  ----------------  -----------------\n",
      " 2    +1.46E-19    +7.57E-09       +1.82E+01        +5.64E+04        0        +6.41E+02          +2.86E+04    \n",
      "==============================================================================================================\n",
      "\n",
      "Cumulative Statistics:\n",
      "===========================================================================\n",
      "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
      "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
      "-----------  ---------  ------------  -----------  -----------  -----------\n",
      " 00:02:33       Yes          44           56         180055       558459   \n",
      "===========================================================================\n",
      "\n",
      "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
      "===================================\n",
      " Sigma:     satellite      wired   \n",
      "---------  -----------  -----------\n",
      "satellite   +1.28E+00              \n",
      "           (+2.23E-01)             \n",
      "                                   \n",
      "  wired     +0.00E+00    +9.03E-01 \n",
      "                        (+2.52E-01)\n",
      "===================================\n",
      "\n",
      "Beta Estimates (Robust SEs in Parentheses):\n",
      "==================================================\n",
      "  prices          x        satellite      wired   \n",
      "-----------  -----------  -----------  -----------\n",
      " -2.00E+00    +9.99E-01    +3.84E+00    +4.03E+00 \n",
      "(+7.17E-02)  (+4.38E-02)  (+2.40E-01)  (+2.34E-01)\n",
      "==================================================\n",
      "\n",
      "Gamma Estimates (Robust SEs in Parentheses):\n",
      "========================\n",
      "     1            w     \n",
      "-----------  -----------\n",
      " +7.98E-01    +2.03E-01 \n",
      "(+1.47E-02)  (+6.74E-03)\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "X3_formulation = pyblp.Formulation('1 + w')\n",
    "product_formulations2 = (X1_formulation, X2_formulation, X3_formulation)\n",
    "columns_to_drop = [col for col in product_data.columns if 'instruments' in col]\n",
    "product_data = product_data.drop(columns=columns_to_drop)\n",
    "product_data['demand_instruments0'] = optimal_iv1.demand_instruments[:, 0]\n",
    "product_data['demand_instruments1'] = optimal_iv1.demand_instruments[:, 1]\n",
    "product_data['demand_instruments2'] = product_data['w']\n",
    "problem2 = pyblp.Problem(product_formulations2, product_data, costs_type='log', integration=integration)\n",
    "results2 = problem2.solve(sigma=np.eye(2), beta=optimal_iv_results1.beta, initial_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4404dc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing optimal instruments for theta ...\n",
      "Computed optimal instruments after 00:00:02.\n",
      "\n",
      "Optimal Instrument Results Summary:\n",
      "=================================================\n",
      "Computation  Error Term  Fixed Point  Contraction\n",
      "   Time        Draws     Iterations   Evaluations\n",
      "-----------  ----------  -----------  -----------\n",
      " 00:00:02        1          8271         8271    \n",
      "=================================================\n",
      "Initializing the problem ...\n",
      "Initialized the problem after 00:00:00.\n",
      "\n",
      "Dimensions:\n",
      "====================================================\n",
      " T    N     F     I      K1    K2    K3    MD    MS \n",
      "---  ----  ---  ------  ----  ----  ----  ----  ----\n",
      "600  2400   4   600000   4     2     2     6     2  \n",
      "====================================================\n",
      "\n",
      "Formulations:\n",
      "=================================================================\n",
      "       Column Indices:             0        1        2        3  \n",
      "-----------------------------  ---------  -----  ---------  -----\n",
      " X1: Linear Characteristics     prices      x    satellite  wired\n",
      "X2: Nonlinear Characteristics  satellite  wired                  \n",
      "X3: Log Cost Characteristics       1        w                    \n",
      "=================================================================\n",
      "Solving the problem ...\n",
      "\n",
      "Nonlinear Coefficient Initial Values:\n",
      "===============================\n",
      " Sigma:    satellite    wired  \n",
      "---------  ---------  ---------\n",
      "satellite  +1.00E+00           \n",
      "  wired    +0.00E+00  +1.00E+00\n",
      "===============================\n",
      "\n",
      "Beta Initial Values:\n",
      "==========================================\n",
      " prices        x      satellite    wired  \n",
      "---------  ---------  ---------  ---------\n",
      "-2.00E+00  +9.99E-01  +3.84E+00  +4.03E+00\n",
      "==========================================\n",
      "\n",
      "Nonlinear Coefficient Lower Bounds:\n",
      "===============================\n",
      " Sigma:    satellite    wired  \n",
      "---------  ---------  ---------\n",
      "satellite  +0.00E+00           \n",
      "  wired    +0.00E+00  +0.00E+00\n",
      "===============================\n",
      "\n",
      "Beta Lower Bounds:\n",
      "==========================================\n",
      " prices        x      satellite    wired  \n",
      "---------  ---------  ---------  ---------\n",
      "  -INF       -INF       -INF       -INF   \n",
      "==========================================\n",
      "\n",
      "Nonlinear Coefficient Upper Bounds:\n",
      "===============================\n",
      " Sigma:    satellite    wired  \n",
      "---------  ---------  ---------\n",
      "satellite    +INF              \n",
      "  wired    +0.00E+00    +INF   \n",
      "===============================\n",
      "\n",
      "Beta Upper Bounds:\n",
      "==========================================\n",
      " prices        x      satellite    wired  \n",
      "---------  ---------  ---------  ---------\n",
      "  +INF       +INF       +INF       +INF   \n",
      "==========================================\n",
      "\n",
      "Updating starting values for the weighting matrix and delta ...\n",
      "Computed results after 00:00:02.\n",
      "\n",
      "Starting optimization ...\n",
      "\n",
      "GMM   Computation  Optimization   Objective   Fixed Point  Contraction  Clipped  Objective   Objective     Projected                                                                    \n",
      "Step     Time       Iterations   Evaluations  Iterations   Evaluations  Shares     Value    Improvement  Gradient Norm                               Theta                              \n",
      "----  -----------  ------------  -----------  -----------  -----------  -------  ---------  -----------  -------------  ----------------------------------------------------------------\n",
      " 1     00:00:01         0             1            0           600         0     +1.75E+01                 +5.49E+02    +1.00E+00, +1.00E+00, -2.00E+00, +9.99E-01, +3.84E+00, +4.03E+00\n",
      "\n",
      "At least one error was encountered. As long as the optimization routine does not get stuck at values of theta that give rise to errors, this is not necessarily a problem. If the errors persist or seem to be impacting the optimization results, consider setting an error punishment or following any of the other suggestions below:\n",
      "Encountered nonpositive marginal costs in a log-linear specification. This problem can sometimes be mitigated by bounding costs from below, choosing more reasonable initial parameter values, setting more conservative parameter bounds, or using a linear costs specification.\n",
      "Reverted problematic marginal costs. Number of reverted elements: 1 out of 2400.\n",
      "\n",
      " 1     00:00:02         0             2          3238         9969         0     +2.32E+04                 +4.88E+04    +1.16E+00, +9.98E-01, -1.13E+00, +1.15E+00, +4.25E+00, +3.89E+00\n",
      " 1     00:00:02         0             3          2767         8700         0     +1.34E+01   +4.11E+00     +1.77E+02    +1.00E+00, +1.00E+00, -1.99E+00, +1.00E+00, +3.85E+00, +4.03E+00\n",
      " 1     00:00:02         1             4          2988         9302         0     +1.21E+01   +1.30E+00     +1.67E+02    +1.00E+00, +9.99E-01, -1.99E+00, +1.00E+00, +3.85E+00, +4.03E+00\n",
      " 1     00:00:02         1             5          3257         10132        0     +7.64E+00   +4.46E+00     +1.27E+02    +1.01E+00, +9.94E-01, -1.99E+00, +9.97E-01, +3.87E+00, +4.02E+00\n",
      " 1     00:00:02         2             6          3346         10453        0     +1.42E+00   +6.22E+00     +8.54E+00    +1.03E+00, +9.79E-01, -1.99E+00, +9.88E-01, +3.91E+00, +3.97E+00\n",
      " 1     00:00:02         3             7          3321         10357        0     +1.40E+00   +2.71E-02     +8.43E+00    +1.04E+00, +9.79E-01, -1.99E+00, +9.87E-01, +3.91E+00, +3.97E+00\n",
      " 1     00:00:02         3             8          3253         10142        0     +1.30E+00   +1.01E-01     +7.96E+00    +1.05E+00, +9.78E-01, -1.99E+00, +9.85E-01, +3.91E+00, +3.98E+00\n",
      " 1     00:00:02         4             9          3283         10094        0     +6.43E-01   +6.52E-01     +2.58E+01    +1.17E+00, +9.66E-01, -2.00E+00, +9.76E-01, +3.89E+00, +4.01E+00\n",
      " 1     00:00:02         5            10          3387         10479        0     +1.28E-01   +5.15E-01     +1.34E+01    +1.28E+00, +9.53E-01, -2.01E+00, +9.94E-01, +3.87E+00, +4.03E+00\n",
      " 1     00:00:02         6            11          3422         10577        0     +1.90E-02   +1.09E-01     +7.38E+00    +1.30E+00, +9.52E-01, -2.01E+00, +1.00E+00, +3.86E+00, +4.04E+00\n",
      " 1     00:00:02         7            12          3426         10603        0     +1.70E-02   +2.03E-03     +8.21E-01    +1.31E+00, +9.52E-01, -2.01E+00, +1.00E+00, +3.86E+00, +4.04E+00\n",
      " 1     00:00:02         8            13          3425         10608        0     +1.69E-02   +9.41E-05     +7.69E-01    +1.30E+00, +9.52E-01, -2.01E+00, +1.00E+00, +3.86E+00, +4.04E+00\n",
      " 1     00:00:02         9            14          3424         10587        0     +1.64E-02   +4.76E-04     +9.71E-01    +1.30E+00, +9.53E-01, -2.01E+00, +1.00E+00, +3.86E+00, +4.04E+00\n",
      " 1     00:00:02         10           15          3420         10573        0     +1.32E-02   +3.23E-03     +1.77E+00    +1.30E+00, +9.57E-01, -2.01E+00, +1.00E+00, +3.86E+00, +4.04E+00\n",
      " 1     00:00:02         11           16          3446         10666        0     +1.18E-02   +1.34E-03     +1.54E+01    +1.31E+00, +9.64E-01, -2.01E+00, +1.00E+00, +3.85E+00, +4.03E+00\n",
      " 1     00:00:02         12           17          3447         10675        0     +6.42E-03   +5.42E-03     +3.89E+00    +1.31E+00, +9.70E-01, -2.01E+00, +1.00E+00, +3.85E+00, +4.03E+00\n",
      " 1     00:00:02         13           18          3450         10680        0     +2.50E-03   +3.91E-03     +2.19E+00    +1.31E+00, +9.75E-01, -2.01E+00, +1.00E+00, +3.85E+00, +4.02E+00\n",
      " 1     00:00:02         14           19          3454         10705        0     +8.53E-04   +1.65E-03     +2.15E+00    +1.31E+00, +9.79E-01, -2.01E+00, +1.00E+00, +3.85E+00, +4.02E+00\n",
      " 1     00:00:02         15           20          3455         10706        0     +7.13E-04   +1.41E-04     +3.89E-01    +1.31E+00, +9.80E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         16           21          3454         10714        0     +6.97E-04   +1.57E-05     +3.31E-01    +1.31E+00, +9.80E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         17           22          3460         10722        0     +6.89E-04   +7.62E-06     +5.66E-01    +1.31E+00, +9.80E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         18           23          3463         10727        0     +6.50E-04   +3.91E-05     +1.33E+00    +1.31E+00, +9.79E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         19           24          3458         10709        0     +5.77E-04   +7.33E-05     +2.13E+00    +1.31E+00, +9.79E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         20           25          3458         10725        0     +4.18E-04   +1.59E-04     +2.97E+00    +1.31E+00, +9.78E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         21           26          3458         10702        0     +2.08E-04   +2.11E-04     +2.90E+00    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         22           27          3458         10703        0     +2.04E-03                 +5.46E+00    +1.31E+00, +9.80E-01, -2.00E+00, +1.00E+00, +3.84E+00, +4.01E+00\n",
      " 1     00:00:02         22           28          3457         10710        0     +1.83E-04   +2.52E-05     +3.17E+00    +1.31E+00, +9.77E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         23           29          3460         10713        0     +2.45E-04                 +1.43E+00    +1.32E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         23           30          3461         10722        0     +1.27E-04   +5.52E-05     +1.31E+00    +1.31E+00, +9.77E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         24           31          3456         10714        0     +3.22E-05   +9.52E-05     +6.76E-01    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         25           32          3457         10707        0     +6.19E-06   +2.60E-05     +1.04E-01    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         26           33          3452         10702        0     +1.20E-06   +5.00E-06     +9.36E-02    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         27           34          3456         10696        0     +2.19E-08   +1.18E-06     +7.95E-03    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         28           35          3452         10695        0     +3.63E-09   +1.83E-08     +1.90E-03    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         29           36          3452         10690        0     +1.42E-09   +2.21E-09     +1.86E-03    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         30           37          3459         10697        0     +1.15E-10   +1.31E-09     +1.33E-03    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         31           38          3449         10692        0     +7.79E-12   +1.07E-10     +4.55E-04    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         32           39          3454         10694        0     +3.30E-15   +7.79E-12     +1.77E-06    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         33           40          3453         10701        0     +1.47E-17   +3.29E-15     +1.41E-07    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         34           41          3450         10703        0     +5.96E-17                 +1.95E-06    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         34           42          3457         10711        0     +1.12E-17   +3.51E-18     +3.08E-07    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         35           43          3451         10686        0     +5.17E-19   +1.07E-17     +2.15E-08    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         36           44          3451         10702        0     +2.90E-19   +2.27E-19     +1.52E-08    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         37           45          3454         10699        0     +2.38E-19   +5.17E-20     +1.88E-08    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      " 1     00:00:02         38           46          3451         10691        0     +1.16E-21   +2.37E-19     +2.81E-09    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      "\n",
      "Optimization completed after 00:01:34.\n",
      "Computing the Hessian and updating the weighting matrix ...\n",
      "Computed results after 00:00:26.\n",
      "\n",
      "Problem Results Summary:\n",
      "==============================================================================================================\n",
      "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Clipped  Weighting Matrix  Covariance Matrix\n",
      "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue   Shares   Condition Number  Condition Number \n",
      "----  ---------  -------------  ---------------  ---------------  -------  ----------------  -----------------\n",
      " 1    +1.16E-21    +2.81E-09       +1.84E+01        +5.69E+04        0        +3.74E+03          +2.80E+04    \n",
      "==============================================================================================================\n",
      "\n",
      "Starting optimization ...\n",
      "\n",
      "GMM   Computation  Optimization   Objective   Fixed Point  Contraction  Clipped  Objective   Objective     Projected                                                                    \n",
      "Step     Time       Iterations   Evaluations  Iterations   Evaluations  Shares     Value    Improvement  Gradient Norm                               Theta                              \n",
      "----  -----------  ------------  -----------  -----------  -----------  -------  ---------  -----------  -------------  ----------------------------------------------------------------\n",
      " 2     00:00:01         0             1            0           600         0     +1.15E-21                 +2.74E-09    +1.31E+00, +9.76E-01, -2.01E+00, +1.00E+00, +3.84E+00, +4.02E+00\n",
      "\n",
      "Optimization completed after 00:00:01.\n",
      "Computing the Hessian and estimating standard errors ...\n",
      "Computed results after 00:00:16.\n",
      "\n",
      "Problem Results Summary:\n",
      "==============================================================================================================\n",
      "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Clipped  Weighting Matrix  Covariance Matrix\n",
      "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue   Shares   Condition Number  Condition Number \n",
      "----  ---------  -------------  ---------------  ---------------  -------  ----------------  -----------------\n",
      " 2    +1.15E-21    +2.74E-09       +1.84E+01        +5.60E+04        0        +3.73E+03          +2.80E+04    \n",
      "==============================================================================================================\n",
      "\n",
      "Cumulative Statistics:\n",
      "===========================================================================\n",
      "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
      "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
      "-----------  ---------  ------------  -----------  -----------  -----------\n",
      " 00:02:19       Yes          39           50         160157       498167   \n",
      "===========================================================================\n",
      "\n",
      "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
      "===================================\n",
      " Sigma:     satellite      wired   \n",
      "---------  -----------  -----------\n",
      "satellite   +1.31E+00              \n",
      "           (+2.25E-01)             \n",
      "                                   \n",
      "  wired     +0.00E+00    +9.76E-01 \n",
      "                        (+2.52E-01)\n",
      "===================================\n",
      "\n",
      "Beta Estimates (Robust SEs in Parentheses):\n",
      "==================================================\n",
      "  prices          x        satellite      wired   \n",
      "-----------  -----------  -----------  -----------\n",
      " -2.01E+00    +1.00E+00    +3.84E+00    +4.02E+00 \n",
      "(+6.90E-02)  (+4.37E-02)  (+2.36E-01)  (+2.32E-01)\n",
      "==================================================\n",
      "\n",
      "Gamma Estimates (Robust SEs in Parentheses):\n",
      "========================\n",
      "     1            w     \n",
      "-----------  -----------\n",
      " +7.95E-01    +2.04E-01 \n",
      "(+1.52E-02)  (+6.94E-03)\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "# Re-estimate with optimal instruments\n",
    "columns_to_drop = [col for col in product_data.columns \n",
    "                   if 'instruments' in col]\n",
    "product_data = product_data.drop(columns=columns_to_drop)\n",
    "optimal_iv2 = results2.compute_optimal_instruments(seed=1995)\n",
    "for i in range(optimal_iv2.demand_instruments.shape[1]-3):\n",
    "    product_data[f'demand_instruments{i}'] = optimal_iv2.demand_instruments[:, i]\n",
    "problem3 = pyblp.Problem(product_formulations2, product_data, \n",
    "                         costs_type='log', integration=integration)\n",
    "optimal_iv_results2 = problem3.solve(sigma=np.eye(2), beta=results2.beta, initial_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "06d4c927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta Comparison:\n",
      "          Estimates                 SEs            \n",
      "            PyBLP D PyBLP D & S PyBLP D PyBLP D & S\n",
      "prices       -2.025      -2.005   0.067       0.069\n",
      "x             1.003       1.004   0.044       0.044\n",
      "satellite     3.912       3.839   0.227       0.236\n",
      "wired         4.102       4.017   0.220       0.232\n",
      "\n",
      "Sigma Comparison:\n",
      "          Estimates                 SEs            \n",
      "            PyBLP D PyBLP D & S PyBLP D PyBLP D & S\n",
      "satellite     1.298       1.312   0.223       0.225\n",
      "wired         0.921       0.976   0.251       0.252\n",
      "\n",
      "\n",
      "Gamma Estimates (only from joint D & S estimation):\n",
      "    Estimates         SEs\n",
      "  PyBLP D & S PyBLP D & S\n",
      "1       0.795       0.015\n",
      "w       0.204       0.007\n"
     ]
    }
   ],
   "source": [
    "# Compare individual and joint PyBLP estimates for beta\n",
    "pyblp_beta_comparison = pd.DataFrame(index=optimal_iv_results1.beta_labels, data={\n",
    "    (\"Estimates\", \"PyBLP D\"): optimal_iv_results1.beta.flat,  # prices, x, satellite, wired\n",
    "    (\"Estimates\", \"PyBLP D & S\"): optimal_iv_results2.beta.flat,\n",
    "    (\"SEs\", \"PyBLP D\"): optimal_iv_results1.beta_se.flat,\n",
    "    (\"SEs\", \"PyBLP D & S\"): optimal_iv_results2.beta_se.flat\n",
    "})\n",
    "print(\"Beta Comparison:\")\n",
    "print(pyblp_beta_comparison)\n",
    "# Compare sigma estimates\n",
    "pyblp_sigma_comparison = pd.DataFrame(index=optimal_iv_results1.sigma_labels, data={\n",
    "    (\"Estimates\", \"PyBLP D\"): optimal_iv_results1.sigma.diagonal(),\n",
    "    (\"Estimates\", \"PyBLP D & S\"): optimal_iv_results2.sigma.diagonal(),\n",
    "    (\"SEs\", \"PyBLP D\"): optimal_iv_results1.sigma_se.diagonal(),\n",
    "    (\"SEs\", \"PyBLP D & S\"): optimal_iv_results2.sigma_se.diagonal()\n",
    "})\n",
    "print(\"\\nSigma Comparison:\")\n",
    "print(pyblp_sigma_comparison)\n",
    "\n",
    "# Compare gamma estimates (only available in joint estimation)\n",
    "print(\"\\n\\nGamma Estimates (only from joint D & S estimation):\")\n",
    "pyblp_gamma_comparison = pd.DataFrame(index=optimal_iv_results2.gamma_labels, data={\n",
    "    (\"Estimates\", \"PyBLP D & S\"): optimal_iv_results2.gamma.flat,\n",
    "    (\"SEs\", \"PyBLP D & S\"): optimal_iv_results2.gamma_se.flat,\n",
    "})\n",
    "print(pyblp_gamma_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8228d9",
   "metadata": {},
   "source": [
    "### 10. \n",
    "Using your preferred estimates from the prior step (explain your preference), provide\n",
    "a table comparing the estimated own-price elasticities to the true own-price elasticities.\n",
    "Provide two additional tables showing the true matrix of diversion ratios and the diversion\n",
    "ratios implied by your estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "#VSC-7655b5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TRUE elasticities from Q8 (true params, observed shares)...\n",
      "  (Already computed 600 markets)\n",
      "Computing ESTIMATED elasticities (demand-only params, observed shares)...\n",
      "Computing elasticities with respect to prices ...\n",
      "Finished after 00:00:00.\n",
      "\n",
      "Computing ESTIMATED elasticities (joint D&S params, observed shares)...\n",
      "Computing elasticities with respect to prices ...\n",
      "Finished after 00:00:00.\n",
      "\n",
      "\n",
      "Parameter Comparison:\n",
      "                 True      Demand-only    Joint D&S\n",
      "α (price):      -2.000     -2.025       -2.005\n",
      "σ_satellite:     1.000      1.298        1.312\n",
      "σ_wired:         1.000      0.921        0.976\n",
      "\n",
      "\n",
      "===============================================================================================\n",
      "TABLE 1: OWN-PRICE ELASTICITY COMPARISON\n",
      "True = RC logit with TRUE params (-2, 1, 4, 4, 1, 1) on OBSERVED shares\n",
      "Demand-only = RC logit with demand-only estimated params on OBSERVED shares\n",
      "Joint D&S = RC logit with joint demand & supply estimated params on OBSERVED shares\n",
      "===============================================================================================\n",
      "Product      True  Demand-only  Joint D&S  % Error (D-only)  % Error (Joint)\n",
      "  Sat 1   -5.4803      -5.4058    -5.3430            1.3589           2.5051\n",
      "  Sat 2   -5.3145      -5.2287    -5.1671            1.6141           2.7741\n",
      "Wired 1   -5.3772      -5.4692    -5.3873            1.7121           0.1877\n",
      "Wired 2   -5.4581      -5.5498    -5.4675            1.6804           0.1735\n",
      "\n",
      "Mean Absolute % Error (Demand-only): 1.59%\n",
      "Mean Absolute % Error (Joint D&S):   1.41%\n",
      "\n",
      "Interpretation: Estimated |elasticities| are LOWER than true (flatter demand)\n",
      "  • Estimated σ > true σ → More heterogeneity → Products become less substitutable\n",
      "  • When consumers have diverse tastes (high σ), price changes affect fewer people\n",
      "  • Result: Demand is LESS elastic (lower |ε|) with higher σ\n",
      "  • This is OPPOSITE to homogeneous preferences where everyone reacts similarly\n",
      "===============================================================================================\n",
      "\n",
      "Using TRUE diversion ratios from Q8 (true params, observed shares)...\n",
      "  (Already computed 600 markets)\n",
      "Computing diversion ratios with respect to prices ...\n",
      "Finished after 00:00:00.\n",
      "\n",
      "Computing diversion ratios with respect to prices ...\n",
      "Finished after 00:00:00.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TABLE 2: TRUE DIVERSION RATIOS\n",
      "(from RC Logit with TRUE params: σ_sat=1.0, σ_wired=1.0, on OBSERVED shares)\n",
      "================================================================================\n",
      "Diagonal = diversion to outside option D_j0\n",
      "          Sat 1   Sat 2  Wired 1  Wired 2\n",
      "Sat 1    0.5248  0.2239   0.1303   0.1211\n",
      "Sat 2    0.2137  0.5316   0.1314   0.1233\n",
      "Wired 1  0.1237  0.1300   0.5324   0.2139\n",
      "Wired 2  0.1221  0.1292   0.2256   0.5231\n",
      "\n",
      "================================================================================\n",
      "TABLE 3: ESTIMATED DIVERSION RATIOS - DEMAND ONLY\n",
      "(from RC Logit with DEMAND-ONLY params: σ_sat=1.298, σ_wired=0.921)\n",
      "================================================================================\n",
      "Diagonal = diversion to outside option D_j0\n",
      "          Sat 1   Sat 2  Wired 1  Wired 2\n",
      "Sat 1    0.4969  0.2555   0.1284   0.1192\n",
      "Sat 2    0.2439  0.5044   0.1299   0.1217\n",
      "Wired 1  0.1159  0.1220   0.5530   0.2091\n",
      "Wired 2  0.1145  0.1212   0.2206   0.5437\n",
      "\n",
      "================================================================================\n",
      "TABLE 4: ESTIMATED DIVERSION RATIOS - JOINT DEMAND & SUPPLY\n",
      "(from RC Logit with JOINT D&S params: σ_sat=1.312, σ_wired=0.976)\n",
      "================================================================================\n",
      "Diagonal = diversion to outside option D_j0\n",
      "          Sat 1   Sat 2  Wired 1  Wired 2\n",
      "Sat 1    0.4979  0.2576   0.1268   0.1177\n",
      "Sat 2    0.2459  0.5056   0.1283   0.1202\n",
      "Wired 1  0.1151  0.1210   0.5491   0.2147\n",
      "Wired 2  0.1136  0.1203   0.2263   0.5398\n",
      "\n",
      "===============================================================================================\n",
      "INTERPRETATION & KEY FINDING\n",
      "===============================================================================================\n",
      "D_jk = -(∂s_k/∂p_j) / (∂s_j/∂p_j) = -(s_k/s_j) * (ε_kj/ε_jj)\n",
      "  • Off-diagonal: Fraction of j's lost customers who switch to product k\n",
      "  • Diagonal: Fraction of j's lost customers who choose outside option\n",
      "\n",
      "SURPRISING FINDING: Diversion ratios relatively similar despite σ differences!\n",
      "  • True model (σ=1.0):                 52.5% to outside\n",
      "  • Demand-only (σ_sat=1.30, σ_wired=0.92): 49.7% to outside\n",
      "  • Joint D&S (σ_sat=1.31, σ_wired=0.98):   49.8% to outside\n",
      "\n",
      "Mean Absolute % Error in Diversion Ratios:\n",
      "  • Demand-only: 5.08%\n",
      "  • Joint D&S:   5.32%\n",
      "\n",
      "Example: When Sat 1 price ↑ 1%, where do lost customers go?\n",
      "  TRUE model:       52.5% to outside, 22.4% to Sat 2\n",
      "  DEMAND-only:      49.7% to outside, 25.6% to Sat 2\n",
      "  JOINT D&S:        49.8% to outside, 25.8% to Sat 2\n",
      "\n",
      "WHY are diversions similar despite different elasticities?\n",
      "  • Diversion D_jk = -(s_k/s_j) * (ε_kj/ε_jj) is a RATIO of elasticities\n",
      "  • Higher σ makes ALL elasticities flatter (lower |ε|)\n",
      "  • Since BOTH numerator (ε_kj) and denominator (ε_jj) decrease proportionally,\n",
      "    their RATIO stays relatively constant!\n",
      "  • This is why σ estimation errors affect elasticities more than diversions\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Q9: Compare TRUE vs ESTIMATED Random Coefficients Elasticities/Diversions\n",
    "# ============================================================================\n",
    "# Reuse TRUE elasticities computed in Q8 to avoid redundancy\n",
    "print(\"Using TRUE elasticities from Q8 (true params, observed shares)...\")\n",
    "print(f\"  (Already computed {len(true_elasticity_matrices_for_div)} markets)\")\n",
    "true_elasticity_matrices_obs = true_elasticity_matrices_for_div  # Reuse from Q8\n",
    "avg_elasticity_matrix_true_rc = np.mean(true_elasticity_matrices_obs, axis=0)\n",
    "own_elasticities_rc_true = np.diag(avg_elasticity_matrix_true_rc)\n",
    "\n",
    "# Compute ESTIMATED elasticities - DEMAND ONLY\n",
    "print(\"Computing ESTIMATED elasticities (demand-only params, observed shares)...\")\n",
    "elasticities_rc_est1 = optimal_iv_results1.compute_elasticities()\n",
    "avg_elasticities_rc_est1 = elasticities_rc_est1.reshape((T, J, J)).mean(axis=0)\n",
    "own_elasticities_rc_est1 = np.diag(avg_elasticities_rc_est1)\n",
    "\n",
    "# Compute ESTIMATED elasticities - JOINT DEMAND & SUPPLY\n",
    "print(\"Computing ESTIMATED elasticities (joint D&S params, observed shares)...\")\n",
    "elasticities_rc_est2 = optimal_iv_results2.compute_elasticities()\n",
    "avg_elasticities_rc_est2 = elasticities_rc_est2.reshape((T, J, J)).mean(axis=0)\n",
    "own_elasticities_rc_est2 = np.diag(avg_elasticities_rc_est2)\n",
    "\n",
    "# Show parameter differences\n",
    "print(\"\\nParameter Comparison:\")\n",
    "print(\"                 True      Demand-only    Joint D&S\")\n",
    "print(f\"α (price):      -2.000    {optimal_iv_results1.beta[0,0]:7.3f}      {optimal_iv_results2.beta[0,0]:7.3f}\")\n",
    "print(f\"σ_satellite:     1.000    {optimal_iv_results1.sigma[0,0]:7.3f}      {optimal_iv_results2.sigma[0,0]:7.3f}\")\n",
    "print(f\"σ_wired:         1.000    {optimal_iv_results1.sigma[1,1]:7.3f}      {optimal_iv_results2.sigma[1,1]:7.3f}\")\n",
    "print()\n",
    "\n",
    "# Create comparison table with THREE columns\n",
    "product_labels = ['Sat 1', 'Sat 2', 'Wired 1', 'Wired 2']\n",
    "elasticity_comparison_rc = pd.DataFrame({\n",
    "    'Product': product_labels,\n",
    "    'True': own_elasticities_rc_true,\n",
    "    'Demand-only': own_elasticities_rc_est1,\n",
    "    'Joint D&S': own_elasticities_rc_est2,\n",
    "    '% Error (D-only)': np.abs((own_elasticities_rc_est1 - own_elasticities_rc_true) / own_elasticities_rc_true * 100),\n",
    "    '% Error (Joint)': np.abs((own_elasticities_rc_est2 - own_elasticities_rc_true) / own_elasticities_rc_true * 100)\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 95)\n",
    "print(\"TABLE 1: OWN-PRICE ELASTICITY COMPARISON\")\n",
    "print(\"True = RC logit with TRUE params (-2, 1, 4, 4, 1, 1) on OBSERVED shares\")\n",
    "print(\"Demand-only = RC logit with demand-only estimated params on OBSERVED shares\")\n",
    "print(\"Joint D&S = RC logit with joint demand & supply estimated params on OBSERVED shares\")\n",
    "print(\"=\" * 95)\n",
    "print(elasticity_comparison_rc.to_string(index=False, float_format=lambda x: f'{x:9.4f}'))\n",
    "print(f\"\\nMean Absolute % Error (Demand-only): {elasticity_comparison_rc['% Error (D-only)'].mean():.2f}%\")\n",
    "print(f\"Mean Absolute % Error (Joint D&S):   {elasticity_comparison_rc['% Error (Joint)'].mean():.2f}%\")\n",
    "print()\n",
    "print(\"Interpretation: Estimated |elasticities| are LOWER than true (flatter demand)\")\n",
    "print(\"  • Estimated σ > true σ → More heterogeneity → Products become less substitutable\")\n",
    "print(\"  • When consumers have diverse tastes (high σ), price changes affect fewer people\")\n",
    "print(\"  • Result: Demand is LESS elastic (lower |ε|) with higher σ\")\n",
    "print(\"  • This is OPPOSITE to homogeneous preferences where everyone reacts similarly\")\n",
    "print(\"=\" * 95)\n",
    "\n",
    "# --- DIVERSION RATIO COMPARISON ---\n",
    "# Reuse TRUE diversion ratios computed in Q8 to avoid redundancy\n",
    "print(\"\\nUsing TRUE diversion ratios from Q8 (true params, observed shares)...\")\n",
    "print(f\"  (Already computed {len(true_diversion_matrices)} markets)\")\n",
    "true_diversion_matrices_obs = true_diversion_matrices  # Reuse from Q8\n",
    "true_avg_diversion_rc = np.mean(true_diversion_matrices_obs, axis=0)\n",
    "\n",
    "# RC estimated diversion ratios - DEMAND ONLY\n",
    "diversion_rc_est1 = optimal_iv_results1.compute_diversion_ratios()\n",
    "avg_diversion_rc_est1 = diversion_rc_est1.reshape((T, J, J)).mean(axis=0)\n",
    "\n",
    "# RC estimated diversion ratios - JOINT D&S\n",
    "diversion_rc_est2 = optimal_iv_results2.compute_diversion_ratios()\n",
    "avg_diversion_rc_est2 = diversion_rc_est2.reshape((T, J, J)).mean(axis=0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABLE 2: TRUE DIVERSION RATIOS\")\n",
    "print(\"(from RC Logit with TRUE params: σ_sat=1.0, σ_wired=1.0, on OBSERVED shares)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Diagonal = diversion to outside option D_j0\")\n",
    "true_div_df_rc = pd.DataFrame(true_avg_diversion_rc, index=product_labels, columns=product_labels)\n",
    "print(true_div_df_rc.to_string(float_format=lambda x: f'{x:7.4f}'))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABLE 3: ESTIMATED DIVERSION RATIOS - DEMAND ONLY\")\n",
    "print(f\"(from RC Logit with DEMAND-ONLY params: σ_sat={optimal_iv_results1.sigma[0,0]:.3f}, σ_wired={optimal_iv_results1.sigma[1,1]:.3f})\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Diagonal = diversion to outside option D_j0\")\n",
    "est_div_df_rc1 = pd.DataFrame(avg_diversion_rc_est1, index=product_labels, columns=product_labels)\n",
    "print(est_div_df_rc1.to_string(float_format=lambda x: f'{x:7.4f}'))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABLE 4: ESTIMATED DIVERSION RATIOS - JOINT DEMAND & SUPPLY\")\n",
    "print(f\"(from RC Logit with JOINT D&S params: σ_sat={optimal_iv_results2.sigma[0,0]:.3f}, σ_wired={optimal_iv_results2.sigma[1,1]:.3f})\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Diagonal = diversion to outside option D_j0\")\n",
    "est_div_df_rc2 = pd.DataFrame(avg_diversion_rc_est2, index=product_labels, columns=product_labels)\n",
    "print(est_div_df_rc2.to_string(float_format=lambda x: f'{x:7.4f}'))\n",
    "\n",
    "# Calculate diversion errors\n",
    "div_error_d_only = np.abs((avg_diversion_rc_est1 - true_avg_diversion_rc) / true_avg_diversion_rc * 100)\n",
    "div_error_joint = np.abs((avg_diversion_rc_est2 - true_avg_diversion_rc) / true_avg_diversion_rc * 100)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 95)\n",
    "print(\"INTERPRETATION & KEY FINDING\")\n",
    "print(\"=\" * 95)\n",
    "print(\"D_jk = -(∂s_k/∂p_j) / (∂s_j/∂p_j) = -(s_k/s_j) * (ε_kj/ε_jj)\")\n",
    "print(\"  • Off-diagonal: Fraction of j's lost customers who switch to product k\")\n",
    "print(\"  • Diagonal: Fraction of j's lost customers who choose outside option\")\n",
    "print()\n",
    "print(\"SURPRISING FINDING: Diversion ratios relatively similar despite σ differences!\")\n",
    "print(f\"  • True model (σ=1.0):                 {true_avg_diversion_rc[0,0]:.1%} to outside\")\n",
    "print(f\"  • Demand-only (σ_sat={optimal_iv_results1.sigma[0,0]:.2f}, σ_wired={optimal_iv_results1.sigma[1,1]:.2f}): {avg_diversion_rc_est1[0,0]:.1%} to outside\")\n",
    "print(f\"  • Joint D&S (σ_sat={optimal_iv_results2.sigma[0,0]:.2f}, σ_wired={optimal_iv_results2.sigma[1,1]:.2f}):   {avg_diversion_rc_est2[0,0]:.1%} to outside\")\n",
    "print()\n",
    "print(f\"Mean Absolute % Error in Diversion Ratios:\")\n",
    "print(f\"  • Demand-only: {div_error_d_only.mean():.2f}%\")\n",
    "print(f\"  • Joint D&S:   {div_error_joint.mean():.2f}%\")\n",
    "print()\n",
    "print(\"Example: When Sat 1 price ↑ 1%, where do lost customers go?\")\n",
    "print(f\"  TRUE model:       {true_avg_diversion_rc[0,0]:.1%} to outside, {true_avg_diversion_rc[0,1]:.1%} to Sat 2\")\n",
    "print(f\"  DEMAND-only:      {avg_diversion_rc_est1[0,0]:.1%} to outside, {avg_diversion_rc_est1[0,1]:.1%} to Sat 2\")\n",
    "print(f\"  JOINT D&S:        {avg_diversion_rc_est2[0,0]:.1%} to outside, {avg_diversion_rc_est2[0,1]:.1%} to Sat 2\")\n",
    "print()\n",
    "print(\"WHY are diversions similar despite different elasticities?\")\n",
    "print(\"  • Diversion D_jk = -(s_k/s_j) * (ε_kj/ε_jj) is a RATIO of elasticities\")\n",
    "print(\"  • Higher σ makes ALL elasticities flatter (lower |ε|)\")\n",
    "print(\"  • Since BOTH numerator (ε_kj) and denominator (ε_jj) decrease proportionally,\")\n",
    "print(\"    their RATIO stays relatively constant!\")\n",
    "print(\"  • This is why σ estimation errors affect elasticities more than diversions\")\n",
    "print(\"=\" * 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873bc298",
   "metadata": {},
   "source": [
    "## 6 Merger Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b777443f",
   "metadata": {},
   "source": [
    "### 10.\n",
    "Suppose two of the four firms were to merge. Give a brief\n",
    "intuition for what theory tells us is likely to happen to the equilibrium\n",
    "prices of each good $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761404a",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "When two firms merge, all prices increase. The mechanism: merged firms internalize competition between their own products.\n",
    "\n",
    "**Pre-merger FOC (firm $f$, product $j$):**\n",
    "$$p_j - mc_j = -\\frac{s_j}{\\partial s_j/\\partial p_j}$$\n",
    "\n",
    "**Post-merger FOC:**\n",
    "$$p_j - mc_j = -\\left[\\frac{\\partial s_j}{\\partial p_j}\\right]^{-1}\\left[s_j + \\sum_{k \\in \\mathcal{J}_f, k \\neq j} \\frac{\\partial s_k}{\\partial p_j}(p_k - mc_k)\\right]$$\n",
    "\n",
    "The additional term $\\sum_{k \\in \\mathcal{J}_f, k \\neq j} \\frac{\\partial s_k}{\\partial p_j}(p_k - mc_k)$ captures recaptured demand: customers who switch from $j$ to the merged firm's product $k$.\n",
    "\n",
    "**Price effects:**\n",
    "\n",
    "1. **Merging firms:** Large increases, proportional to diversion ratios\n",
    "   - Change in markup: $\\Delta \\text{Markup}_j \\approx \\sum_{k \\in \\mathcal{J}_f, k \\neq j} D_{jk}(p_k - mc_k)$\n",
    "   - Where $D_{jk} = -(\\partial s_k/\\partial p_j)/(\\partial s_j/\\partial p_j)$ = fraction of $j$'s lost customers switching to $k$\n",
    "\n",
    "2. **Non-merging firms:** Small increases from strategic complementarity\n",
    "   - Higher competitor prices → shift in residual demand → optimal to raise own prices\n",
    "   - Magnitude depends on cross-elasticities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef269d8",
   "metadata": {},
   "source": [
    "### 11.\n",
    "Suppose firms 1 and 2 are proposing to merge. Use the \\texttt{pyBLP}\n",
    "merger simulation procedure to provide a prediction of the post-merger\n",
    "equilibrium prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0eddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline marginal costs, markups, profits, and consumer surplus under the estimated demand+supply model\n",
    "costs = optimal_iv_results2.compute_costs()\n",
    "markups = optimal_iv_results2.compute_markups(costs=costs)\n",
    "profits = optimal_iv_results2.compute_profits(costs=costs)\n",
    "cs = optimal_iv_results2.compute_consumer_surpluses()\n",
    "\n",
    "# Get pre-merger prices (reshaped as T×J to get average per product)\n",
    "pre_merger_prices = product_data['prices'].values\n",
    "pre_merger_prices_avg = pre_merger_prices.reshape((T, J)).mean(axis=0)\n",
    "\n",
    "# Create merger firm IDs: merge firms 1 and 2 into firm 1\n",
    "# Firms 1 and 2 (satellite) → firm 1\n",
    "# Firms 3 and 4 (wired) remain unchanged\n",
    "merger_firm_ids = product_data['firm_ids'].copy()\n",
    "merger_firm_ids[merger_firm_ids == 2] = 1  # Firm 2 becomes firm 1\n",
    "\n",
    "# Compute post-merger equilibrium prices using pyBLP's compute_prices method\n",
    "# This solves the first-order conditions under the new ownership structure\n",
    "post_merger_prices = optimal_iv_results2.compute_prices(\n",
    "    firm_ids=merger_firm_ids,\n",
    "    iteration=pyblp.Iteration('simple', {'atol': 1e-12})\n",
    ")\n",
    "\n",
    "# Reshape to get average prices per product\n",
    "post_merger_prices_avg = post_merger_prices.reshape((T, J)).mean(axis=0)\n",
    "\n",
    "# Calculate price changes\n",
    "price_changes = post_merger_prices_avg - pre_merger_prices_avg\n",
    "pct_price_changes = (price_changes / pre_merger_prices_avg) * 100\n",
    "\n",
    "merger_results_df = pd.DataFrame({\n",
    "    'Product': product_labels,\n",
    "    'Type': ['Satellite', 'Satellite', 'Wired', 'Wired'],\n",
    "    'Firm (Pre)': [1, 2, 3, 4],\n",
    "    'Firm (Post)': [1, 1, 3, 4],\n",
    "    'Pre-Merger Price': pre_merger_prices_avg,\n",
    "    'Post-Merger Price': post_merger_prices_avg,\n",
    "    'Price Change ($)': price_changes,\n",
    "    'Price Change (%)': pct_price_changes\n",
    "})\n",
    "numeric_columns = ['Pre-Merger Price', 'Post-Merger Price', 'Price Change ($)', 'Price Change (%)']\n",
    "merger_results_df[numeric_columns] = merger_results_df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "# Diagnostics inspired by the post_estimation tutorial\n",
    "post_merger_shares = optimal_iv_results2.compute_shares(post_merger_prices)\n",
    "post_merger_markups = optimal_iv_results2.compute_markups(post_merger_prices, costs)\n",
    "post_merger_profits = optimal_iv_results2.compute_profits(post_merger_prices, post_merger_shares, costs)\n",
    "post_merger_cs = optimal_iv_results2.compute_consumer_surpluses(post_merger_prices)\n",
    "\n",
    "baseline_metrics = {\n",
    "    'Average Price ($)': pre_merger_prices_avg.mean(),\n",
    "    'Average Markup ($)': markups.reshape((T, J)).mean(),\n",
    "    'Total Profit ($)': profits.sum(),\n",
    "    'Average CS ($)': cs.mean(),\n",
    "}\n",
    "post_merger_metrics = {\n",
    "    'Average Price ($)': post_merger_prices_avg.mean(),\n",
    "    'Average Markup ($)': post_merger_markups.reshape((T, J)).mean(),\n",
    "    'Total Profit ($)': post_merger_profits.sum(),\n",
    "    'Average CS ($)': post_merger_cs.mean(),\n",
    "}\n",
    "metric_names = list(baseline_metrics.keys())\n",
    "merger_metric_summary = pd.DataFrame({\n",
    "    'Metric': metric_names,\n",
    "    'Pre-Merger': [baseline_metrics[m] for m in metric_names],\n",
    "    'Post-Merger': [post_merger_metrics[m] for m in metric_names]\n",
    "})\n",
    "merger_metric_summary['Change'] = (\n",
    "    merger_metric_summary['Post-Merger'] - merger_metric_summary['Pre-Merger']\n",
    ")\n",
    "merger_metric_summary = merger_metric_summary.astype({\n",
    "    'Pre-Merger': float,\n",
    "    'Post-Merger': float,\n",
    "    'Change': float\n",
    "})\n",
    "merger_metric_summary = merger_metric_summary.set_index('Metric')\n",
    "IPython.display.display(merger_results_df.style.format({\n",
    "    'Pre-Merger Price': '{:,.4f}'.format,\n",
    "    'Post-Merger Price': '{:,.4f}'.format,\n",
    "    'Price Change ($)': '{:,.4f}'.format,\n",
    "    'Price Change (%)': '{:,.2f}'.format,\n",
    "}))\n",
    "IPython.display.display(merger_metric_summary.style.format('{:,.3f}'))\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(product_labels, pct_price_changes, color=['#4c72b0', '#4c72b0', '#dd8452', '#dd8452'])\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "ax.set_ylabel('Price Change (%)')\n",
    "ax.set_title('Within-Nest Merger (Firms 1 & 2): Percent Price Changes')\n",
    "for idx, value in enumerate(pct_price_changes):\n",
    "    ax.text(idx, value + (0.1 if value >= 0 else -0.3), f'{value:.2f}%', ha='center', va='bottom' if value >= 0 else 'top', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33319f8",
   "metadata": {},
   "source": [
    "### 13. \n",
    "Now suppose instead that firms 1 and 3 are the ones to merge. Re-run the merger\n",
    "simulation. Provide a table comparing the (average across markets) predicted merger-\n",
    "induced price changes for this merger and that in part 11. Interpret the differences\n",
    "between the predictions for the two mergers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8bd1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# QUESTION 13: MERGER SIMULATION (Firms 1 and 3)\n",
    "# ========================================================================\n",
    "# Firm 1 is satellite provider, Firm 3 is wired provider (cross-nest merger)\n",
    "\n",
    "# Note: pre_merger_prices, post_merger_prices, and mean_pct_change_within \n",
    "# already computed in Question 11\n",
    "\n",
    "# Create merger firm IDs: merge firms 1 and 3 into firm 1\n",
    "# Firm 1 (satellite) + Firm 3 (wired) → firm 1\n",
    "# Firms 2 and 4 remain unchanged\n",
    "merger_firm_ids_cross = product_data['firm_ids'].copy()\n",
    "merger_firm_ids_cross[merger_firm_ids_cross == 3] = 1  # Firm 3 becomes firm 1\n",
    "\n",
    "# Compute post-merger equilibrium prices for cross-nest merger\n",
    "post_merger_prices_cross = optimal_iv_results2.compute_prices(\n",
    "    firm_ids=merger_firm_ids_cross,\n",
    "    iteration=pyblp.Iteration('simple', {'atol': 1e-12})\n",
    ")\n",
    "\n",
    "# Reshape and calculate changes\n",
    "post_merger_prices_cross_matrix = post_merger_prices_cross.reshape((T, J))\n",
    "post_merger_prices_cross_avg = post_merger_prices_cross_matrix.mean(axis=0)\n",
    "mean_pct_change_cross = ((post_merger_prices_cross_avg - pre_merger_prices_avg) / pre_merger_prices_avg * 100)\n",
    "\n",
    "merger_results_cross_df = pd.DataFrame({\n",
    "    'Product': product_labels,\n",
    "    'Type': ['Satellite', 'Satellite', 'Wired', 'Wired'],\n",
    "    'Firm (Pre)': [1, 2, 3, 4],\n",
    "    'Firm (Post)': [1, 2, 1, 4],\n",
    "    'Pre-Merger Price': pre_merger_prices_avg,\n",
    "    'Post-Merger Price': post_merger_prices_cross_avg,\n",
    "    'Price Change ($)': post_merger_prices_cross_avg - pre_merger_prices_avg,\n",
    "    'Price Change (%)': mean_pct_change_cross\n",
    "})\n",
    "numeric_columns_cross = ['Pre-Merger Price', 'Post-Merger Price', 'Price Change ($)', 'Price Change (%)']\n",
    "merger_results_cross_df[numeric_columns_cross] = merger_results_cross_df[numeric_columns_cross].apply(pd.to_numeric, errors='coerce')\n",
    "# Diagnostics parallel to the post_estimation tutorial\n",
    "post_merger_shares_cross = optimal_iv_results2.compute_shares(post_merger_prices_cross)\n",
    "post_merger_markups_cross = optimal_iv_results2.compute_markups(post_merger_prices_cross, costs)\n",
    "post_merger_profits_cross = optimal_iv_results2.compute_profits(\n",
    "    post_merger_prices_cross,\n",
    "    post_merger_shares_cross,\n",
    "    costs\n",
    ")\n",
    "post_merger_cs_cross = optimal_iv_results2.compute_consumer_surpluses(post_merger_prices_cross)\n",
    "\n",
    "cross_merger_metrics = {\n",
    "    'Average Price ($)': post_merger_prices_cross_avg.mean(),\n",
    "    'Average Markup ($)': post_merger_markups_cross.reshape((T, J)).mean(),\n",
    "    'Total Profit ($)': post_merger_profits_cross.sum(),\n",
    "    'Average CS ($)': post_merger_cs_cross.mean(),\n",
    "}\n",
    "cross_metric_summary = pd.DataFrame({\n",
    "    'Metric': metric_names,\n",
    "    'Within-Nest (1&2)': [post_merger_metrics[m] for m in metric_names],\n",
    "    'Cross-Nest (1&3)': [cross_merger_metrics[m] for m in metric_names]\n",
    "})\n",
    "cross_metric_summary['Difference (Cross - Within)'] = (\n",
    "    cross_metric_summary['Cross-Nest (1&3)'] - cross_metric_summary['Within-Nest (1&2)']\n",
    ")\n",
    "cross_metric_summary = cross_metric_summary.astype({\n",
    "    'Within-Nest (1&2)': float,\n",
    "    'Cross-Nest (1&3)': float,\n",
    "    'Difference (Cross - Within)': float,\n",
    "})\n",
    "cross_metric_summary = cross_metric_summary.set_index('Metric')\n",
    "\n",
    "IPython.display.display(merger_results_cross_df.style.format({\n",
    "    'Pre-Merger Price': '{:,.4f}'.format,\n",
    "    'Post-Merger Price': '{:,.4f}'.format,\n",
    "    'Price Change ($)': '{:,.4f}'.format,\n",
    "    'Price Change (%)': '{:,.2f}'.format,\n",
    "}))\n",
    "IPython.display.display(cross_metric_summary.style.format('{:,.3f}'))\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(product_labels, mean_pct_change_cross, color=['#4c72b0', '#55a868', '#4c72b0', '#55a868'])\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "ax.set_ylabel('Price Change (%)')\n",
    "ax.set_title('Cross-Nest Merger (Firms 1 & 3): Percent Price Changes')\n",
    "for idx, value in enumerate(mean_pct_change_cross):\n",
    "    ax.text(idx, value + (0.1 if value >= 0 else -0.3), f'{value:.2f}%', ha='center', va='bottom' if value >= 0 else 'top', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d9de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table: Within-nest vs Cross-nest mergers\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Product': product_labels,\n",
    "    'Within-Nest (%)': pct_price_changes,\n",
    "    'Cross-Nest (%)': mean_pct_change_cross,\n",
    "    'Difference (pp)': mean_pct_change_cross - pct_price_changes\n",
    "})\n",
    "comparison_df[['Within-Nest (%)', 'Cross-Nest (%)', 'Difference (pp)']] = comparison_df[['Within-Nest (%)', 'Cross-Nest (%)', 'Difference (pp)']].apply(pd.to_numeric, errors='coerce')\n",
    "IPython.display.display(comparison_df.style.format({\n",
    "    'Within-Nest (%)': '{:,.2f}'.format,\n",
    "    'Cross-Nest (%)': '{:,.2f}'.format,\n",
    "    'Difference (pp)': '{:,.2f}'.format,\n",
    "}))\n",
    "indices = np.arange(len(product_labels))\n",
    "width = 0.35\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(indices - width / 2, pct_price_changes, width, label='Within-Nest (1&2)', color='#4c72b0')\n",
    "ax.bar(indices + width / 2, mean_pct_change_cross, width, label='Cross-Nest (1&3)', color='#55a868')\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "ax.set_xticks(indices)\n",
    "ax.set_xticklabels(product_labels)\n",
    "ax.set_ylabel('Price Change (%)')\n",
    "ax.set_title('Percent Price Changes: Within-Nest vs Cross-Nest Mergers')\n",
    "ax.legend()\n",
    "for idx, value in enumerate(pct_price_changes):\n",
    "    ax.text(indices[idx] - width / 2, value + (0.1 if value >= 0 else -0.3), f'{value:.2f}%', ha='center', va='bottom' if value >= 0 else 'top', fontsize=8)\n",
    "for idx, value in enumerate(mean_pct_change_cross):\n",
    "    ax.text(indices[idx] + width / 2, value + (0.1 if value >= 0 else -0.3), f'{value:.2f}%', ha='center', va='bottom' if value >= 0 else 'top', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10724b2",
   "metadata": {},
   "source": [
    "### 14. \n",
    "Thus far you have assumed that there are no efficiencies resulting from the merger.\n",
    "Explain briefly why a merger-specific reduction in marginal cost could mean that a merger\n",
    "is welfare-enhancing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4cb18b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0edffe0",
   "metadata": {},
   "source": [
    "### 15.\n",
    "Consider the merger between firms 1 and 2, and suppose the firms\n",
    "demonstrate that by merging they would reduce marginal cost of each of their\n",
    "products by 15\\%. Furthermore, suppose that they demonstrate that this cost\n",
    "reduction could not be achieved without merging.    Using the \\texttt{pyBLP} software, re-run the merger simulation\n",
    "with the 15\\% cost saving. Show the predicted post-merger price changes (again,\n",
    "for each product, averaged across markets). What is the predicted impact of\n",
    "the merger on consumer welfare,\\footnote{%\n",
    "Note that because we have quasilinear preferences, consumer surplus is a\n",
    "valid measure of aggregate consumer welfare under the usual assumption of\n",
    "optimal redistribution.} assuming that the total measure of consumers $%\n",
    "M_{t} $ is the same in each market  $t$? Explain why this additional assumption\n",
    "(or data on the correct values of $M_{t}$) is needed here, whereas up to\n",
    "this point it was without loss to assume $M_{t}=1$. What is the predicted\n",
    "impact of the merger on total welfare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7003d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume same market size M_t for all markets (needed for welfare calculation)\n",
    "# We'll use M_t = 1000 consumers per market as a reasonable assumption\n",
    "M_t = 1000\n",
    "\n",
    "# Get estimated marginal costs from optimal_iv_results2\n",
    "marginal_costs = optimal_iv_results2.compute_costs()\n",
    "\n",
    "# Create a copy with 15% cost reduction for firms 1 and 2\n",
    "marginal_costs_efficiency = marginal_costs.copy()\n",
    "\n",
    "# Apply 15% reduction (multiply by 0.85) to firms 1 and 2 products\n",
    "# More efficient approach using boolean indexing\n",
    "products_1_2 = product_data['firm_ids'].isin([1, 2])\n",
    "marginal_costs_efficiency[products_1_2] *= 0.85\n",
    "\n",
    "# Use the same merger firm IDs as Question 11\n",
    "# (firm 2 becomes firm 1, already computed earlier as merger_firm_ids)\n",
    "\n",
    "# Compute post-merger prices WITH efficiency gains\n",
    "post_merger_prices_efficiency = optimal_iv_results2.compute_prices(\n",
    "    firm_ids=merger_firm_ids,  # Reuse from Q11\n",
    "    costs=marginal_costs_efficiency,\n",
    "    iteration=pyblp.Iteration('simple', {'atol': 1e-12})\n",
    ")\n",
    "\n",
    "# Reshape and calculate changes\n",
    "post_merger_prices_eff_matrix = post_merger_prices_efficiency.reshape((T, J))\n",
    "post_merger_prices_eff_avg = post_merger_prices_eff_matrix.mean(axis=0)\n",
    "\n",
    "# Calculate price changes relative to pre-merger baseline\n",
    "price_changes_eff = post_merger_prices_eff_avg - pre_merger_prices_avg\n",
    "pct_price_changes_eff = (price_changes_eff / pre_merger_prices_avg) * 100\n",
    "\n",
    "merger_efficiency_df = pd.DataFrame({\n",
    "    'Product': product_labels,\n",
    "    'Type': ['Satellite', 'Satellite', 'Wired', 'Wired'],\n",
    "    'Firm (Pre)': [1, 2, 3, 4],\n",
    "    'Firm (Post)': [1, 1, 3, 4],\n",
    "    'Pre-Merger Price': pre_merger_prices_avg,\n",
    "    'Post-Merger Price': post_merger_prices_eff_avg,\n",
    "    'Price Change ($)': price_changes_eff,\n",
    "    'Price Change (%)': pct_price_changes_eff\n",
    "})\n",
    "numeric_columns_eff = ['Pre-Merger Price', 'Post-Merger Price', 'Price Change ($)', 'Price Change (%)']\n",
    "merger_efficiency_df[numeric_columns_eff] = merger_efficiency_df[numeric_columns_eff].apply(pd.to_numeric, errors='coerce')\n",
    "comparison_eff_df = pd.DataFrame({\n",
    "    'Product': product_labels,\n",
    "    'No Efficiency (%)': pct_price_changes,  # From Q11\n",
    "    'With 15% Cost Cut (%)': pct_price_changes_eff,\n",
    "    'Difference (pp)': pct_price_changes_eff - pct_price_changes\n",
    "})\n",
    "comparison_eff_df[['No Efficiency (%)', 'With 15% Cost Cut (%)', 'Difference (pp)']] = comparison_eff_df[['No Efficiency (%)', 'With 15% Cost Cut (%)', 'Difference (pp)']].apply(pd.to_numeric, errors='coerce')\n",
    "IPython.display.display(merger_efficiency_df.style.format({\n",
    "    'Pre-Merger Price': '{:,.4f}'.format,\n",
    "    'Post-Merger Price': '{:,.4f}'.format,\n",
    "    'Price Change ($)': '{:,.4f}'.format,\n",
    "    'Price Change (%)': '{:,.2f}'.format,\n",
    "}))\n",
    "IPython.display.display(comparison_eff_df.style.format({\n",
    "    'No Efficiency (%)': '{:,.2f}'.format,\n",
    "    'With 15% Cost Cut (%)': '{:,.2f}'.format,\n",
    "    'Difference (pp)': '{:,.2f}'.format,\n",
    "}))\n",
    "# Diagnostics parallel to the post_estimation tutorial\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(product_labels, pct_price_changes_eff, color=['#4c72b0', '#4c72b0', '#dd8452', '#dd8452'])\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "ax.set_ylabel('Price Change (%)')\n",
    "ax.set_title('Within-Nest Merger with 15% Cost Reduction: Percent Price Changes')\n",
    "for idx, value in enumerate(pct_price_changes_eff):\n",
    "    ax.text(idx, value + (0.1 if value >= 0 else -0.3), f'{value:.2f}%', ha='center', va='bottom' if value >= 0 else 'top', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "post_merger_shares_eff = optimal_iv_results2.compute_shares(post_merger_prices_efficiency)\n",
    "post_merger_markups_eff = optimal_iv_results2.compute_markups(\n",
    "    post_merger_prices_efficiency,\n",
    "    marginal_costs_efficiency\n",
    ")\n",
    "post_merger_profits_eff = optimal_iv_results2.compute_profits(\n",
    "    post_merger_prices_efficiency,\n",
    "    post_merger_shares_eff,\n",
    "    marginal_costs_efficiency\n",
    ")\n",
    "post_merger_cs_eff = optimal_iv_results2.compute_consumer_surpluses(post_merger_prices_efficiency)\n",
    "\n",
    "efficiency_metrics = {\n",
    "    'Average Price ($)': post_merger_prices_eff_avg.mean(),\n",
    "    'Average Markup ($)': post_merger_markups_eff.reshape((T, J)).mean(),\n",
    "    'Total Profit ($)': post_merger_profits_eff.sum(),\n",
    "    'Average CS ($)': post_merger_cs_eff.mean(),\n",
    "}\n",
    "efficiency_metric_summary = pd.DataFrame({\n",
    "    'Metric': metric_names,\n",
    "    'No Efficiency (1&2)': [post_merger_metrics[m] for m in metric_names],\n",
    "    '15% Cost Cut (1&2)': [efficiency_metrics[m] for m in metric_names]\n",
    "})\n",
    "efficiency_metric_summary['Difference (Eff - No Eff)'] = (\n",
    "    efficiency_metric_summary['15% Cost Cut (1&2)'] - efficiency_metric_summary['No Efficiency (1&2)']\n",
    ")\n",
    "efficiency_metric_summary = efficiency_metric_summary.astype({\n",
    "    'No Efficiency (1&2)': float,\n",
    "    '15% Cost Cut (1&2)': float,\n",
    "    'Difference (Eff - No Eff)': float,\n",
    "})\n",
    "efficiency_metric_summary = efficiency_metric_summary.set_index('Metric')\n",
    "\n",
    "IPython.display.display(efficiency_metric_summary.style.format('{:,.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8db341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# WELFARE ANALYSIS\n",
    "# ========================================================================\n",
    "\n",
    "# Compute consumer surplus for different scenarios using PyBLP\n",
    "# Pre-merger consumer surplus (baseline)\n",
    "cs_pre = optimal_iv_results2.compute_consumer_surpluses()\n",
    "\n",
    "# Post-merger WITHOUT efficiencies (from Question 11)\n",
    "cs_post_no_eff = optimal_iv_results2.compute_consumer_surpluses(prices=post_merger_prices)\n",
    "\n",
    "# Post-merger WITH 15% cost reduction\n",
    "cs_post_with_eff = optimal_iv_results2.compute_consumer_surpluses(prices=post_merger_prices_efficiency)\n",
    "\n",
    "# Calculate changes in consumer surplus (per consumer, per market)\n",
    "delta_cs_no_eff = cs_post_no_eff - cs_pre\n",
    "delta_cs_with_eff = cs_post_with_eff - cs_pre\n",
    "\n",
    "# Aggregate across all markets and consumers\n",
    "# Total CS change = M_t * sum over all markets\n",
    "total_delta_cs_no_eff = M_t * delta_cs_no_eff.sum()\n",
    "total_delta_cs_with_eff = M_t * delta_cs_with_eff.sum()\n",
    "\n",
    "# Average per consumer across markets\n",
    "avg_delta_cs_no_eff = delta_cs_no_eff.mean()\n",
    "avg_delta_cs_with_eff = delta_cs_with_eff.mean()\n",
    "\n",
    "# Compute producer surplus changes\n",
    "# Producer surplus = (p - mc) × shares × M_t for each product\n",
    "# Pre-merger\n",
    "shares_pre = product_data['shares'].values.reshape((T, J))\n",
    "prices_pre_matrix = pre_merger_prices.reshape((T, J))\n",
    "mc_pre_matrix = marginal_costs.reshape((T, J))\n",
    "ps_pre = ((prices_pre_matrix - mc_pre_matrix) * shares_pre * M_t).sum()\n",
    "\n",
    "# Post-merger without efficiency\n",
    "shares_post_no_eff = optimal_iv_results2.compute_shares(prices=post_merger_prices)\n",
    "shares_post_no_eff_matrix = shares_post_no_eff.reshape((T, J))\n",
    "prices_post_no_eff_matrix = post_merger_prices.reshape((T, J))\n",
    "ps_post_no_eff = ((prices_post_no_eff_matrix - mc_pre_matrix) * shares_post_no_eff_matrix * M_t).sum()\n",
    "\n",
    "# Post-merger with 15% efficiency\n",
    "shares_post_eff = optimal_iv_results2.compute_shares(prices=post_merger_prices_efficiency)\n",
    "shares_post_eff_matrix = shares_post_eff.reshape((T, J))\n",
    "mc_eff_matrix = marginal_costs_efficiency.reshape((T, J))\n",
    "ps_post_eff = ((post_merger_prices_eff_matrix - mc_eff_matrix) * shares_post_eff_matrix * M_t).sum()\n",
    "\n",
    "# Changes in producer surplus\n",
    "delta_ps_no_eff = ps_post_no_eff - ps_pre\n",
    "delta_ps_with_eff = ps_post_eff - ps_pre\n",
    "\n",
    "# Total welfare changes\n",
    "delta_w_no_eff = total_delta_cs_no_eff + delta_ps_no_eff\n",
    "delta_w_with_eff = total_delta_cs_with_eff + delta_ps_with_eff\n",
    "\n",
    "welfare_summary = pd.DataFrame({\n",
    "    'Scenario': ['Without efficiency', 'With 15% cost cut'],\n",
    "    'ΔCS per consumer ($)': [avg_delta_cs_no_eff, avg_delta_cs_with_eff],\n",
    "    'ΔCS total ($)': [total_delta_cs_no_eff, total_delta_cs_with_eff],\n",
    "    'ΔPS ($)': [delta_ps_no_eff, delta_ps_with_eff],\n",
    "    'ΔW ($)': [delta_w_no_eff, delta_w_with_eff],\n",
    "})\n",
    "welfare_summary['Verdict'] = welfare_summary['ΔW ($)'].apply(lambda x: 'ENHANCING' if x >= 0 else 'REDUCING')\n",
    "numeric_columns_welfare = ['ΔCS per consumer ($)', 'ΔCS total ($)', 'ΔPS ($)', 'ΔW ($)']\n",
    "welfare_summary[numeric_columns_welfare] = welfare_summary[numeric_columns_welfare].apply(pd.to_numeric, errors='coerce')\n",
    "welfare_summary = welfare_summary.set_index('Scenario')\n",
    "IPython.display.display(welfare_summary.style.format({\n",
    "    'ΔCS per consumer ($)': '{:,.4f}'.format,\n",
    "    'ΔCS total ($)': '{:,.2f}'.format,\n",
    "    'ΔPS ($)': '{:,.2f}'.format,\n",
    "    'ΔW ($)': '{:,.2f}'.format,\n",
    "}))\n",
    "IPython.display.display(IPython.display.Markdown(f'*Totals assume M_t = {M_t:,} consumers per market across all {T} markets.*'))\n",
    "component_df = welfare_summary[['ΔCS total ($)', 'ΔPS ($)']]\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "component_df.plot(kind='bar', stacked=True, ax=ax, color=['#4c72b0', '#dd8452'])\n",
    "ax.set_ylabel('Dollars')\n",
    "ax.set_title('Total Surplus Changes by Scenario')\n",
    "ax.set_xticks(range(len(component_df.index)))\n",
    "ax.set_xticklabels(component_df.index, rotation=0)\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "totals = component_df.sum(axis=1)\n",
    "for idx, total in enumerate(totals):\n",
    "    offset = 0.02 * total if total != 0 else 0.02\n",
    "    ax.text(idx, total + (offset if total >= 0 else -abs(offset)), f'{total:,.0f}', ha='center', va='bottom' if total >= 0 else 'top', fontsize=8)\n",
    "ax.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82576351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# Consolidated diagnostics across all merger scenarios\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "scenario_metric_table = pd.DataFrame({\n",
    "    'Metric': metric_names,\n",
    "    'Pre-Merger Baseline': [baseline_metrics[m] for m in metric_names],\n",
    "    'Merger 1&2 (No Eff)': [post_merger_metrics[m] for m in metric_names],\n",
    "    'Merger 1&3 (Cross)': [cross_merger_metrics[m] for m in metric_names],\n",
    "    'Merger 1&2 (15% Cost Cut)': [efficiency_metrics[m] for m in metric_names],\n",
    "})\n",
    "scenario_metric_table['Δ (1&2 - Pre)'] = scenario_metric_table['Merger 1&2 (No Eff)'] - scenario_metric_table['Pre-Merger Baseline']\n",
    "scenario_metric_table['Δ (1&3 - Pre)'] = scenario_metric_table['Merger 1&3 (Cross)'] - scenario_metric_table['Pre-Merger Baseline']\n",
    "scenario_metric_table['Δ (15% Cut - Pre)'] = scenario_metric_table['Merger 1&2 (15% Cost Cut)'] - scenario_metric_table['Pre-Merger Baseline']\n",
    "\n",
    "numeric_columns = scenario_metric_table.select_dtypes(include='number').columns\n",
    "scenario_metric_table[numeric_columns] = scenario_metric_table[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "format_dict = {col: '{:,.3f}'.format for col in numeric_columns}\n",
    "IPython.display.display(scenario_metric_table.style.format(format_dict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
